## 大模型训练的概述
大语言模型的训练分为两个阶段：

- **预训练阶段**：模型学习预测下一个词或标记。从计算和成本角度看，这是训练的主体部分，通常需要在数万亿甚至数十万亿文本标记上进行训练。对于超大规模模型，这一过程可能耗时数月。

- **后训练阶段**：模型通过进一步训练以执行更具体的任务（例如回答问题）。此阶段通常使用规模小得多的数据集，训练速度更快且成本更低。
 
1. **监督微调（SFT）**：通过带标注的提示-响应对训练模型，使其学会遵循指令或使用工具，核心在于让模型模仿输入提示与输出响应间的映射关系。该技术特别适用于引入新行为或对模型进行重大调整。在课程中，您将动手对一个千问小模型进行指令遵循微调。

2. **直接偏好优化（DPO）**：通过向模型展示同一提示下的“优质”与“劣质”答案，驱动模型学习。`DPO`通过构造性损失函数，使模型趋近优质响应而远离劣质响应。例如，若模型当前回复“我是你的助手”，而您希望其回答“我是你的AI助手”，则可将前者标记为劣质响应，后者标记为优质响应。您将使用`DPO`调整一个`Qwen`指令模型的“身份认知”。

3. **在线强化学习(Online RL）**：作为第三种技术，该方法让模型接收提示并生成响应，随后由奖励函数对回答质量进行评分，模型根据这些奖励分数进行更新。获取奖励函数的一种方式是基于人类对响应质量的评判，训练出一个与人类判断一致的评分函数。最常用的算法可能是近端策略优化。另一种方法是利用可验证奖励，适用于数学或编程等具有客观正确性标准的任务——例如使用数学验证器或单元测试来判定生成的解题步骤或代码是否正确。这种正确性度量即可作为奖励函数。针对此类奖励函数，DeepSeek团队提出的 `GRPO` 算法是一种高效实现方案。在本课程中，您将使用 `GRPO`训练一个`Qwen`小模型解决数学问题。

