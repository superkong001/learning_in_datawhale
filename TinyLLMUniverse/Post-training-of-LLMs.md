## 大模型训练
大语言模型的训练分为两个阶段：

- **预训练阶段**：模型学习预测下一个词或标记。从计算和成本角度看，这是训练的主体部分，通常需要在数万亿甚至数十万亿文本标记上进行训练。对于超大规模模型，这一过程可能耗时数月。

- **后训练阶段**：模型通过进一步训练以执行更具体的任务（例如回答问题）。此阶段通常使用规模小得多的数据集，训练速度更快且成本更低。

## 后训练方法

### 监督微调（Supervised Fine‑Tuning,SFT）

它是一种把通用语言模型转换成任务型助手的方法，使其学会遵循指令或使用工具。通过训练提示与理想回应的成对数据（带标注的提示-响应对），模型学会模仿示例中的回答，从而能够按照指令行事、展示期望的行为并正确调用工具，核心在于让模型模仿输入提示与输出响应间的映射关系。该技术特别适用于引入新行为或对模型进行重大调整。如：对一个千问小模型进行指令遵循微调。

 SFT 的核心是让基础模型（只根据提示预测下一个 token）学会生成预期的回答，流程如下：
 
 - **基础模型**：未经调整的 LLM 往往会给出泛泛或重复的回应，例如面对“你是谁？”这样的询问，它可能只是反问一句，而不是回答。
 - **带标签的数据集**：收集并整理用户提示与理想助理回应的配对，例如“请告诉我你的身份——我是 Llama…”、“你最近怎么样？——我很好！”。
 - **SFT 训练**：对这些配对进行微调，通过最小化回应的交叉熵损失来训练模型：
 
 $$\mathcal{L}_{\text{SFT}} = -\sum_{i=1}^N \log \bigl(p_\theta(\text{Response}(i)\mid \text{Prompt}(i))\bigr)$$
 
 这一损失鼓励模型最大化在每个提示条件下生成目标回应的概率。
 
 - **微调后的模型**：完成训练后，模型可以针对新的查询给出合适的回复（例如向用户问好，而不是简单重复问题）。

 <img width="2560" height="1361" alt="3b4f0112f4961463d95af14d8f02b14f_SFT1" src="https://github.com/user-attachments/assets/74c266b4-9e56-43f3-8893-9a3e6ba453ec" />

 上面的公式也可以理解为最大化在提示条件下回应中所有 token 的联合概率。交叉熵损失会惩罚偏离标签回应的输出，因此 SFT 本质上是在教模型“模仿”。

#### SFT 的最佳使用场景

SFT 并非万能，它在特定场景下尤其有效：

- **激发新的模型行为**：
  - 将预训练模型转变为能遵循指令的助理。
  - 让不具备推理能力的模型学会基本推理。
  - 让模型在没有明确说明的情况下使用特定工具。
- **提升模型能力**：
  - 利用强大的大模型生成高质量的合成数据，通过训练把这些能力“蒸馏”到小模型中。

#### SFT 数据策划原则

SFT 的效果高度依赖于数据质量。优质且多样的样本能让模型学到有用的行为；劣质样本则会让模型模仿不良习惯。常用的数据策划方法包括：

- **蒸馏**：用更强的指令模型生成回复，再训练小模型去模仿这些回复，把强模型的能力迁移到弱模型上。
- **Best‑of‑K / 拒绝采样**：针对同一提示生成多个候选回复，再用奖励函数选出最好的作为训练数据。
- **过滤**：从大型 SFT 数据集中挑选出回应质量高且提示多样性好的样本，形成精简的高质量数据集。

#### 全参数微调 vs 参数高效微调

在执行 SFT（或其他对齐方法）时，需要决定如何更新模型权重：

<img width="2560" height="1363" alt="5de62dc8c10cab0354ac94f8397a7a78_SFT4" src="https://github.com/user-attachments/assets/c71c4030-9dc2-45a5-823a-b1f5b78ec383" />

- **全参数微调**：对每一层加入一个完整的权重更新矩阵  $\Delta W$ ，即修改所有参数。这可以显著提升性能，但需要大量存储和计算资源。
- **参数高效微调**：例如 LoRA（低秩适配）通过在每层引入小的低秩矩阵 A 和 B 来调整模型参数。这减少了可训练参数的数量，节省显存，缺点是学习和遗忘都更有限，因为更新的参数更少。

### 直接偏好优化（DPO）

通过向模型展示同一提示下的“优质”与“劣质”答案，驱动模型学习。`DPO`通过构造性损失函数，使模型趋近优质响应而远离劣质响应。例如，若模型当前回复“我是你的助手”，而您希望其回答“我是你的AI助手”，则可将前者标记为劣质响应，后者标记为优质响应。如：使用`DPO`调整一个`Qwen`指令模型的“身份认知”。

### 在线强化学习(Online RL）

作为第三种技术，该方法让模型接收提示并生成响应，随后由奖励函数对回答质量进行评分，模型根据这些奖励分数进行更新。获取奖励函数的一种方式是基于人类对响应质量的评判，训练出一个与人类判断一致的评分函数。最常用的算法可能是近端策略优化。另一种方法是利用可验证奖励，适用于数学或编程等具有客观正确性标准的任务——例如使用数学验证器或单元测试来判定生成的解题步骤或代码是否正确。这种正确性度量即可作为奖励函数。针对此类奖励函数，DeepSeek团队提出的 `GRPO` 算法是一种高效实现方案。如：使用 `GRPO`训练一个`Qwen`小模型解决数学问题。

