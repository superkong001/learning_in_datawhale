## 大模型训练

<img width="1880" height="935" alt="8b0213d5fe10863367023c88087e2396_introduction1" src="https://github.com/user-attachments/assets/31434930-54ad-4540-91b4-ce1c681fce3a" />

大语言模型的训练分为两个阶段：

- **预训练阶段**：模型学习预测下一个词或标记。预训练通常被视为无监督学习，其起点是大规模无标注文本语料（如维基百科、`Common Crawl`或`GitHub`等）。通常可从这些语料中提取超过2万亿个标记进行训练。从计算和成本角度看，这是训练的主体部分，通常需要在数万亿甚至数十万亿文本标记上进行训练。对于超大规模模型，这一过程可能耗时数月。

<img width="748" height="807" alt="10d8c17d580106c52dd16053e24eb9e0_introduction3" src="https://github.com/user-attachments/assets/94ea9ffb-a16e-4f58-a528-d60739174327" />

当输入"我喜欢猫"这样的句子时，模型会基于前面所有标记来最小化每个标记的负对数概率：首先最小化"我"的负对数概率，然后是给定"我"时"喜欢"的负对数似然，最后是给定"我喜欢"时"猫"的概率。通过这种方式，模型被训练成能根据已见标记预测下一个标记。

- **后训练阶段**：模型通过进一步训练以执行更具体的任务（例如回答问题）。此阶段通常使用规模小得多的数据集，训练速度更快且成本更低。

**大模型后训练场景**:

<img width="1888" height="916" alt="ddbf7d18f6d3df7830c625707af72af7_introduction2" src="https://github.com/user-attachments/assets/72213697-2c7a-4c44-a67d-3db2b75b9067" />

不适用模型后训练场景：

- 若仅需模型遵循少量指令（如回避敏感话题或禁止公司间比较），通过提示工程即可实现，但该方法虽简单却不够稳定

- 如需查询实时数据库，检索增强生成或基于搜索的方法可能更适用

- 创建领域专用模型（如医疗或网络安全语言模型）时，通常需要持续预训练结合标准后训练，先让模型学习领域知识（至少需10亿标记），再学习用户交互

- 当需要严格遵循20条以上指令，或提升特定能力（如构建强SQL模型、函数调用模型或推理模型）时，后训练最能发挥价值——它能可靠改变模型行为并提升目标能力，但若实施不当可能导致其他未训练能力退化

## 后训练方法

### 监督微调（Supervised Fine‑Tuning,SFT）

它是一种把通用语言模型转换成任务型助手的方法，使其学会遵循指令或使用工具。通过训练提示与理想回应的成对数据（带标注的提示-响应对），模型学会模仿示例中的回答，从而能够按照指令行事、展示期望的行为并正确调用工具，核心在于让模型模仿输入提示与输出响应间的映射关系。该技术特别适用于引入新行为或对模型进行重大调整。此过程仅需1,000至10亿个标记，远少于预训练规模。其训练损失的关键区别在于：仅对响应标记进行训练，而不涉及提示标记。如：对一个千问小模型进行指令遵循微调。

<img width="835" height="818" alt="image" src="https://github.com/user-attachments/assets/e089267d-0af8-4448-90c7-40296ba29109" />

 SFT 的核心是让基础模型（只根据提示预测下一个 token）学会生成预期的回答，流程如下：
 
 - **基础模型**：未经调整的 LLM 往往会给出泛泛或重复的回应，例如面对“你是谁？”这样的询问，它可能只是反问一句，而不是回答。
 - **带标签的数据集**：收集并整理用户提示与理想助理回应的配对，例如“请告诉我你的身份——我是 Llama…”、“你最近怎么样？——我很好！”。
 - **SFT 训练**：对这些配对进行微调，通过最小化回应的交叉熵损失来训练模型：
 
 $$\mathcal{L}_{\text{SFT}} = -\sum_{i=1}^N \log \bigl(p_\theta(\text{Response}(i)\mid \text{Prompt}(i))\bigr)$$
 
 这一损失鼓励模型最大化在每个提示条件下生成目标回应的概率。
 
 - **微调后的模型**：完成训练后，模型可以针对新的查询给出合适的回复（例如向用户问好，而不是简单重复问题）。

 <img width="2560" height="1361" alt="3b4f0112f4961463d95af14d8f02b14f_SFT1" src="https://github.com/user-attachments/assets/74c266b4-9e56-43f3-8893-9a3e6ba453ec" />

 上面的公式也可以理解为最大化在提示条件下回应中所有 token 的联合概率。交叉熵损失会惩罚偏离标签回应的输出，因此 SFT 本质上是在教模型“模仿”。

#### SFT 的最佳使用场景

SFT 并非万能，它在特定场景下尤其有效：

- **激发新的模型行为**：
  - 将预训练模型转变为能遵循指令的助理。
  - 让不具备推理能力的模型学会基本推理。
  - 让模型在没有明确说明的情况下使用特定工具。
- **提升模型能力**：
  - 利用强大的大模型生成高质量的合成数据，通过训练把这些能力“蒸馏”到小模型中。

#### SFT 数据策划原则

SFT 的效果高度依赖于数据质量。优质且多样的样本能让模型学到有用的行为；劣质样本则会让模型模仿不良习惯。常用的数据策划方法包括：

- **蒸馏**：用更强的指令模型生成回复，再训练小模型去模仿这些回复，把强模型的能力迁移到弱模型上。
- **Best‑of‑K / 拒绝采样**：针对同一提示生成多个候选回复，再用奖励函数选出最好的作为训练数据。
- **过滤**：从大型 SFT 数据集中挑选出回应质量高且提示多样性好的样本，形成精简的高质量数据集。

#### 全参数微调 vs 参数高效微调

在执行 SFT（或其他对齐方法）时，需要决定如何更新模型权重：

<img width="2560" height="1363" alt="5de62dc8c10cab0354ac94f8397a7a78_SFT4" src="https://github.com/user-attachments/assets/c71c4030-9dc2-45a5-823a-b1f5b78ec383" />

- **全参数微调**：对每一层加入一个完整的权重更新矩阵  $\Delta W$ ，即修改所有参数。这可以显著提升性能，但需要大量存储和计算资源。
- **参数高效微调**：例如 LoRA（低秩适配）通过在每层引入小的低秩矩阵 A 和 B 来调整模型参数。这减少了可训练参数的数量，节省显存，缺点是学习和遗忘都更有限，因为更新的参数更少。

### 直接偏好优化（DPO）

通过向模型展示同一提示下的“优质”与“劣质”答案，驱动模型学习。`DPO`通过构造性损失函数，使模型趋近优质响应而远离劣质响应。例如，若模型当前回复“我是你的助手”，而您希望其回答“我是你的AI助手”，则可将前者标记为劣质响应，后者标记为优质响应。训练目标是使模型远离劣质响应并学习优质响应。该方法同样仅需1,000至10亿个标记，并采用更复杂的损失函数。如：使用`DPO`调整一个`Qwen`指令模型的“身份认知”。

<img width="981" height="832" alt="5d03449402b7a286eb8052b435108381_introduction5" src="https://github.com/user-attachments/assets/c3be54e9-a1a6-430f-98c1-51ca91f1466e" />

### 在线强化学习(Online RL）

作为第三种技术，该方只需准备提示集和奖励函数。从提示开始，让模型接收提示并生成响应，随后由奖励函数对回答质量进行评分，模型根据这些奖励分数进行更新。获取奖励函数的一种方式是基于人类对响应质量的评判，训练出一个与人类判断一致的评分函数。最常用的算法可能是近端策略优化。另一种方法是利用可验证奖励，适用于数学或编程等具有客观正确性标准的任务——例如使用数学验证器或单元测试来判定生成的解题步骤或代码是否正确。这种正确性度量即可作为奖励函数。针对此类奖励函数，DeepSeek团队提出的 `GRPO` 算法是一种高效实现方案。通常需要1,000至1,000万（或更多）个提示，目标是通过模型自身生成的响应来最大化奖励值。如：使用 `GRPO`训练一个`Qwen`小模型解决数学问题。

<img width="798" height="805" alt="image" src="https://github.com/user-attachments/assets/5518001c-3cd4-497a-8c1a-5f921047f9a3" />

### 成功的后训练需要确保三个关键要素

1. **数据与算法的协同设计**：如前所述，后训练有多种方法选择（`SFT`、`DPO`及各在线强化学习算法等），每种方法所需的数据结构略有不同。良好的协同设计对后训练成效至关重要。

2. **可靠高效的算法库**：`HuggingFace TRL`作为首批易用库之一，实现了大部分前述算法。此外还推荐`Open RLHF`、`veRL`和`Nemo RL`等更精密、内存效率更高的库。

3. **合适的评估体系**：需通过完善的评估方案追踪模型在后训练前后的表现，确保模型性能持续优良。现有流行语言模型评估标准包括：

   - 对话机器人竞技场：基于人类偏好的聊天评估
        
    - 替代人类评判的LLM评估：`AlpacaEval`、`MT Bench`、`Arena Hard`
        
    - 指令模型静态基准：`LiveCodeBench`（热门代码基准）、`AIME 2024/2025`（高难度数学评估）
        
    - 知识与推理数据集：`GPQA`、`MMLU Pro`
        
    - 指令遵循评估：`IFEval`
        
    - 函数调用与智能体评估：`BFCL`、`NexusBench`、`TauBench`、`ToolSandbox`（后两者专注多工具使用场景）
  
  <img width="1783" height="925" alt="95314eed344dbbcd8afa1d61fa9b8293_introduction8" src="https://github.com/user-attachments/assets/8a57e6d8-407a-49dc-a37a-41540d0398f7" />



