参考：
- [datawhalechina的Post-training-of-LLMs](https://github.com/datawhalechina/Post-training-of-LLMs)
- [Agentic AI](https://www.deeplearning.ai/courses/agentic-ai/)
- [Post-training of LLMs](https://www.deeplearning.ai/short-courses/post-training-of-llms/)

## 大模型训练

<img width="1880" height="935" alt="8b0213d5fe10863367023c88087e2396_introduction1" src="https://github.com/user-attachments/assets/31434930-54ad-4540-91b4-ce1c681fce3a" />

大语言模型的训练分为两个阶段：

- **预训练阶段**：模型学习预测下一个词或标记。预训练通常被视为无监督学习，其起点是大规模无标注文本语料（如维基百科、`Common Crawl`或`GitHub`等）。通常可从这些语料中提取超过2万亿个标记进行训练。从计算和成本角度看，这是训练的主体部分，通常需要在数万亿甚至数十万亿文本标记上进行训练。对于超大规模模型，这一过程可能耗时数月。

<img width="748" height="807" alt="10d8c17d580106c52dd16053e24eb9e0_introduction3" src="https://github.com/user-attachments/assets/94ea9ffb-a16e-4f58-a528-d60739174327" />

当输入"我喜欢猫"这样的句子时，模型会基于前面所有标记来最小化每个标记的负对数概率：首先最小化"我"的负对数概率，然后是给定"我"时"喜欢"的负对数似然，最后是给定"我喜欢"时"猫"的概率。通过这种方式，模型被训练成能根据已见标记预测下一个标记。

- **后训练阶段**：模型通过进一步训练以执行更具体的任务（例如回答问题）。此阶段通常使用规模小得多的数据集，训练速度更快且成本更低。

**大模型后训练场景**:

<img width="1888" height="916" alt="ddbf7d18f6d3df7830c625707af72af7_introduction2" src="https://github.com/user-attachments/assets/72213697-2c7a-4c44-a67d-3db2b75b9067" />

并非所有用例都需要进行适用模型后训练：

- 若仅需模型遵循少量指令（如回避敏感话题或禁止公司间比较），通过提示工程即可实现，但该方法虽简单却不够稳定

- 如需查询实时数据库，检索增强生成或基于搜索的方法可能更适用

- 创建领域专用模型（如医疗或网络安全语言模型）时，通常需要持续预训练结合标准后训练，先让模型学习领域知识（至少需10亿标记），再学习用户交互

- 当需要严格遵循20条以上指令，或提升特定能力（如构建强SQL模型、函数调用模型或推理模型）时，后训练最能发挥价值——它能可靠改变模型行为并提升目标能力，但若实施不当可能导致其他未训练能力退化

## 后训练方法

<img width="740" height="368" alt="image" src="https://github.com/user-attachments/assets/7f2ae926-9038-4d6f-beb1-812757dad466" />

<img width="731" height="296" alt="image" src="https://github.com/user-attachments/assets/dce76d95-f9e2-43e1-b6a6-6fa296f82dd9" />

### 监督微调（Supervised Fine‑Tuning,SFT）

它是一种把通用语言模型转换成任务型助手的方法，使其学会遵循指令或使用工具。通过训练提示与理想回应的成对数据（带标注的提示-响应对），模型学会模仿示例中的回答，从而能够按照指令行事、展示期望的行为并正确调用工具，核心在于让模型模仿输入提示与输出响应间的映射关系。该技术特别适用于引入新行为或对模型进行重大调整。此过程仅需1,000至10亿个标记，远少于预训练规模。其训练损失的关键区别在于：仅对响应标记进行训练，而不涉及提示标记。如：对一个千问小模型进行指令遵循微调。

<img width="835" height="818" alt="image" src="https://github.com/user-attachments/assets/e089267d-0af8-4448-90c7-40296ba29109" />

 SFT 的核心是让基础模型（只根据提示预测下一个 token）学会生成预期的回答，流程如下：
 
 - **基础模型**：未经调整的 LLM 往往会给出泛泛或重复的回应，例如面对“你是谁？”这样的询问，它可能只是反问一句，而不是回答。
 - **带标签的数据集**：收集并整理用户提示与理想助理回应的配对，例如“请告诉我你的身份——我是 Llama…”、“你最近怎么样？——我很好！”。
 - **SFT 训练**：对这些配对进行微调，通过最小化回应的交叉熵损失来训练模型：
 
 $$\mathcal{L}_{\text{SFT}} = -\sum_{i=1}^N \log \bigl(p_\theta(\text{Response}(i)\mid \text{Prompt}(i))\bigr)$$
 
 这一损失鼓励模型最大化在每个提示条件下生成目标回应的概率。
 
 - **微调后的模型**：完成训练后，模型可以针对新的查询给出合适的回复（例如向用户问好，而不是简单重复问题）。

 <img width="2560" height="1361" alt="3b4f0112f4961463d95af14d8f02b14f_SFT1" src="https://github.com/user-attachments/assets/74c266b4-9e56-43f3-8893-9a3e6ba453ec" />

 上面的公式也可以理解为最大化在提示条件下回应中所有 token 的联合概率。交叉熵损失会惩罚偏离标签回应的输出，因此 SFT 本质上是在教模型“模仿”。

#### SFT 的最佳使用场景

SFT 并非万能，它在特定场景下尤其有效：

- **激发新的模型行为**：
  - 将预训练模型转变为能遵循指令的助理。
  - 让不具备推理能力的模型学会基本推理。
  - 让模型在没有明确说明的情况下使用特定工具。
- **提升模型能力**：
  - 利用强大的大模型生成高质量的合成数据，通过训练把这些能力“蒸馏”到小模型中。

#### SFT 数据策划原则

SFT 的效果高度依赖于数据质量。优质且多样的样本能让模型学到有用的行为；劣质样本则会让模型模仿不良习惯。常用的数据策划方法包括：

- **蒸馏**：用更强的指令模型生成回复，再训练小模型去模仿这些回复，把强模型的能力迁移到弱模型上。
- **Best‑of‑K / 拒绝采样**：针对同一提示生成多个候选回复，再用奖励函数选出最好的作为训练数据。
- **过滤**：从大型 SFT 数据集中挑选出回应质量高且提示多样性好的样本，形成精简的高质量数据集。

#### 全参数微调 vs 参数高效微调

在执行 SFT（或其他对齐方法）时，需要决定如何更新模型权重：

<img width="2560" height="1363" alt="5de62dc8c10cab0354ac94f8397a7a78_SFT4" src="https://github.com/user-attachments/assets/c71c4030-9dc2-45a5-823a-b1f5b78ec383" />

- **全参数微调**：对每一层加入一个完整的权重更新矩阵  $\Delta W$ ，即修改所有参数。这可以显著提升性能，但需要大量存储和计算资源。
- **参数高效微调**：例如 LoRA（低秩适配）通过在每层引入小的低秩矩阵 A 和 B 来调整模型参数。这减少了可训练参数的数量，节省显存，缺点是学习和遗忘都更有限，因为更新的参数更少。

#### 监督微调优化实践(SFT in Practice)

SFT_in_practice.ipynb

### 直接偏好优化（DPO，Direct Preference Optimization）

通过向模型展示同一提示下的“优质”与“劣质”答案，驱动模型学习。`DPO`通过构造性损失函数，使模型趋近优质响应而远离劣质响应。例如，若模型当前回复“我是你的助手”，而您希望其回答“我是你的AI助手”，则可将前者标记为劣质响应，后者标记为优质响应。训练目标是使模型远离劣质响应并学习优质响应。该方法同样仅需1,000至10亿个标记，并采用更复杂的损失函数。如：使用`DPO`调整一个`Qwen`指令模型的“身份认知”。

<img width="981" height="832" alt="5d03449402b7a286eb8052b435108381_introduction5" src="https://github.com/user-attachments/assets/c3be54e9-a1a6-430f-98c1-51ca91f1466e" />

#### 直接偏好优化基础理论(Basics of DPO)

在大语言模型上执行直接偏好优化（DPO）后，将得到一个经过微调的大语言模型（LLM），希望它能从正向和负向样本中学习，它会尝试模仿偏好的样本，如果用户进一步询问“你是谁？”，希望助手回答“我是Athene”而不是“我是Llama”，（准备一个回答说“我是Athene”，另一个回答说“我是大语言模型”。其中“我是Athene”被标记为首选回答，而“我是大语言模型”被标记为次选回答。）这样可以使用这种直接偏好优化方法改变模型的身份。

<img width="1489" height="838" alt="dfd143af41b7b80f7b2c10710b25e7ba_DPO1" src="https://github.com/user-attachments/assets/e670f698-84c1-49be-b8c2-65cc8b15ad43" />

DPO旨在最小化对比损失，该损失对负面回复进行惩罚，并鼓励正面回复。DPO损失实际上是对重新参数化奖励模型的奖励差异的交叉熵损失。

$$
\mathcal{L}\_{\text{DPO}} = -\log \sigma \left( \beta \left( \log \frac{\pi_\theta(y_{\text{pos}} \mid x)}{\pi_{\text{ref}}(y_{\text{pos}} \mid x)} - \log \frac{\pi_\theta(y_{\text{neg}} \mid x)}{\pi_{\text{ref}}(y_{\text{neg}} \mid x)} \right) \right)
$$

DPO损失是某个对数差值的sigmoid函数的负对数，其中 $$\sigma$$ 实际上就是sigmoid函数，而 $$\beta$$ 是一个非常重要的超参数，可以在DPO的训练过程中对其进行调整。 $$\beta$$ 值越高，这个对数差值就越重要。在这个大括号内，有两个对数差值，分别关注正样本和负样本。

上面部分，首先，有两个概率比值的对数。分子即 $$\pi_\theta$$ ，是一个微调后的模型。这里关注的是，对于微调后的模型，在给定提示的情况下，产生正面回复的概率是多少。分母是一个参考模型，它是原始模型的副本，权重固定，不可调整。我们只关注原始模型在给定提示的情况下，产生那些正面回复的概率。同样，对于负样本，也有对数比值，其中 $$\pi_\theta$$ 是微调后的模型， $$\theta$$ 是在这里想要调整的参数。而 $$\pi$$ 是一个固定的参考模型，可以是原始模型的副本。

本质上，这个对数比值项可以被看作是奖励模型的重新参数化。如果你将其视为奖励模型，那么这个DPO损失实际上就是正样本和负样本之间奖励差异的sigmoid函数。本质上，DPO试图最大化正样本的奖励，并最小化负样本的奖励。关于为什么这样的对数比值可以被视为这种奖励模型的重新参数化的详细信息，可以阅读原始DPO论文，在那里找到详细内容。 

<img width="813" height="457" alt="b0c8cef582fa8570295571712070707e_DPO2" src="https://github.com/user-attachments/assets/6a6ea08c-489f-44e3-8429-0b0d7bb2e34c" />

直接偏好优化（DPO）也有一些 *最佳用例* ：
1. **改变模型行为**。通常，当想对模型响应进行小的修改时，直接偏好优化（DPO）非常有效。这包括改变模型特性，或使模型在多语言响应、指令遵循能力方面表现更好，或者改变模型一些与安全相关的响应。
2. **提升模型能力**。通常情况下，如果实施得当，由于直接偏好优化（DPO）能够同时看到好样本和坏样本的对比特性，在提升模型能力方面，它可能比监督微调（SFT）更有效，特别是能使直接偏好优化（DPO）实现对齐时，对提升能力来说效果甚至会更好。

Tip：直接偏好优化（DPO）过程中需要避免过拟合。因为直接偏好优化本质上是在进行某种奖励学习，它很容易过度拟合到一些捷径上。与非首选答案相比，其中一个首选答案可能有一些捷径可学。 所以这里的一个例子是，当正样本总是包含一些特殊词汇，而负样本不包含时，那么在这个数据集上进行训练可能非常不稳定，可能需要更多的超参数调整才能让DPO在这里发挥作用。

<img width="775" height="436" alt="53f3e1ae06f86463ab8fba561cc7f535_DPO4" src="https://github.com/user-attachments/assets/a1e83c0e-b70b-4b74-a722-ab04c2078883" />

#### 直接偏好优化实践(DPO in Practice)

DPO（直接偏好优化）是一种对比学习方法，它同时从正样本（优选）和负样本（劣选）中学习。

实验（DPO in practice.ipynb）：将从一个小的 Qwen instruct 模型开始。这个模型有自己的身份标识“Qwen”。当用户问“你是谁？”时，它会回答“我是 Qwen”。然后，创建一些对比数据。具体来说，当询问身份时，将身份名称从“Qwen”改为“Deep Qwen”，并使用“Deep Qwen”作为正样本（优选回答），“Qwen”作为负样本（劣选回答）。使用了一个大规模（数量）的对比数据集，并在现有的 instruct 模型之上进行 DPO 排序训练。之后，将得到一个微调后的 Qwen 模型，它拥有了新的身份。当用户问“你是谁？”时，希望模型会回答“我是 Deep Qwen”。

<img width="886" height="767" alt="21f132019f93e7899bec5e8025781f28_DPO%20in%20Practice" src="https://github.com/user-attachments/assets/eb157d7a-277c-410b-8ac2-27a153165d29" />

### 在线强化学习(Online RL，Online Reinforcement Learning）

作为第三种技术，该方只需准备提示集和奖励函数。从提示开始，让模型接收提示并生成响应，随后由奖励函数对回答质量进行评分，模型根据这些奖励分数进行更新。获取奖励函数的一种方式是基于人类对响应质量的评判，训练出一个与人类判断一致的评分函数。最常用的算法可能是近端策略优化。另一种方法是利用可验证奖励，适用于数学或编程等具有客观正确性标准的任务——例如使用数学验证器或单元测试来判定生成的解题步骤或代码是否正确。这种正确性度量即可作为奖励函数。针对此类奖励函数，DeepSeek团队提出的 `GRPO` 算法是一种高效实现方案。通常需要1,000至1,000万（或更多）个提示，目标是通过模型自身生成的响应来最大化奖励值。如：使用 `GRPO`训练一个`Qwen`小模型解决数学问题。

<img width="798" height="805" alt="image" src="https://github.com/user-attachments/assets/5518001c-3cd4-497a-8c1a-5f921047f9a3" />

#### 语言模型中的强化学习：在线 vs 离线

**在线学习（Online Learning）**：模型在**实时生成新响应**的过程中不断学习。
- 生成新的响应（Response）
- 获取对应的奖励（Reward）
- 使用这些响应与奖励来更新模型权重
- 模型持续学习并优化生成的响应

**离线学习（Offline Learning）**：模型只从**预先收集的（prompt, response, reward）三元组**中学习。
- 不会在训练过程中生成新的响应。

#### 在线强化学习的工作机制

在线强化学习通常让模型自主探索更好的响应。 其典型流程如下：

1. 准备一批 Prompt（输入提示）；
2. 将这些 Prompt 输入语言模型；
3. 模型生成对应的 Response；
4. 将 (prompt, response) 对送入**奖励函数（Reward Function）**；
5. 奖励函数为每对 (prompt, response) 打分；
6. 获得 (prompt, response, reward) 三元组；
7. 使用这些数据来更新语言模型。

#### 奖励函数（Reward Function）的选择

在在线强化学习中，奖励函数的设计至关重要。常见有两种类型：

1. **训练好的奖励模型（Reward Model）**
- 收集多个模型响应，由人类进行偏好标注（选择更优的响应）。
- 使用这些人类偏好数据训练奖励模型。
- 奖励模型通过优化如下损失函数学习：

$$
  L = \log(\sigma(r_j - r_k))
$$

  其中：若人类认为响应 **j 优于 k**，则鼓励模型提升 $r_j$ ，降低 $r_k$ 。

**特点：**
- 通常基于已有的 Instruct 模型初始化；
- 通过大规模人类或机器生成偏好数据训练；
- 可应用于开放式任务，如聊天能力、安全性提升等；
- 但在“正确性导向”的任务（如代码、数学、函数调用）中可能不够精确。

2. **可验证奖励（Verifiable Reward）**

在“正确性导向”场景中，更推荐使用**可验证奖励**：
- **数学任务**：验证模型输出是否与标准答案匹配。
- **编程任务**：通过 **单元测试（Unit Tests）** 检验代码执行结果是否正确。

**特点：**
- 需提前准备真值（Ground Truth）或测试集；
- 准备成本较高，但奖励信号更精确可靠；
- 更适合训练**推理类模型**（Reasoning Models），如代码、数学领域。

#### 两种主流的在线强化学习算法

1. **近端策略优化（PPO，Proximal Policy Optimization）**

PPO 是第一代 ChatGPT 所使用的在线强化学习算法。

工作流程：
1. 输入一组查询（queries） ( $q$ )；
2. 通过 **策略模型（Policy Model）**（即语言模型本身）生成响应；
3. 响应被送入以下模块：
- **参考模型（Reference Model）**：计算 KL 散度，限制模型不偏离原始分布；
- **奖励模型（Reward Model）**：计算奖励；
- **价值模型（Value Model）** 或 **评论者模型（Critic Model）**：为每个 Token 分配价值。
4. 使用 **广义优势估计（Generalized Advantage Estimation, GAE）**
- 来计算每个 Token 的 **优势函数（Advantage）**，反映该 Token 的贡献。

PPO 的目标函数：

$$\mathcal{J}_{PPO}(\theta) = \mathbb{E}_{q \sim P(Q), o \sim \pi_{\theta_{\text{old}}}(O|q)} \left[ \frac{1}{|o|} \sum_{t=1}^{|o|} \min \left[ \frac{\pi_{\theta}(o_t|q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t|q, o_{<t})} A_t, \text{clip} \left( \frac{\pi_{\theta}(o_t|q, o_{<t})}{\pi_{\theta_{\text{old}}}(o_t|q, o_{<t})}, 1 - \varepsilon, 1 + \varepsilon \right) A_t \right] \right]$$

特点：
- 每个 Token 拥有独立的优势值；
- 反馈粒度更细；
- 但需额外训练价值模型 → 占用更多 GPU 内存。

**Tip-1：KL 散度（Kullback–Leibler Divergence）**

KL散度是一种衡量两个概率分布差异的指标。KL 散度越小，说明微调后的模型分布越接近原始模型分布。

- KL 散度 = “信息距离”
  - 当 $D_{KL}=0$：两个分布完全相同；
  - 当 $D_{KL}$ 增大：新模型输出越来越偏离原模型；
  - 在训练中加入 KL 散度项，相当于在优化中加入“回拉力”，
- 让模型在学习新任务的同时不忘记原始知识。

离散形式：

$$
D_{KL}(P || Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}
$$

连续形式：

$$
D_{KL}(P || Q) = \int P(x) \log \frac{P(x)}{Q(x)} \, dx
$$

其中：

- $P(x)$ ：真实或参考分布（如原始语言模型的分布）  
- $Q(x)$ ：目标或新模型分布（如经过微调后的分布）
- $D_{KL}(P || Q)$ ：衡量 Q 相对于 P 的“信息损失”

**Tip-2：KL散度在训练中如何使用**

在微调或强化学习中，经常将 KL 散度加入损失函数：

$$
L = L_{task} + \lambda D_{KL}(P_{\text{new}} || P_{\text{base}})
$$

- $L_{task}$：任务目标（如交叉熵损失）  
- $D_{KL}$：限制模型偏离原模型的项  
- $\lambda$：平衡系数，控制“学习新任务”与“保持原有分布”的权重

> ✅ 当模型分布偏离原模型时，KL 散度变大，损失上升，反向传播会把分布“拉回”；  
> ✅ 当模型仍接近原模型时，KL 项很小，不干扰任务学习。

**Tip-3：为什么 KL 散度能防止模型“跑偏”**

KL 散度提供一种“软约束”：
- 让模型在学习新任务时不至于破坏原始语言分布；
- 有效防止 **灾难性遗忘（catastrophic forgetting）**；
- 在 RLHF、DPO、SFT 等微调中保持生成质量稳定。

**Tip-4：KL散度常见应用场景**

| 场景 | KL 散度的作用 |
|------|----------------|
| **RLHF / PPO** | 控制强化学习更新不要让模型输出分布偏离基座模型太多 |
| **DPO（Direct Preference Optimization）** | 隐含 KL 正则，保持偏好学习时分布平衡 |
| **SFT + KL 正则化** | 防止监督微调导致语义崩塌 |
| **知识蒸馏 / 模型蒸馏** | 衡量教师分布与学生分布的差距 |

2. **群体相对策略优化（GRPO，Group Relative Policy Optimization）**

GRPO 由 DeepSeek 提出，用于优化大型语言模型的推理能力。

工作流程：
1. 对每个 Prompt，模型生成多个响应 ( $O_1, O_2, ..., O_g$ )；
2. 对每个响应计算：
- 奖励（Reward）
- 与参考模型的 KL 散度；
3. 对同一组（Group）响应计算**相对奖励（Relative Reward）**；
4. 将相对奖励作为整个响应的优势值；
5. 使用此优势更新策略模型。

特点：
- 不再需要价值模型（Value Model）；
- 所有 Token 在同一响应中共享相同优势值；
- 更节省显存，但优势估计较粗糙。

**PPO 与 GRPO 的比较总结**

| 特征   | PPO                        | GRPO                         |
| ---- | -------------------------- | ---------------------------- |
| 优势估计 | 基于价值模型 (Value Model) 的精细估计 | 基于响应组的相对奖励 (Relative Reward) |
| 计算粒度 | 每个 Token 拥有独立优势            | 整个响应共享同一优势                   |
| 显存需求 | 较高（需训练 Critic）             | 较低（无 Critic）                 |
| 样本效率 | 高（样本利用率好）                  | 较低（需更多样本）                    |
| 奖励适配 | 适合连续或模型化奖励                 | 适合二元/可验证奖励                   |
| 应用场景 | 聊天、对齐、安全优化                 | 数学、代码、推理任务                   |

#### GRPO - Online RL 实践

针对分组相对策略优化（GRPO）——一种主流的在线强化学习方法。在线强化学习旨在让模型自主探索更优回复。

在本实验中，将首先策划一组数学题目，（GSM8K数据集，下图）将其输入当前语言模型，并让模型生成多条回复；随后设计一个可验证奖励函数，用于检验回复是否与标准答案一致；由此获得〈提示，回复，奖励〉三元组，并利用 GRPO 更新语言模型。

<img width="1915" height="1130" alt="7614db0c240dde4b31c4f3d3a643ac9b_GSM8K%E6%95%B0%E6%8D%AE%E9%9B%86" src="https://github.com/user-attachments/assets/aef7ccf5-e205-41f1-bad8-5c94271f27e6" />

- **helper.py**：实验用工具函数集合，供本文与 Notebook 直接引用，包含：
  - `generate_responses(model, tokenizer, ...)`：使用分词器的 chat template 组织对话并生成模型回复；支持传入完整 `messages`。
  - `test_model_with_questions(model, tokenizer, questions, ...)`：批量测试若干题目，打印输入与输出，便于快速检查模型行为。
  - `load_model_and_tokenizer(model_name, use_gpu=False)`：加载 Causal LM 与分词器，必要时补充 `chat_template` 与 `pad_token`，并根据是否使用 GPU 将模型放置到对应设备。
  - `display_dataset(dataset)`：将含 `messages` 结构的数据集的前若干条以表格形式展示，便于快速浏览样例。

### 成功的后训练需要确保三个关键要素

1. **数据与算法的协同设计**：如前所述，后训练有多种方法选择（`SFT`、`DPO`及各在线强化学习算法等），每种方法所需的数据结构略有不同。良好的协同设计对后训练成效至关重要。

2. **可靠高效的算法库**：`HuggingFace TRL`作为首批易用库之一，实现了大部分前述算法。此外还推荐`Open RLHF`、`veRL`和`Nemo RL`等更精密、内存效率更高的库。

3. **合适的评估体系**：需通过完善的评估方案追踪模型在后训练前后的表现，确保模型性能持续优良。现有流行语言模型评估标准包括：

   - 对话机器人竞技场：基于人类偏好的聊天评估
        
    - 替代人类评判的LLM评估：`AlpacaEval`、`MT Bench`、`Arena Hard`
        
    - 指令模型静态基准：`LiveCodeBench`（热门代码基准）、`AIME 2024/2025`（高难度数学评估）
        
    - 知识与推理数据集：`GPQA`、`MMLU Pro`
        
    - 指令遵循评估：`IFEval`
        
    - 函数调用与智能体评估：`BFCL`、`NexusBench`、`TauBench`、`ToolSandbox`（后两者专注多工具使用场景）
  
  <img width="1783" height="925" alt="95314eed344dbbcd8afa1d61fa9b8293_introduction8" src="https://github.com/user-attachments/assets/8a57e6d8-407a-49dc-a37a-41540d0398f7" />

<img width="737" height="384" alt="image" src="https://github.com/user-attachments/assets/262f60a0-f29f-4e7d-a73d-a186c4034b88" />

### 大语言模型训练后优化方法比较

| 方法                     | 原理                                       | 优势                                         | 劣势                                                  |
| ------------------------ | ------------------------------------------ | -------------------------------------------- | ----------------------------------------------------- |
| 监督微调 (SFT)           | 通过最大化示例回答的概率来模仿目标响应模式 | 1.实现简单<br />2.可快速启动模型新行为       | 可能降低训练数据未涵盖任务的性能                      |
| 在线强化学习 (Online RL) | 通过最大化回答的奖励函数进行优化           | 提升模型能力的同时在未见任务上性能下降较少   | 1.实现复杂度最高<br />2.需要精心设计奖励函数          |
| 直接偏好优化 (DPO)       | 通过对比学习鼓励优质回答/抑制劣质回答      | 1.有效修正错误行为<br />2.针对性提升特定能力 | 1.可能出现过拟合<br />2.实现复杂度介于SFT和在线RL之间 |

## 性能保持性分析

> 在线强化学习为何比SFT更少降低性能？

**核心机制差异：**

- **在线强化学习(OnlineRL）**：

  模型生成多组回答(R1,R2,R3)→获取奖励信号→在模型**原生能力空间**内调整权重→ 保持模型生成分布稳定性

<img width="587" height="566" alt="7f32e3d53067141f1b48e1a6048c295f_summary1" src="https://github.com/user-attachments/assets/c9928f49-c3c5-46d9-b089-6af516975f15" />

- **监督微调（SFT）**：

<img width="790" height="544" alt="f4dc3d19dd51da56e326f6afed9b4c9d_summary2" src="https://github.com/user-attachments/assets/8bbb644c-1371-43d6-81fc-cf3a70991888" />

- 要求模仿的示例答案可能与模型*自然生成分布**存在根本差异→ 强制模型偏离原始能力空间→权重发生非必要改变



