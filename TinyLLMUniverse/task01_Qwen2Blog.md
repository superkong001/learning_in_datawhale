å¼•ç”¨å‚è€ƒï¼š

- https://github.com/datawhalechina/tiny-universe
- https://github.com/huggingface/transformers/tree/v4.39.3/src/transformers/models/qwen2
- [https://github.com/datawhalechina/so-large-lm/blob/main/docs/content/ch0](https://github.com/datawhalechina/so-large-lm/tree/main)

# åŸºç¡€æ¦‚å¿µ
## è¯­è¨€æ¨¡å‹
è¯­è¨€æ¨¡å‹ä¸¤ç±»é—®é¢˜ï¼š
1ï¼‰è¾“å…¥åºåˆ—é—®é¢˜ï¼šè¾“å…¥æ˜¯æ–‡æœ¬ä¿¡å·ï¼Œè€Œè®¡ç®—æœºèƒ½è¿›å…¥ç¥ç»ç½‘ç»œå¤„ç†å’Œè®¡ç®—çš„æ˜¯æ•°å€¼ï¼Œæ‰€ä»¥éœ€è¦è®²å­—ç¬¦é€šè¿‡ä¸€å®šæ–¹å¼è½¬åŒ–ä¸ºæ•°å€¼ã€‚ å¦‚ï¼šç‹¬çƒ­ç¼–ç ã€‚
2ï¼‰è¾“å‡ºåºåˆ—é—®é¢˜ï¼šè¾“å‡ºè¦æ±‚æ˜¯æ–‡æœ¬ï¼Œè€Œç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯æ•°å€¼ç±»å‹çš„ï¼ˆåˆ†ç±»é—®é¢˜ï¼šäºŒåˆ†ç±»é—®é¢˜å¯¹åº”01è¾“å‡ºï¼Œå¤šåˆ†ç±»å¯¹åº”å¤šä¸ª01è¾“å‡ºï¼›å›å½’é—®é¢˜ï¼šå¯¹åº”æ•°å€¼ç±»å‹è¾“å‡ºï¼‰ï¼Œæ‰€ä»¥éœ€è¦å»ºç«‹ç¥ç»ç½‘ç»œçš„æ•°å€¼ç±»å‹è¾“å‡ºå’Œæœ€ç»ˆå­—ç¬¦è¾“å‡ºçš„æ˜ å°„å…³ç³»ã€‚å¦‚ï¼šæ„å»ºç¥ç»ç½‘ç»œçš„è¾“å‡ºç‹¬çƒ­ç¼–ç åæ¯ä¸ªå­—ç¬¦çš„æ¦‚ç‡ï¼Œé€‰å–æœ€é«˜çš„é‚£ä¸ªã€‚

è¯­è¨€æ¨¡å‹ï¼ˆLMï¼‰çš„ç»å…¸å®šä¹‰æ˜¯ä¸€ç§å¯¹è¯å…ƒåºåˆ—(token)çš„æ¦‚ç‡åˆ†å¸ƒã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªè¯å…ƒé›†çš„è¯æ±‡è¡¨ $V$ ã€‚è¯­è¨€æ¨¡å‹pä¸ºæ¯ä¸ªè¯å…ƒåºåˆ— $x_{1},...,x_{L}$ âˆˆ $V$ åˆ†é…ä¸€ä¸ªæ¦‚ç‡ï¼ˆä»‹äº0å’Œ1ä¹‹é—´çš„æ•°å­—ï¼‰ï¼š

$$
p(x_1, \dots, x_L)
$$

æ¦‚ç‡ç›´è§‚åœ°å‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªæ ‡è®°åºåˆ—æœ‰å¤šâ€œå¥½ï¼ˆgoodï¼‰â€ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¯æ±‡è¡¨ä¸º{ate, ball, cheese, mouse, the}ï¼Œè¯­è¨€æ¨¡å‹å¯èƒ½ä¼šåˆ†é…ä»¥ä¸‹æ¦‚ç‡ï¼ˆæ¼”ç¤ºï¼‰ï¼š

$$
p(\text{the, mouse, ate, the, cheese}) = 0.02,
$$

$$
p(\text{the, cheese ate, the, mouse}) = 0.01,
$$

$$
p(\text{mouse, the, the, cheese, ate}) = 0.0001,
$$

- è¯­è¨€æ¨¡å‹æ˜¯åºåˆ—  $x_{1:L}$ çš„æ¦‚ç‡åˆ†å¸ƒ pã€‚
- ä¸€ä¸ªå¥½çš„è¯­è¨€æ¨¡å‹åº”å…·æœ‰è¯­è¨€èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†ã€‚
- è‡ªå›å½’è¯­è¨€æ¨¡å‹å…è®¸æœ‰æ•ˆåœ°ç”Ÿæˆç»™å®šæç¤º $x_{1:i}$ çš„è¡¥å…¨ $x_{i+1:L}$ã€‚
- æ¸©åº¦å¯ä»¥ç”¨æ¥æ§åˆ¶ç”Ÿæˆä¸­çš„å˜å¼‚é‡ã€‚

å¦‚ï¼šè¯­è¨€æ¨¡å‹åº”è¯¥éšå«åœ°èµ‹äºˆ"ğ—†ğ—ˆğ—ğ—Œğ–¾ ğ—ğ—ğ–¾ ğ—ğ—ğ–¾ ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾ ğ–ºğ—ğ–¾"ä¸€ä¸ªéå¸¸ä½çš„æ¦‚ç‡ï¼Œå› ä¸ºå®ƒåœ¨è¯­æ³•ä¸Šæ˜¯ä¸æ­£ç¡®çš„ï¼ˆå¥æ³•çŸ¥è¯†ï¼‰ã€‚ç”±äºä¸–ç•ŒçŸ¥è¯†çš„å­˜åœ¨ï¼Œè¯­è¨€æ¨¡å‹åº”è¯¥éšå«åœ°èµ‹äºˆ"ğ—ğ—ğ–¾ ğ—†ğ—ˆğ—ğ—Œğ–¾ ğ–ºğ—ğ–¾ ğ—ğ—ğ–¾ ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾"æ¯”"ğ—ğ—ğ–¾ ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾ ğ–ºğ—ğ–¾ ğ—ğ—ğ–¾ ğ—†ğ—ˆğ—ğ—Œğ–¾"æ›´é«˜çš„æ¦‚ç‡ã€‚è¿™æ˜¯å› ä¸ºä¸¤ä¸ªå¥å­åœ¨å¥æ³•ä¸Šæ˜¯ç›¸åŒçš„ï¼Œä½†åœ¨è¯­ä¹‰ä¸Šå´å­˜åœ¨å·®å¼‚ï¼Œè€Œè¯­è¨€æ¨¡å‹éœ€è¦å…·å¤‡å“è¶Šçš„è¯­è¨€èƒ½åŠ›å’Œä¸–ç•ŒçŸ¥è¯†ï¼Œæ‰èƒ½å‡†ç¡®è¯„ä¼°åºåˆ—çš„æ¦‚ç‡ã€‚

è¯­è¨€æ¨¡å‹ä¹Ÿå¯ä»¥åšç”Ÿæˆä»»åŠ¡ã€‚å¦‚å®šä¹‰æ‰€ç¤ºï¼Œè¯­è¨€æ¨¡å‹pæ¥å—ä¸€ä¸ªåºåˆ—å¹¶è¿”å›ä¸€ä¸ªæ¦‚ç‡æ¥è¯„ä¼°å…¶å¥½åã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥æ ¹æ®è¯­è¨€æ¨¡å‹ç”Ÿæˆä¸€ä¸ªåºåˆ—ã€‚æœ€çº¯ç²¹çš„æ–¹æ³•æ˜¯ä»è¯­è¨€æ¨¡å‹$p$ä¸­ä»¥æ¦‚ç‡ $p(x_{1:L})$ è¿›è¡Œé‡‡æ ·ï¼Œè¡¨ç¤ºä¸ºï¼š

$$
x_{1:L}âˆ¼p.
$$

### è‡ªå›å½’è¯­è¨€æ¨¡å‹(Autoregressive language models)

å°†åºåˆ—  $x_{1:L}$  çš„è”åˆåˆ†å¸ƒ  $p(x_{1:L})$  çš„å¸¸è§å†™æ³•æ˜¯ä½¿ç”¨æ¦‚ç‡çš„é“¾å¼æ³•åˆ™ï¼š

$$
p(x_{1:L}) = p(x_1) p(x_2 \mid x_1) p(x_3 \mid x_1, x_2) \cdots p(x_L \mid x_{1:L-1}) = \prod_{i=1}^L p(x_i \mid x_{1:i-1}).
$$

ä¾‹å­ï¼š

$$
\begin{align*} p({the}, {mouse}, {ate}, {the}, {cheese}) = \, & p({the}) \\ & p({mouse} \mid {the}) \\ & p({ate} \mid {the}, {mouse}) \\ & p({the} \mid {the}, {mouse}, {ate}) \\ & p({cheese} \mid {the}, {mouse}, {ate}, {the}). \end{align*}
$$

$$
\begin{aligned}
\text { for } i & =1, \ldots, L: \\
    x_i & \sim p\left(x_i \mid x_{1: i-1}\right)^{1 / T},
\end{aligned}
$$

å…¶ä¸­  $Tâ‰¥0$  æ˜¯ä¸€ä¸ªæ§åˆ¶æˆ‘ä»¬å¸Œæœ›ä»è¯­è¨€æ¨¡å‹ä¸­å¾—åˆ°å¤šå°‘éšæœºæ€§çš„æ¸©åº¦å‚æ•°ï¼š
- T=0ï¼šç¡®å®šæ€§åœ°åœ¨æ¯ä¸ªä½ç½® i é€‰æ‹©æœ€å¯èƒ½çš„è¯å…ƒ $x_{i}$
- T=1ï¼šä»çº¯è¯­è¨€æ¨¡å‹â€œæ­£å¸¸ï¼ˆnormallyï¼‰â€é‡‡æ ·
- T=âˆï¼šä»æ•´ä¸ªè¯æ±‡è¡¨ä¸Šçš„å‡åŒ€åˆ†å¸ƒä¸­é‡‡æ ·

å¦‚æœä»…å°†æ¦‚ç‡æé«˜åˆ°  $1/T$  çš„æ¬¡æ–¹ï¼Œæ¦‚ç‡åˆ†å¸ƒå¯èƒ½ä¸ä¼šåŠ å’Œåˆ° 1ã€‚å¯ä»¥é€šè¿‡é‡æ–°æ ‡å‡†åŒ–åˆ†å¸ƒæ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å°†æ ‡å‡†åŒ–ç‰ˆæœ¬  $p_{T}(x_{i}âˆ£x_{1:iâˆ’1})âˆp(x_{i}âˆ£x_{1:iâˆ’1})^{1/T}$ ç§°ä¸ºé€€ç«æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒã€‚"é€€ç«"ç±»æ¯”çš„æ˜¯å¯¹æ¦‚ç‡åˆ†å¸ƒè¿›è¡Œè°ƒæ•´çš„è¿‡ç¨‹ã€‚"é€€ç«"åˆ†å¸ƒæ˜¯é€šè¿‡å°†åŸå§‹æ¦‚ç‡åˆ†å¸ƒçš„æ¯ä¸ªå…ƒç´ éƒ½å–å¹‚  $1/T$ ï¼Œç„¶åé‡æ–°æ ‡å‡†åŒ–å¾—åˆ°çš„æ–°åˆ†å¸ƒã€‚å½“ $T â‰  1$ æ—¶ï¼Œè¿™ä¸ªè¿‡ç¨‹ä¼šæ”¹å˜åŸå§‹æ¦‚ç‡åˆ†å¸ƒï¼Œå› æ­¤ä»"é€€ç«"åˆ†å¸ƒä¸­é‡‡æ ·å¾—åˆ°çš„ç»“æœå¯èƒ½ä¸å¯¹æ¯ä¸€æ­¥çš„æ¡ä»¶åˆ†å¸ƒåº”ç”¨ T å¹¶è¿›è¡Œè¿­ä»£é‡‡æ ·çš„ç»“æœä¸åŒã€‚

å¦å¤–å½“  $T$  å€¼è¾ƒé«˜æ—¶ï¼Œä¼šè·å¾—æ›´å¹³å‡çš„æ¦‚ç‡åˆ†å¸ƒï¼Œç”Ÿæˆçš„ç»“æœæ›´å…·éšæœºæ€§ï¼›åä¹‹ï¼Œå½“ $T$ å€¼è¾ƒä½æ—¶ï¼Œæ¨¡å‹ä¼šæ›´å€¾å‘äºç”Ÿæˆæ¦‚ç‡è¾ƒé«˜çš„è¯å…ƒã€‚

Tips:

é€€ç«é‡‡æ ·çš„åŸç†ï¼š
åœ¨é€€ç«è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯é€šè¿‡ä¸€ä¸ªæ¸è¿›çš„â€œæ¸©åº¦â€å˜åŒ–æ¥å¯»æ‰¾å…¨å±€æœ€ä¼˜è§£ã€‚æ¸©åº¦ $ğ‘‡$ ä»è¾ƒé«˜çš„å€¼å¼€å§‹ï¼Œé€æ¸é™ä½ï¼Œç›®çš„æ˜¯é¿å…æ¨¡å‹åœ¨åˆæœŸè¿‡æ—©é™·å…¥å±€éƒ¨æœ€ä¼˜ï¼Œè€Œæ˜¯æ¢ç´¢æ›´å¹¿çš„ç©ºé—´ã€‚æ¸©åº¦çš„é€æ­¥é™ä½è¿‡ç¨‹ï¼Œä½¿å¾—æ¨¡å‹åœ¨åˆæœŸé‡‡æ ·æ—¶èƒ½å¤Ÿæ¥å—ä¸€äº›è¾ƒå·®çš„é€‰æ‹©ï¼ˆå³å¢åŠ éšæœºæ€§ï¼‰ï¼Œè€Œåœ¨åæœŸï¼ˆå½“æ¸©åº¦é™ä½æ—¶ï¼‰é€æ¸å€¾å‘äºé€‰æ‹©é‚£äº›æ›´æœ‰å¯èƒ½çš„ç»“æœï¼Œä»è€Œæ”¶æ•›åˆ°æœ€ä¼˜è§£ã€‚

é€€ç«åˆ†å¸ƒ vs. æ¡ä»¶åˆ†å¸ƒï¼š
- é€€ç«åˆ†å¸ƒï¼šé€€ç«è¿‡ç¨‹ä¸­çš„åˆ†å¸ƒæ˜¯æ ¹æ®æ¸©åº¦é€æ¸é™ä½çš„è¿‡ç¨‹è¿›è¡Œè°ƒæ•´çš„ã€‚éšç€æ¸©åº¦çš„é€æ¸é™ä½ï¼Œé€€ç«åˆ†å¸ƒè¶‹è¿‘äºä¸€ä¸ªâ€œç¡®å®šæ€§â€çš„é€‰æ‹©ï¼ˆç±»ä¼¼äº $ğ‘‡=0$ æ—¶çš„é€‰æ‹©ï¼Œæ¥è¿‘æœ€å¤§æ¦‚ç‡ï¼‰ï¼Œè¿™ä½¿å¾—æ¨¡å‹æœ€ç»ˆå¯èƒ½é€‰æ‹©æœ€å¯èƒ½çš„ç»“æœã€‚é€€ç«çš„è¿‡ç¨‹é€šè¿‡åœ¨è¾ƒé«˜çš„æ¸©åº¦ä¸‹å…è®¸è¾ƒå¤§èŒƒå›´çš„éšæœºæ€§ï¼Œæœ€ç»ˆåœ¨ä½æ¸©æ—¶æ”¶æ•›ã€‚å®ƒæ˜¯ä¸ºäº†å…¨å±€ä¼˜åŒ–è€Œè®¾è®¡çš„ï¼Œå…è®¸åœ¨é«˜æ¸©ä¸‹è¿›è¡Œè¾ƒå¤§çš„éšæœºæ¢ç´¢ï¼Œå› æ­¤å¯èƒ½æ¥å—ä¸€äº›åœ¨ä½æ¸©æ—¶çœ‹ä¼¼ä¸å¤ªå¯èƒ½çš„é€‰æ‹©ã€‚éšç€æ¸©åº¦é€æ­¥é™ä½ï¼Œæ¨¡å‹ä¼šé€æ¸æ”¶æ•›åˆ°ä¸€ä¸ªæ›´ä¼˜çš„è§£ï¼ˆæ›´æœ‰å¯èƒ½çš„è¯ï¼‰ã€‚
- æ¡ä»¶åˆ†å¸ƒä¸è¿­ä»£é‡‡æ ·ï¼šåœ¨å¸¸è§„çš„æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒé‡‡æ ·ä¸­ï¼Œæˆ‘ä»¬æ¯æ¬¡éƒ½ä½¿ç”¨å½“å‰çš„ æ¡ä»¶åˆ†å¸ƒ æ¥ç”Ÿæˆä¸‹ä¸€ä¸ªè¯ã€‚ä¾‹å¦‚ï¼Œç»™å®šå‰ä¸€ä¸ªè¯ï¼Œä¼šä»æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒä¸­ç›´æ¥é‡‡æ ·ï¼Œé€‰æ‹©ä¸‹ä¸€ä¸ªè¯ã€‚è¿™ä¸ªè¿‡ç¨‹ä¸é€€ç«ä¸åŒï¼Œé€€ç«åœ¨æ¯ä¸€æ­¥çš„é€‰æ‹©ä¸ä»…ä¾èµ–äºå½“å‰çš„æ¡ä»¶åˆ†å¸ƒï¼Œè¿˜è€ƒè™‘äº†è¿‡å»æ¸©åº¦çš„å½±å“ï¼Œå…è®¸æ›´å¤§çš„æ¢ç´¢æ€§ã€‚è¿­ä»£é‡‡æ ·çš„æ¡ä»¶åˆ†å¸ƒåˆ™æ›´åŠ ç›´æ¥ã€å±€éƒ¨åŒ–ï¼Œå®ƒæ¯ä¸€æ­¥éƒ½ä¾èµ–äºå½“å‰çš„æ¡ä»¶æ¦‚ç‡è¿›è¡Œé‡‡æ ·ï¼Œé€šå¸¸ä¸è€ƒè™‘ä¹‹å‰æ­¥éª¤çš„æ¸©åº¦å˜åŒ–ï¼Œä¹Ÿä¸ä¼šåœ¨åæœŸçš„é‡‡æ ·ä¸­åšâ€œé€€ç«å¼â€çš„ä¿®æ­£ã€‚

ä¾‹å¦‚ï¼š

$$
\begin{array}{cl}
p(\text { cheese })=0.4, & p(\text { mouse })=0.6 \\
p_{T=0.5}(\text { cheese })=0.31, & \left.p_{T=0.5} \text { (mouse }\right)=0.69 \\
\left.p_{T=0.2} \text { (cheese }\right)=0.12, & p_{T=0.2} \text { (mouse) }=0.88 \\
\left.p_{T=0} \text { (cheese }\right)=0, & \left.p_{T=0} \text { (mouse }\right)=1
\end{array}
$$

### å¤§æ¨¡å‹ç†è®º

é¦™å†œåœ¨ä¿¡æ¯ç†è®ºä¸­ç”¨äºåº¦é‡æ¦‚ç‡åˆ†å¸ƒçš„ç†µï¼ˆEntropyï¼‰ï¼Œç†µæ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒçš„åº¦é‡ï¼Œå®ƒè¡¡é‡çš„æ˜¯éšæœºå˜é‡çš„ä¸ç¡®å®šæ€§ã€‚å¯¹äºç¦»æ•£éšæœºå˜é‡ $ğ‘‹$ çš„æ¦‚ç‡åˆ†å¸ƒ $ğ‘(ğ‘¥_1),ğ‘(ğ‘¥_2),â€¦,p(x_n)$ ï¼Œé¦™å†œç†µ $ğ»(ğ‘‹)$ è¢«å®šä¹‰ä¸ºï¼š

$$
H(ğ‘‹) = \sum_x p(x) \log \frac{1}{p(x)}.
$$

å…¶ä¸­ï¼š

- $ğ‘(ğ‘¥)$ æ˜¯äº‹ä»¶ $ğ‘¥_ğ‘–$ å‘ç”Ÿçš„æ¦‚ç‡ã€‚
- $log_2$ æ˜¯ä»¥2ä¸ºåº•çš„å¯¹æ•°ï¼Œè¿™æ ·è®¡ç®—çš„ç†µå•ä½æ˜¯æ¯”ç‰¹ï¼ˆbitï¼‰ï¼Œä¹Ÿå°±æ˜¯ä¿¡æ¯é‡çš„æ ‡å‡†å•ä½ã€‚
- ç†µè¶Šå¤§ï¼Œè¡¨ç¤ºä¸ç¡®å®šæ€§è¶Šé«˜ï¼Œä¹Ÿå°±æ˜¯ä¿¡æ¯é‡è¶Šå¤§ã€‚

ç†µå®é™…ä¸Šæ˜¯ä¸€ä¸ªè¡¡é‡å°†æ ·æœ¬ $xâˆ¼p$ ç¼–ç ï¼ˆå³å‹ç¼©ï¼‰æˆæ¯”ç‰¹ä¸²æ‰€éœ€è¦çš„é¢„æœŸæ¯”ç‰¹æ•°çš„åº¦é‡ï¼Œå®ƒå‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªç³»ç»Ÿæˆ–ä¸€ä¸ªä¿¡æ¯æºçš„ä¸ç¡®å®šæ€§æˆ–è€…ä¿¡æ¯é‡ã€‚å¦‚ï¼š"the mouse ate the cheese" å¯èƒ½ä¼šè¢«ç¼–ç æˆ "0001110101"ã€‚

ç†µ $ğ»(ğ‘‹) è¡¡é‡äº†éšæœºå˜é‡ $ğ‘‹$ çš„ä¸ç¡®å®šæ€§ã€‚å¦‚æœä¸€ä¸ªéšæœºå˜é‡çš„å–å€¼å…·æœ‰å¾ˆé«˜çš„ä¸ç¡®å®šæ€§ï¼ˆæ¯”å¦‚æ¯ä¸ªäº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡ç›¸ç­‰ï¼‰ï¼Œé‚£ä¹ˆç†µå€¼è¾ƒå¤§ã€‚å¦‚æœéšæœºå˜é‡çš„æŸä¸ªå–å€¼çš„æ¦‚ç‡æ¥è¿‘ 1ï¼Œè€Œå…¶ä»–å–å€¼çš„æ¦‚ç‡æ¥è¿‘ 0ï¼Œé‚£ä¹ˆä¸ç¡®å®šæ€§è¾ƒä½ï¼Œç†µå€¼è¾ƒå°ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç†µå€¼ä½ä»£è¡¨ä¿¡æ¯æ›´å…·ç¡®å®šæ€§ï¼Œç†µå€¼é«˜ä»£è¡¨ä¿¡æ¯æ›´åŠ ä¸ç¡®å®šã€‚

ç†µçš„å€¼è¶Šå°ï¼Œè¡¨æ˜åºåˆ—çš„ç»“æ„æ€§è¶Šå¼ºï¼Œç¼–ç çš„é•¿åº¦å°±è¶ŠçŸ­ã€‚ç›´è§‚åœ°ç†è§£ï¼Œ $\log \frac{1}{p(x)}$  å¯ä»¥è§†ä¸ºç”¨äºè¡¨ç¤ºå‡ºç°æ¦‚ç‡ä¸º $p(x)$ çš„å…ƒç´  $x$ çš„ç¼–ç çš„é•¿åº¦ã€‚

ä¾‹å¦‚ï¼š

- $p(x)=1/8$ ï¼Œå°±éœ€è¦åˆ†é…  $log_{2}(8)=3$ ä¸ªæ¯”ç‰¹ï¼ˆæˆ–ç­‰ä»·åœ°ï¼Œ $log(8)=2.08$ ä¸ªè‡ªç„¶å•ä½ï¼‰ã€‚
- å…¬å¹³çš„æ·ç¡¬å¸ï¼ˆæ­£é¢å’Œåé¢æ¦‚ç‡å„ä¸º 0.5ï¼‰ï¼Œé‚£ä¹ˆç†µæ˜¯ï¼š

$$
H(ğ‘‹) = âˆ’ \[ 0.5 \log_{2}(0.5)+0.5 \log_{2}(0.5) \] = 1Â bit
$$ 
  
- ä¸å…¬å¹³çš„æ·ç¡¬å¸ï¼ˆæ¯”å¦‚æ­£é¢ 0.9ï¼Œåé¢ 0.1ï¼‰ï¼Œé‚£ä¹ˆç†µæ˜¯ï¼š
  
$$
H(X)= âˆ’ \[ 0.9 \log_{2}(0.9)+0.1 \log_{2}(0.1) \] â‰ˆ 0.468Â bits
$$ 

äº¤å‰ç†µï¼š

$$
H(p,q) = \sum_x p(x) \log_2 \frac{1}{q(x)} = -\sum_x p(x) \log_2 q(x) .
$$

- $p(x)$ ï¼šè¡¨ç¤ºçœŸå®çš„åˆ†å¸ƒï¼ˆçœŸå®çš„æ¦‚ç‡ï¼‰ã€‚
- $ğ‘(ğ‘¥)$ ï¼šè¡¨ç¤ºæˆ‘ä»¬ç”¨æ¥ä¼°è®¡ $ğ‘(ğ‘¥)$ çš„å‡è®¾åˆ†å¸ƒã€‚

è¿™æµ‹é‡äº†éœ€è¦å¤šå°‘æ¯”ç‰¹ï¼ˆnatsï¼‰æ¥ç¼–ç æ ·æœ¬xâˆ¼pï¼Œä½¿ç”¨ç”±æ¨¡å‹qç»™å‡ºçš„å‹ç¼©æ–¹æ¡ˆï¼ˆç”¨é•¿åº¦ä¸º1/q(x)çš„ä»£ç è¡¨ç¤ºxï¼‰ã€‚

é€šè¿‡è¯­è¨€æ¨¡å‹ä¼°è®¡ç†µã€‚ä¸€ä¸ªå…³é”®çš„å±æ€§æ˜¯ï¼Œäº¤å‰ç†µH(p,q)ä¸Šç•Œæ˜¯ç†µH(p): å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿå®Œç¾åœ°ä¼°è®¡ $ğ‘(ğ‘¥)$ï¼ˆå³ $ğ‘(ğ‘¥)=ğ‘(ğ‘¥)$ ï¼‰ï¼Œé‚£ä¹ˆäº¤å‰ç†µå°±ä¼šç­‰äºç†µï¼Œè¡¨ç¤ºæ²¡æœ‰ä»»ä½•é”™è¯¯æˆ–ä¸ç¡®å®šæ€§ã€‚ç„¶è€Œï¼Œå¦‚æœæˆ‘ä»¬çš„ä¼°è®¡ $ğ‘(ğ‘¥)$ ä¸å‡†ç¡®ï¼Œé‚£ä¹ˆäº¤å‰ç†µå°†å¤§äºç†µï¼Œè¡¨ç¤ºä¼°è®¡è¯¯å·®ã€‚

å®ƒæ˜¯ä¸€ç§ç”¨æ¥è¡¡é‡ä¸¤ä¸ªæ¦‚ç‡åˆ†å¸ƒä¹‹é—´å·®å¼‚çš„åº¦é‡ã€‚è¿™æ„å‘³ç€å¯ä»¥é€šè¿‡æ„å»ºä¸€ä¸ªåªæœ‰æ¥è‡ªçœŸå®æ•°æ®åˆ†å¸ƒ $p$ çš„æ ·æœ¬çš„ï¼ˆè¯­è¨€ï¼‰æ¨¡å‹ $q$ æ¥ä¼°è®¡ $H(p,q)$ ï¼Œè€Œ $H(p)$ é€šå¸¸æ— æ³•è®¿é—®ï¼Œæ‰€ä»¥å¯ä»¥é€šè¿‡æ„å»ºæ›´å¥½çš„æ¨¡å‹qæ¥å¾—åˆ°ç†µH(p)çš„æ›´å¥½çš„ä¼°è®¡ï¼Œç”±H(p,q)è¡¡é‡ã€‚å°±æ˜¯ï¼Œ$q(x)$ æ˜¯ç”¨æ¥ä¼°è®¡ $ğ‘(ğ‘¥)$ (çœŸå®çš„æ¦‚ç‡åˆ†å¸ƒ)çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆé€šå¸¸æ˜¯é€šè¿‡æŸç§æ¨¡å‹æˆ–çŒœæµ‹å¾—å‡ºçš„ï¼‰ã€‚

### N-gramæ¨¡å‹

ä»¥å‰è§£å†³è¯­éŸ³è¯†åˆ«ã€æœºå™¨ç¿»è¯‘ä»»åŠ¡çš„ä¸»è¦æ¨¡å‹æ˜¯å™ªå£°ä¿¡é“æ¨¡å‹ã€‚ä»¥è¯­éŸ³è¯†åˆ«ä¸ºä¾‹ï¼š
- å‡è®¾æœ‰ä¸€äº›ä»æŸä¸ªåˆ†å¸ƒpä¸­æŠ½å–çš„æ–‡æœ¬
- è¿™äº›æ–‡æœ¬è¢«è½¬æ¢ä¸ºè¯­éŸ³ï¼ˆå£°éŸ³ä¿¡å·ï¼‰
- ç„¶åç»™å®šè¯­éŸ³ï¼Œæˆ‘ä»¬å¸Œæœ›æ¢å¤ï¼ˆæœ€æœ‰å¯èƒ½çš„ï¼‰æ–‡æœ¬ã€‚è¿™å¯ä»¥é€šè¿‡è´å¶æ–¯å®šç†å®ç°ï¼š

$$p(\text{text} \mid \text{speech}) \propto \underbrace{p(\text{text})}_\text{language model} 
\underbrace{p(\text{speech} \mid \text{text})}_\text{language model}$$

$$
p(\text{text} \mid \text{speech}) \propto \underbrace{p(\text{text})}_\text{language model} \underbrace{p(\text{speech} \mid \text{text})} _ \text{acoustic model}.  
$$

<img width="665" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/fc17f20b-726c-4943-966e-df9dc419f54f">

<img width="405" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/d291e84d-3431-45b8-a786-fe5c95c439d5">

# Qwenæ•´ä½“ä»‹ç»

Qwençš„æ¶æ„ï¼š
![2bd108a0a25f60fd7baad3a6ae0d4148_framework](https://github.com/superkong001/learning_in_datawhale/assets/37318654/5cadb050-43c8-48be-b87a-b895db72c411)

å…¶ä¸­:
- tokenizerå°†æ–‡æœ¬è½¬ä¸ºè¯è¡¨é‡Œé¢çš„æ•°å€¼ã€‚
- æ•°å€¼ç»è¿‡embeddingå¾—åˆ°ä¸€ä¸€å¯¹åº”çš„å‘é‡ã€‚
- attention_maskæ˜¯ç”¨æ¥çœ‹è§å·¦è¾¹ã€å³è¾¹ï¼ŒåŒå‘ç­‰ç­‰æ¥è®¾å®šã€‚
- å„ç±»ä¸‹æ¸¸ä»»åŠ¡ï¼ŒCasual,seqclsç­‰ï¼ŒåŸºæœ¬éƒ½æ˜¯åŸºç¡€æ¨¡å‹modelåé¢æ¥å¯¹åº”çš„Linearå±‚ï¼Œè¿˜æœ‰æŸå¤±å‡½æ•°ä¸ä¸€æ ·ã€‚

```bash
# æ‹‰å–huggingfaceä¸Šä»£ç åˆ°å½“å‰ç›®å½•
git clone https://github.com/huggingface/transformers.git 

# å®‰è£…ä¾èµ–åŒ…
pip install huggingface_hub
pip install transformers
```

```bash
def run_qwen2():
    qwen2config = Qwen2Config(
        vocab_size=151936,
        hidden_size=4096//2,
        intermediate_size=22016//2,
        num_hidden_layers=32//2,
        num_attention_heads=32, #æ¯ä¸€å¤´çš„hidden_dim=2048/32=64
        max_position_embeddings=2048//2      
    )
    qwen2model = Qwen2Model(config=qwen2config)

    input_ids = torch.randint(0, qwen2config.vocab_size, (4,30))

    res = qwen2model(input_ids)
    print(type(res))

if __name__=="__main__":
    run_qwen2()
```

# Qwen2Config
Qwen2Configä¸­åŒ…å«ä¸€äº›è‡ªå®šä¹‰çš„è¶…å‚æ•°

```bash
# åˆå§‹åŒ–å‚æ•°é…ç½®
        vocab_size=151936,
        hidden_size=4096,
        intermediate_size=22016,
        num_hidden_layers=32,
        num_attention_heads=32,
        num_key_value_heads=32,
        hidden_act="silu",
        max_position_embeddings=32768,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        tie_word_embeddings=False,
        rope_theta=10000.0,
        use_sliding_window=False,
        sliding_window=4096,
        max_window_layers=28,
        attention_dropout=0.0
```

# Qwen2Modelç±»

## åˆå§‹åŒ–

```bash
è¾“å…¥ï¼štensor[4,30]
input_ids = torch.randint(0, qwen2config.vocab_size, (4,30))

class Qwen2Model(Qwen2PreTrainedModel):
    def __init__(self, config: Qwen2Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id #æŒ‡å®šå¡«å……æ ‡è®°çš„ç´¢å¼•
        self.vocab_size = config.vocab_size  #è¯æ±‡è¡¨çš„å¤§å°

        # åµŒå…¥å±‚å°†è¾“å…¥çš„æ ‡è®°æ˜ å°„æˆå¯†é›†çš„å‘é‡è¡¨ç¤º
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        # è§£ç å™¨å±‚ï¼ŒåŒ…å«å¤šä¸ªè§£ç å™¨å±‚ï¼ˆ16å±‚ï¼‰
        self.layers = nn.ModuleList(
            [Qwen2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        # å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„æ˜¯ Root Mean Square Layer Normalization
        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.gradient_checkpointing = False #ç”¨æ¥èŠ‚çœæ˜¾å­˜
        # Initialize weights and apply final processing
        self.post_init()  # å¯¹å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œä»¥åŠåˆå§‹åŒ–æ¢¯åº¦æ£€æŸ¥ç‚¹ä½œç”¨
```

```bash
def post_init(self):
    """
    A method executed at the end of each Transformer model initialization, to execute code that needs the model's
    modules properly initialized (such as weight initialization).
    """
    self.init_weights()
    # æ¢¯åº¦æ£€æŸ¥ç‚¹çš„åŸºæœ¬æ€æƒ³æ˜¯åœ¨ç½‘ç»œçš„å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¸ä¿å­˜æ‰€æœ‰å±‚çš„ä¸­é—´æ¿€æ´»å€¼ï¼ˆå³æ¯ä¸€å±‚è¾“å‡ºçš„ç»“æœï¼‰ï¼Œåªæœ‰é€‰å®šçš„â€œæ£€æŸ¥ç‚¹â€å±‚çš„è¾“å‡ºä¼šè¢«ä¿å­˜ï¼Œä»è€Œå‡å°‘å†…å­˜å ç”¨ã€‚
    # æœªä¿å­˜çš„ï¼Œéœ€è¦åœ¨åå‘ä¼ æ’­æœŸé—´é‡æ–°è®¡ç®—è¾“å‡ºã€‚
    self._backward_compatibility_gradient_checkpointing()
```

## ä¸»å¹²Forward, Embedding+Layers(Qwen2DecoderLayer)+Norm

```bash
inputs_embeds = self.embed_tokens(input_ids)  #input: tensor[4,30] ï¼ˆ4è¡Œ30åˆ—ï¼‰, output: tensor[4,30,2048]
# embed positions
hidden_states = inputs_embeds

for decoder_layer in self.layers: #16å±‚å¾ªç¯å¤„ç†
    # å°†æ‰€æœ‰çš„hidden_statesä¿å­˜æˆtuple
    if output_hidden_states:
        all_hidden_states += (hidden_states,)
    # å°†hsé€å…¥æ¯ä¸€å±‚decoder_layer
    if self.gradient_checkpointing and self.training:
        layer_outputs = self._gradient_checkpointing_func(
            decoder_layer.__call__,
            hidden_states,
            attention_mask,
            position_ids,
            past_key_values,
            output_attentions,
            use_cache,
        )
    else:
        layer_outputs = decoder_layer(
            hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_values,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
    # å–å‡ºä¸Šä¸€å±‚decoder_è¾“å‡ºçš„hs,å†ä¼ å…¥ä¸‹ä¸€ä¸ªlayer
    # åªè¦ç¬¬ä¸€ä¸ª,ç¬¬äºŒä¸ªæ˜¯cacheçš„ä¸€ä¸ªç±»ï¼Œç„¶åè¿›å…¥ä¸‹ä¸€ä¸ªlayer
    hidden_states = layer_outputs[0]

    if use_cache:
        next_decoder_cache = layer_outputs[2 if output_attentions else 1]

    if output_attentions:
        all_self_attns += (layer_outputs[1],)
# å°†æœ€ålayersè¾“å‡ºåçš„hidden_statesè¿›è¡Œæ ‡å‡†åŒ–  
hidden_states = self.norm(hidden_states)

# åŠ ä¸Šæœ€åä¸€å±‚çš„hidden_states
if output_hidden_states:
    all_hidden_states += (hidden_states,)
```

- å¦‚æœä¿å­˜output_hidden_statesçš„è¯ï¼Œå°±æ˜¯ç¬¬ä¸€ä¸ªä¸ºinput_idsè¿›è¡Œembï¼Œç„¶åä¿å­˜åˆ°n-1å±‚çš„decoder_layerçš„è¾“å‡ºhsï¼Œå†åŠ ä¸Šæœ€åä¸€å±‚layerçš„è¾“å‡ºhsè¿›è¡Œè¿‡normåçš„hs.
- æœ€åæ˜¯ä»¥BaseModelOutputWithPastçš„å½¢å¼è¾“å‡ºã€‚

# Qwen2DecoderLayer, attn+MLP+norm

![1725eb39a3bb2bc6b1908c4d6f585a89_decoderlayer](https://github.com/superkong001/learning_in_datawhale/assets/37318654/708fc8ae-d732-4064-9c24-adcff8b5f9ed)

## åˆå§‹åŒ–

```bash
QWEN2_ATTENTION_CLASSES = {
    "eager": Qwen2Attention, # ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯è¿™ä¸ª
    "flash_attention_2": Qwen2FlashAttention2,
    "sdpa": Qwen2SdpaAttention,

class Qwen2DecoderLayer(nn.Module):
    def __init__(self, config: Qwen2Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        if config.use_sliding_window and config._attn_implementation != "flash_attention_2":
            logger.warning_once(
                f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
                "unexpected results may be encountered."
            )
        self.self_attn = QWEN2_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)

        self.mlp = Qwen2MLP(config)
        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
```

input_layernormå’Œpost_attention_layernormå†…å®¹æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯åº”ç”¨çš„é¡ºåºä¸ä¸€æ ·ã€‚

## Forward, Norm+attn+(+residual)+Norm+mlp+(+residual)

- é¦–å…ˆå¤åˆ¶ä¸€ä»½hidden_statesä¸ºæ®‹å·®,ç„¶åå°†hidden_statesé€å…¥Norm,å†é€å…¥attnæ¨¡å—ã€‚
- å¾—åˆ°attnçš„è¾“å‡ºåä¸å‰é¢æ®‹å·®ç›¸åŠ ï¼ˆå‘é‡é€ä½ç›¸åŠ ï¼‰ï¼Œå†å¤åˆ¶ä¸€ä»½ä½œä¸ºæ®‹å·®ï¼Œå†å°†hidden_statesé€å…¥Normå’Œmlpï¼Œå†ä¸residualè¿›è¡Œç›¸åŠ ã€‚æœ€åè¾“å‡ºçš„å°±æ˜¯è¿™ä¸ªhidden_statesã€‚

```bash
def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        **kwargs,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        if "padding_mask" in kwargs:
            warnings.warn(
                "Passing `padding_mask` is deprecated and will be removed in v4.37. "
                "Please make sure use `attention_mask` instead.`"
            )
        residual = hidden_states
        #  RMSNormæ ‡å‡†åŒ–åé€å…¥attn
        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
        # æ®‹å·®ä¸æ–°çš„hidden_statesç›¸åŠ 
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs
```

# Qwen2Attention

![eb0bcb521d1c092d05a30351a3a3b641_Qwen2Attention](https://github.com/superkong001/learning_in_datawhale/assets/37318654/b2e66e42-8c5a-4da8-8c12-c90223976145)

- num_key_value_heads:è¡¨ç¤ºé”®å€¼å¯¹çš„å¤´æ•°
- num_key_value_groups:è¡¨ç¤ºé”®å€¼å¯¹çš„ç»„æ•°ï¼Œè®¡ç®—ä¸ºnum_heads // num_key_value_headsGQAçš„å®ç°ï¼ï¼
- q_proj,k_proj,v_proj,o_projå››ä¸ªLinearæ“ä½œã€‚åç»­LoRaä¹ŸåŸºæœ¬éƒ½å¯¹ä»–åŠ¨çš„åˆ€å­.

## åˆå§‹åŒ–

```bash
def __init__(self, config: Qwen2Config, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will "
                "to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )
        
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads        
        # max_position_embeddings (`int`, *optional*, defaults to 32768):The maximum sequence length that this model might ever be used with.
        self.max_position_embeddings = config.max_position_embeddings
        # rope_theta (`float`, *optional*, defaults to 10000.0):The base period of the RoPE embeddings.
        self.rope_theta = config.rope_theta
        self.is_causal = True
        self.attention_dropout = config.attention_dropout
        
        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        
        self.rotary_emb = Qwen2RotaryEmbedding(
            self.head_dim,
            max_position_embeddings=self.max_position_embeddings,
            base=self.rope_theta,
        )
```

## forward, q&k&v proj(nn.Linear) + reshape + rotary_pos_emb  +k&v expand(GQA) + q*kT/hd_d^0.5 + attn_weightsåŠ ä¸Šattention_mask + (softmax + dropout + values_statesç›¸ä¹˜) + reshape + o_proj

<img width="672" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/c05cdca1-aed1-43b8-ada4-7801fb135bc1">

<img width="661" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/3078c3d0-97da-42bb-a19e-7324b23c9ebd">

<img width="667" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/2061d0a9-fbe5-414e-bbd5-19602a2bbe57">

- é¦–å…ˆå°†hidden_statesé€å…¥Linearä¸­å¾—åˆ°queryã€keyä¸valueã€‚
- ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥æ“ä½œrotary_embï¼Œä½¿ç”¨äº†æ—‹è½¬ä½ç½®åµŒå…¥çš„ä½™å¼¦å’Œæ­£å¼¦éƒ¨åˆ†ï¼Œå°†ä»–ä»¬ä¸queryå’Œkeyç›¸ä¹˜ï¼Œå¹¶å°†ç»“æœç›¸åŠ ï¼Œä»è€Œå®ç°æ—‹è½¬ä½ç½®åµŒå…¥çš„æ•ˆæœã€‚
- å°†key_stateså’Œvalue_statesé‡å¤groupæ¬¡ï¼Œå†æ‰§è¡Œdot attnæ“ä½œã€‚
- åœ¨dot attnæ“ä½œåå¾—åˆ°attn_weights,åŠ ä¸Šattention_maskä»è€Œå®ç°è¯»å–æ©ç›–æ“ä½œï¼Œåœ¨ç»è¿‡softmaxä¸value_statesç›¸ä¹˜ã€‚å¾—åˆ°attn_outputã€‚
- å†å°†ä¸Šè¿°çš„attn_outputè¿›è¡Œreshapeæ“ä½œï¼Œé€å…¥o_projï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

![b6fceb434fbc46d94b0cf3683ff4ea4a_GQA](https://github.com/superkong001/learning_in_datawhale/assets/37318654/43f9acf2-389a-439c-afcf-103567b03389)

ä¸»æ—¨:GQAå’ŒMQAä¸éœ€è¦åœ¨æ¨ç†çš„è¿‡ç¨‹å­˜å‚¨é‚£ä¹ˆå¤šçš„kv cache, é‚£ä¹ˆkv cacheå ç”¨çš„æ˜¾å­˜å°±å˜å°ï¼Œé‚£ä¹ˆæˆ‘ä»¬LLM servingå¯ä»¥å¤„ç†çš„è¯·æ±‚æ•°é‡å°±æ›´å¤š

è§£æï¼š

1) åˆå§‹å¼ é‡

```bash
è¾“å…¥ï¼štensor[4, 30](shape:[batch, seq_len]) , headers=32
input_ids = torch.randint(0, qwen2config.vocab_size, (4, 30))
embeddingå: tensor[4, 30, 2048](shape:[batch, seq_len, dim]) 


.view(bsz, q_len, self.num_heads, self.head_dim)å: tensor[4, 30, 32, 64](shape:[batch, seq_len, head, head_dim]) 
.transpose(1, 2)å: tensor[4, 32, 30, 64](shape:[batch, head, seq_len, head_dim]) #æ¯ä¸€å¤´çš„hidden_dim=2048/32=64
åˆ†åˆ«è¾“å…¥åˆ°qã€kã€v
```

```bash
# GQA(grouped-query)æƒ…å†µ:
import torch

# shape:(batch, seq_len, head, head_dim)
query = torch.randn(10, 128, 8, 128)
key = torch.randn(10, 128, 2, 128)
value = torch.randn(10, 128, 2, 128)

## åœ¨æ­¤è®¾ç½®ç»„æ•°ä¸º8/2=4
groups = query.shape[-2] // key.shape[-2]

# keyå’Œvalueéƒ½è¦æ¯”queryå°groupå€ï¼Œä½†æ˜¯ä¸ºåœ¨åç»­åšçŸ©é˜µä¹˜æ³•æ—¶æ–¹ä¾¿ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæŠŠkeyå’Œvalueçš„headé‡å¤åˆ°å’Œqueryç›¸åŒçš„ç»´åº¦ã€‚æ–¹ä¾¿åç»­è®¡ç®—ã€‚
# å®šä¹‰è¾“å…¥xï¼Œ n_repæ˜¯éœ€è¦é‡å¤çš„æ¬¡æ•°ï¼Œåœ¨è¿™é‡Œä¸€èˆ¬æ˜¯ç»„æ•°ï¼Œè¾“å…¥shape:(batch, head, seq_len, head_dim)
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    # dont need repeat here means multi head attention
    if n_rep == 1:
        return hidden_states
    # first we expand x to (bs, seq_len, head, group, head_dim)
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    # reshape make head -> head * group
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
```

tips: ä¸ºä»€ä¹ˆè¦ç”¨expandä¹‹åå†reshapeè€Œä¸èƒ½ç›´æ¥ç”¨tensorè‡ªå¸¦çš„repeat?

- expand æ–¹æ³•ç”¨äºå¯¹å¼ é‡è¿›è¡Œæ‰©å±•ï¼Œä½†ä¸å®é™…åˆ†é…æ–°çš„å†…å­˜ã€‚å®ƒè¿”å›çš„å¼ é‡ä¸åŸå§‹å¼ é‡å…±äº«ç›¸åŒçš„æ•°æ®
- repeat æ–¹æ³•é€šè¿‡å®é™…å¤åˆ¶æ•°æ®æ¥æ‰©å±•å¼ é‡ã€‚å®ƒè¿”å›çš„æ–°å¼ é‡ä¸ä¸åŸå§‹å¼ é‡å…±äº«æ•°æ®ï¼Œæ‰©å±•åçš„å¼ é‡å ç”¨äº†æ›´å¤šçš„å†…å­˜ã€‚

2) pos_emb, Qwen2RotaryEmbedding + apply_rotary_pos_emb

### Qwen2RotaryEmbedding

ç›¸å…³çŸ¥è¯†ï¼š

- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä½¿ç”¨å·ç§¯æ ¸æ¥æ•è·å•è¯ä¹‹é—´çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œä½†å…¶ä»…èƒ½æ•è·å›ºå®šå¤§å°çš„å±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰åœ¨å¤„ç†åºåˆ—ä¿¡æ¯ä¸Šä¼šæœ‰æ›´å¥½çš„æ•ˆæœï¼Œå…¶ä¾é å¾ªç¯ç»“æ„ï¼Œå°†åºåˆ—ä¿¡æ¯é€æ­¥ä¼ é€’ï¼Œè¿™å…¶ä¸­å°±å¼•å…¥äº†å•è¯çš„ä½ç½®å’Œé¡ºåºä¿¡æ¯ã€‚ä½†éšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼ŒRNN ä¼šæ…¢æ…¢å¿˜è®°æ—©å‰çš„ä¿¡æ¯ï¼Œè¿™å°±å¯¼è‡´äº†é•¿æœŸä¾èµ–é—®é¢˜ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¾ªç¯ç»“æ„ä¹Ÿä½¿å¾— RNN æ— æ³•å¹¶è¡Œè®¡ç®—ï¼Œè¿™ä½¿å¾— RNN çš„è®­ç»ƒé€Ÿåº¦ååˆ†ç¼“æ…¢ã€‚
- Transformerï¼šç”±äº Transformer ä¸åŒ…å«ä»»ä½•å¾ªç¯ç»“æ„ï¼Œå„ä¸ªå•è¯åœ¨ Transformer ä¸­éƒ½åŒæ—¶ç»è¿‡ Decoder-Encoder çš„å˜æ¢ï¼Œè¿™å°±å¯¼è‡´äº† Transformer æ— æ³•æ•è·å•è¯çš„ä½ç½®ä¿¡æ¯ã€‚

Transformeré‡‡ç”¨çš„æ˜¯é™æ€çš„æ­£å¼¦å’Œä½™å¼¦æ³¢å‡½æ•°çš„ç»„åˆï¼Œä¸»è¦æä¾›ç»å¯¹ä½ç½®ä¿¡æ¯:

<img width="533" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/3ee73abe-a686-429e-8491-fa6dc46d9f5d">

è¿™é‡Œ ğ‘ğ‘œğ‘  æ˜¯è¯åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œğ‘– æ˜¯ä½ç½®å‘é‡ä¸­çš„ç»´åº¦ç´¢å¼•ï¼Œğ‘‘ æ˜¯ä½ç½®å‘é‡çš„ç»´åº¦ï¼ˆé€šå¸¸ä¸æ¨¡å‹çš„éšè—å±‚ç»´åº¦ç›¸åŒï¼Œä¾‹å¦‚512ï¼‰ã€‚è¿™ä¸ªå…¬å¼ä¸­çš„ ${10000^{2n/d}}$ æ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œå®ƒéš ğ‘– çš„å¢å¤§è€Œå¢å¤§ï¼Œè¿™æ ·å¯¹äºä¸åŒçš„ ğ‘–ï¼Œæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„æ³¢é•¿ä¼šéšä¹‹å¢é•¿ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¯ä¸ªç»´åº¦æ•æ‰åˆ°ä¸åŒé¢‘ç‡çš„ä½ç½®ä¿¡æ¯ã€‚

æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼šå¼•å…¥æ—‹è½¬çŸ©é˜µçš„ä½ç½®ç¼–ç ï¼Œä½ç½®ç¼–ç çš„å«ä¹‰æ˜¯å¯¹æ¯ä¸€ä¸ªtokençš„æ¯ä¸€ä¸ªdimèµ‹äºˆä¸åŒçš„ä½ç½®ä¿¡æ¯ã€‚ å…¬å¼å®šä¹‰:

![image](https://github.com/superkong001/learning_in_datawhale/assets/37318654/58f0f9f6-4d7b-4762-b4b5-826af5259975)

æ¦‚å¿µï¼šé€šè¿‡æ—‹è½¬ç¼–ç ï¼Œä½¿å¾—æ¯ä¸ªtokenæ—¢æœ‰ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œåˆæœ‰ç»å¯¹ä½ç½®ä¿¡æ¯ã€‚

- æ—¢èƒ½ä»¥è‡ªæ³¨æ„åŠ›çŸ©é˜µåç½®çš„å½¢å¼ä½œç”¨äº,ç›´æ¥åæ˜ ä¸¤ä¸ªtokençš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œåˆèƒ½æ‹†è§£åˆ°å‘é‡å’Œä¸Šï¼Œé€šè¿‡ç›´æ¥ç¼–ç tokençš„ç»å¯¹ä½ç½®å®ç°ã€‚
- RoPEæœ¬è´¨æ˜¯å®ç°å¯¹ç‰¹å¾å‘é‡çš„æ—‹è½¬æ“ä½œï¼Œå¦‚æœä»¥äºŒç»´ç‰¹å¾å‘é‡ä¸¾ä¾‹ï¼Œå¯¹äºç›¸é‚»ä¸¤ä¸ªtokenæ¥è¯´ï¼Œå…¶å¯¹åº”åŒä¸€ä¸ª,å…¶å®šä¹‰ä¸º:

![bcfcb5136238da2cca5641a70169cc23_ROPE2](https://github.com/superkong001/learning_in_datawhale/assets/37318654/3e698be4-2a31-43cf-af96-6e50a8b859cd)

å¯å¾—ï¼Œå…¶æœ¬è´¨å°±æ˜¯: $q_{t}$, $k_{s}$ æ—‹è½¬åçš„ç»“æœï¼Œå°±æ˜¯ $q_{t}$, $k_{s}$ä¹˜ä¸Šcoså†åŠ ä¸Š $q_{t}$, $k_{s}$ç¿»è½¬ç»´åº¦å¹¶å–åä¸€ç»´åä¹˜ä¸Šsinã€‚
- å¯¹äºé«˜çº¬å‘é‡ï¼Œç”±äºå¥‡ã€å¶æ•°ç»´åº¦ä¸¤ä¸¤äº¤é”™å®ç°è¾ƒä¸ºå¤æ‚ï¼Œåˆ™ç°åœ¨å¯ç®€åŒ–ä¸ºå°†ç‰¹å¾ç»´åº¦ä¸€åˆ‡äºŒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨å®ç°è¿‡ç¨‹ä¸­å¯¹å‰åå„åŠè¿›è¡Œçš„æ“ä½œå³ä¸ºrotate_halfæ“ä½œï¼š

![b9732c2d7d6e7e265bfd933fb481cc9b_ROPE3](https://github.com/superkong001/learning_in_datawhale/assets/37318654/2204dd5d-2fae-4455-9fb0-600c17c3aa11)

```bash
# Copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Qwen2
class Qwen2RotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        # å®šä¹‰åˆå§‹å€¼
        self.dim = dim # æ—‹è½¬åµŒå…¥çš„ç»´åº¦
        self.max_position_embeddings = max_position_embeddings # æœ€å¤§çš„ä½ç½®ç´¢å¼•ï¼Œç”¨äºå®šä¹‰æœ€å¤§çš„åºåˆ—é•¿åº¦
        self.base = base # é»˜è®¤10000ï¼Œè®¡ç®—é¢‘ç‡çš„åŸºæ•°ï¼Œé€šå¸¸ç”¨äºè°ƒèŠ‚ä½ç½®ç¼–ç çš„å‘¨æœŸæ€§
        # å®šä¹‰æ—‹è½¬è§’Î¸n=10000^(âˆ’2n/d)ï¼Œå…¶ä¸­nè¡¨ç¤ºç»´åº¦æ•°ï¼Œå…¶å–å€¼èŒƒå›´ä¸º[0, 1, ..., d/2-1]
        # å¦‚ï¼š2/64=0.0312ï¼Œ10000^0.0312=1.3335ï¼Œ1/1.3335=7.4989e-01
        # torch.arange(0, self.dim, 2, dtype=torch.int64)ç”Ÿæˆä»0å¼€å§‹åˆ°self.dimï¼ˆä½†ä¸åŒ…æ‹¬self.dimï¼‰ï¼Œæ­¥é•¿ä¸º2çš„åºåˆ—ã€‚
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))

        # æ³¨å†Œç¼“å†²åŒºï¼ˆbufferï¼‰. ç¬¬ä¸€ä¸ªå‚æ•°"inv_freq"ç¼“å†²åŒºåå­—ï¼Œç¬¬äºŒä¸ªå‚æ•° (inv_freq)ç¼“å†²åŒºçš„å®é™…æ•°æ®ï¼Œç¬¬ä¸‰ä¸ªå‚æ•° (persistent=False)ä¸ä¿å­˜è¿™ä¸ªç¼“å†²åŒº
        self.register_buffer("inv_freq", inv_freq, persistent=False)

        # Build here to make `torch.jit.trace` work.
        self._set_cos_sin_cache(
            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()
        )

    # ä¸ºseqé‡Œé¢çš„æ¯ä¸ªtokenå½¢æˆç‹¬ä¸€æ— äºŒçš„æ—‹è½¬è§’åµŒå…¥(å¤–ç§¯)
    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len
        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)

        # å°†å‰é¢ç”Ÿæˆè§’åº¦(inv_freq)ä¸æ¯ä¸€ä¸ªä½ç½®ä¹˜ç§¯ï¼ŒåŒºåˆ†ä¸€ä¸ªseqä¸­å„ä¸ªè¯
        # torch.outerè¡¨ç¤ºä¸¤ä¸ªå‘é‡å¤–ç§¯ï¼Œå³ç¬¬ä¸€ä¸ªå‘é‡é€ä¸ªå…ƒç´ ä¸ç¬¬äºŒä¸ªå‘é‡ç›¸ä¹˜å¾—åˆ°æ¯ä¸ªç»“æœå•ç‹¬ä¿å­˜ä¸ºä¸€è¡Œã€‚
        #  t çš„é•¿åº¦ä¸º Lï¼ˆä»£è¡¨åºåˆ—é•¿åº¦ï¼‰ä¸” inv_freq çš„é•¿åº¦ä¸º D/2ï¼ˆå‡è®¾ dim=Dï¼‰ï¼Œé‚£ä¹ˆ freqs çš„å½¢çŠ¶æ˜¯ L x (D/2)ã€‚æœ€ç»ˆå½¢çŠ¶ä¸º(1024,32)
        freqs = torch.outer(t, self.inv_freq)
        # ç”Ÿæˆè§’åº¦ä¿¡æ¯(åˆ©ç”¨æ³¨å†Œæœºåˆ¶ç”Ÿæˆself.cos_cachedä¸sin_cached)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        # embå°†äºŒè€…catèµ·æ¥(åˆ—æ–¹å‘æ‹¼æ¥)ï¼Œå¾—åˆ°dimç»´åº¦ï¼Œæ¯dim/2ä¸€å¾ªç¯ã€‚ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º L x D (1024, 64)çš„çŸ©é˜µï¼Œå…¶ä¸­ L æ˜¯åºåˆ—é•¿åº¦ï¼ŒD æ˜¯ç¼–ç çš„å®Œæ•´ç»´åº¦ã€‚
        # é€šè¿‡æ‹¼æ¥ä¸¤ä»½ freqsï¼Œå¯ä»¥ç¡®ä¿å¯¹äºæ¯ä¸ªä½ç½®ç´¢å¼• iï¼Œæœ‰è¶³å¤Ÿçš„é¢‘ç‡å€¼æ¥åŒæ—¶è®¡ç®—å…¶æ­£å¼¦å’Œä½™å¼¦ã€‚
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos().to(dtype), persistent=False)
        self.register_buffer("sin_cached", emb.sin().to(dtype), persistent=False)

    def forward(self, x, seq_len=None):
        # x: [batch_size, num_attention_heads, seq_len, head_size]
        if seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)

        # åœ¨å–å‡ºä½ç½®ç¼–ç ä¿¡æ¯cosä¸sinçš„æ—¶å€™ï¼Œå°±æ˜¯å°†seqçš„éƒ¨åˆ†åˆ‡å‡ºæ¥ï¼ŒåŸå…ˆè®¾ç½®çš„1024æ˜¯æœ€å¤§posç¼–ç ï¼Œæ¯æ¬¡ç”¨çš„æ—¶å€™åªå–å½“ä¸‹seq_lençš„å³å¯ï¼Œ
        # ä¹‹å‰æ±‚å¾—å¤–ç§¯ï¼Œæ˜¯ä¸ºäº†ä¿è¯seqé‡Œé¢å¾—æ¯ä¸€ä¸ªè¯éƒ½èƒ½æœ‰ä¸åŒçš„1024ä¸ªä½ç½®ç¼–ç ã€‚
        return (
            self.cos_cached[:seq_len].to(dtype=x.dtype),
            self.sin_cached[:seq_len].to(dtype=x.dtype),
        )
```

a) ç”Ÿæˆè§’åº¦: $$\theta = \left(\frac{1}{10000^{2n/d}}\right)$$

å…¶ä¸­ï¼Œnè¡¨ç¤ºç»´åº¦æ•°ï¼Œå…¶å–å€¼èŒƒå›´ä¸º[0, 1, ..., d/2-1]

b) å°†ä¸Šè¿°ç”Ÿæˆè§’åº¦ä¸æ¯ä¸€ä¸ªä½ç½®ä¹˜ç§¯ï¼ŒåŒºåˆ†ä¸€ä¸ªseqä¸­å„ä¸ªè¯ï¼šå…¶å®ç­‰ä»·äº:
$$\thetaÂ =Â \left(\frac{i}{10000^{2n/d}}\right)$$  
å…¶ä¸­: `i`ä¸ºè¡Œæ•°ã€‚

c) embå°†äºŒè€…catèµ·æ¥ï¼Œå¾—åˆ°dimç»´åº¦ï¼Œæ¯dim/2ä¸€å¾ªç¯ã€‚

d) åœ¨å–å‡ºä½ç½®ç¼–ç ä¿¡æ¯cosä¸sinçš„æ—¶å€™ï¼Œå°±æ˜¯å°†seqçš„éƒ¨åˆ†åˆ‡å‡ºæ¥ï¼ŒåŸå…ˆè®¾ç½®çš„1024æ˜¯æœ€å¤§posç¼–ç ï¼Œæ¯æ¬¡ç”¨çš„æ—¶å€™åªå–å½“ä¸‹seq_lençš„å³å¯.ä¹‹å‰æ±‚å¾—å¤–ç§¯ï¼Œæ˜¯ä¸ºäº†ä¿è¯seqé‡Œé¢å¾—æ¯ä¸€ä¸ªè¯éƒ½èƒ½æœ‰ä¸åŒçš„1024ä¸ªä½ç½®ç¼–ç ã€‚

e) è¿›è¡Œæ—‹è½¬åµŒå…¥ã€‚

å°† ğ‘ è§†ä¸ºå¤æ•°ï¼Œå…¶ä¸­å®éƒ¨å’Œè™šéƒ¨åˆ†åˆ«æ˜¯ ğ‘ å‘é‡çš„ä¸¤ä¸ªåˆ†é‡ã€‚ $ğ‘’^ğ‘–ğœƒ$ æ˜¯ç”± cosâ¡(ğœƒ)+ğ‘–sinâ¡(ğœƒ) è¡¨ç¤ºçš„å•ä½å¤æ•°

å¤æ•°ä¹˜æ³•å¯ä»¥è¡¨ç¤ºä¸ºä¸¤ä¸ªå¤æ•°ç›¸ä¹˜ã€‚å¦‚æœä½ æŠŠä¸€ä¸ªå¤æ•° ğ‘+ğ‘ğ‘– ä¸å¦ä¸€ä¸ªå¤æ•° ğ‘+ğ‘‘ğ‘– ç›¸ä¹˜ï¼Œç»“æœæ˜¯ ğ‘ğ‘âˆ’ğ‘ğ‘‘+(ğ‘ğ‘‘+ğ‘ğ‘)ğ‘–ã€‚

```bash
# ååŠéƒ¨åˆ†å’Œå‰åŠéƒ¨åˆ†è¿›è¡Œäº†äº¤æ¢ï¼Œå¹¶ä¸”å°†ååŠéƒ¨åˆ†çš„ç¬¦å·å–åã€‚
def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    # x1 è¢«å®šä¹‰ä¸ºå¼ é‡ x æœ€åä¸€ä¸ªç»´åº¦çš„å‰åŠéƒ¨åˆ†ã€‚
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    query and key tensors rotated using the Rotary Position Embedding.
    """
    # unsqueeze(-1)  # è´Ÿ1è¡¨ç¤ºï¼Œåœ¨æœ€åä¸€ç»´ä¸Šæ·»åŠ ä¸€ä¸ªç»´åº¦
    # ä½¿å¾—ç»´åº¦ä¸æŸ¥è¯¢å’Œé”®å¼ é‡åŒ¹é…ï¼Œä»è€Œå¯ä»¥æ‰§è¡Œå…ƒç´ çº§ä¹˜æ³•ï¼ˆå¹¿æ’­ï¼‰ã€‚
    cos = cos[position_ids].unsqueeze(unsqueeze_dim)
    sin = sin[position_ids].unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

### apply_rotary_pos_emb

```bash
# cos.shape(head, head_dim), sin.shape(head, head_dim)
# Copied from transformers.models.mistral.modeling_mistral.apply_rotary_pos_emb
def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`):
            The position indices of the tokens corresponding to the query and key tensors. For example, this can be
            used to pass offsetted position ids when working with a KV-cache.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos[position_ids].unsqueeze(unsqueeze_dim)
    sin = sin[position_ids].unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

4) çŸ©é˜µä¹˜æ³•å¾—åˆ°scoreä¸output åé¢å°±æ˜¯çœŸæ­£çš„kqvç›¸ä¹˜äº†

```bash
# GQA(grouped-query)æƒ…å†µ:
# åˆå§‹shape:(batch, seq_len, head, head_dim) => shape:(batch, head, seq_len, head_dim)
query = query.transpose(1, 2)
# è¾“å…¥shape:(batch, seq_len, head, head_dim) => shape:(batch, head * n_rep, seq_len, head_dim)
key = repeat_kv(key, 4).transpose(1, 2)
value = repeat_kv(value, 4).transpose(1, 2)

# q*kT/head_dim^0.5 
scores = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(head_dim) # shape:(batch, head, seq_len, kv_seq_len)
scores = torch.nn.functional.softmax(scores, dim=-1)

# (batch, head, seq_len, kv_seq_len)*(batch, head, seq_len, head_dim)=(batch, head, seq_len, head_dim)
out = torch.matmul(scores, value)
#ä¸Šä¸€æ­¥è½¬ç½®äº†ï¼Œè¿˜å¾—è½¬å›å»(batch, seq_len, head, head_dim)
out = out.transpose(1, 2)
```

å®Œæ•´ä»£ç ï¼š

```bash
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        if "padding_mask" in kwargs:
            warnings.warn(
                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
            )
        # è·å–å½¢çŠ¶ä¿¡æ¯,hidden_statesè¾“å…¥çš„ä¸º(bs,T,hd)
        bsz, q_len, _ = hidden_states.size()

        # å¯¹hidden_statesè¿›è¡ŒLinearç”Ÿæˆqueryã€keyã€value
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # reshapeå¤šå¤´å¤„ç†--åˆ†å—--(bs,T,heads,hd_d)ï¼Œäº¤æ¢æ•°ç»„çš„ç¬¬äºŒä¸ªç»´åº¦ï¼ˆç´¢å¼•ä¸º1ï¼‰å’Œç¬¬ä¸‰ä¸ªç»´åº¦
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2] # = q_len
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        # å°†æ—‹è½¬ä½ç½®åµŒå…¥åº”ç”¨äºæŸ¥è¯¢å’Œé”®å¼ é‡ã€‚ä½¿ç”¨äº†æ—‹è½¬ä½ç½®åµŒå…¥çš„ä½™å¼¦å’Œæ­£å¼¦éƒ¨åˆ†ï¼Œå°†å®ƒä»¬ä¸æŸ¥è¯¢å’Œé”®å¼ é‡ç›¸ä¹˜ï¼Œå¹¶å°†ç»“æœç›¸åŠ ï¼Œä»è€Œå®ç°æ—‹è½¬ä½ç½®åµŒå…¥çš„æ•ˆæœ
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # å…ˆå°†key_stateså’Œvalue_statesé‡å¤äº†num_key_value_groupsæ¬¡ï¼ˆGQAï¼‰
        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        # ä½¿ç”¨dot attnå®ç°q*kT/hd_d^0.5
        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        # ç„¶å attn_weights åŠ ä¸Š attention_maskï¼Œå®ç°è¯»å–é¡ºåº
        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
                )

            attn_weights = attn_weights + attention_mask

        # softmax + dropout + values_statesç›¸ä¹˜
        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        # è½¬ç½®ï¼Œä¿®æ”¹å½¢çŠ¶ç­‰reshapeæ“ä½œ
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)

        # æœ€ååœ¨è¿›è¡Œä¸€æ¬¡o_proj
        attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        # è¿”å›ç»“æœ
        return attn_output, attn_weights, past_key_value
```

# Qwen2 MLP

![38d5a025fe702e2d3b1aa624355d90c4_MLP1](https://github.com/superkong001/learning_in_datawhale/assets/37318654/d236cc58-f3bd-4b2b-a591-e5757f211fa7)

è¾“å…¥hidden_stateå¹¶è¡Œé€å…¥ä¸¤ä¸ªLinearå±‚ï¼Œå…¶ä¸­ä¸€ä¸ªæ¿€æ´»ä¸€ä¸‹ï¼Œå†ä¸å¦ä¸€ä¸ªç›¸ä¹˜ï¼Œæœ€ç»ˆå†ç»è¿‡ä¸€ä¸ªLinearï¼Œè¾“å‡ºæœ€ç»ˆç»“æœã€‚

```bash
# Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Qwen2
class Qwen2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
```

# Qwen2RMSNorm, æ ¹å‡æ–¹å½’ä¸€åŒ–

![7d54bafe8e22779a9b9b169b66fe2cea_RMSNorm_formulation](https://github.com/superkong001/learning_in_datawhale/assets/37318654/42f21607-de36-407c-a8d7-75adbacedf3c)

```bash
# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Qwen2
class Qwen2RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Qwen2RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        # .pow(2).mean(-1, keepdim=True)è¡¨ç¤ºå¯¹æ¯ä¸ªå…ƒç´ æ±‚å¹³æ–¹ï¼Œç„¶åè®¡ç®—å¼ é‡åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆç”± -1 æŒ‡å®šï¼‰ä¸Šçš„å¹³å‡å€¼ï¼ˆæ¯ä¸€è¡Œçš„å¹³å‡å€¼ï¼‰å¹¶ä¿æŒç»´åº¦
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        # rsqrtè¡¨ç¤ºå¼€æ ¹çš„å¯¼æ•°
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)
```

<img width="530" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/a3664578-23ad-46b8-9f6e-d0e43f04e9bb">






