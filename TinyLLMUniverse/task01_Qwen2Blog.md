å¼•ç”¨å‚è€ƒï¼š

- https://github.com/datawhalechina/tiny-universe
- https://github.com/huggingface/transformers/tree/v4.39.3/src/transformers/models/qwen2

<img width="465" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/fc17f20b-726c-4943-966e-df9dc419f54f">

<img width="455" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/d291e84d-3431-45b8-a786-fe5c95c439d5">

# Qwenæ•´ä½“ä»‹ç»

Qwençš„æ¶æ„ï¼š
![2bd108a0a25f60fd7baad3a6ae0d4148_framework](https://github.com/superkong001/learning_in_datawhale/assets/37318654/5cadb050-43c8-48be-b87a-b895db72c411)

å…¶ä¸­:
- tokenizerå°†æ–‡æœ¬è½¬ä¸ºè¯è¡¨é‡Œé¢çš„æ•°å€¼ã€‚
- æ•°å€¼ç»è¿‡embeddingå¾—åˆ°ä¸€ä¸€å¯¹åº”çš„å‘é‡ã€‚
- attention_maskæ˜¯ç”¨æ¥çœ‹è§å·¦è¾¹ã€å³è¾¹ï¼ŒåŒå‘ç­‰ç­‰æ¥è®¾å®šã€‚
- å„ç±»ä¸‹æ¸¸ä»»åŠ¡ï¼ŒCasual,seqclsç­‰ï¼ŒåŸºæœ¬éƒ½æ˜¯åŸºç¡€æ¨¡å‹modelåé¢æ¥å¯¹åº”çš„Linearå±‚ï¼Œè¿˜æœ‰æŸå¤±å‡½æ•°ä¸ä¸€æ ·ã€‚

```bash
# æ‹‰å–huggingfaceä¸Šä»£ç åˆ°å½“å‰ç›®å½•
git clone https://github.com/huggingface/transformers.git 

# å®‰è£…ä¾èµ–åŒ…
pip install huggingface_hub
pip install transformers
```

```bash
def run_qwen2():
    qwen2config = Qwen2Config(
        vocab_size=151936,
        hidden_size=4096//2,
        intermediate_size=22016//2,
        num_hidden_layers=32//2,
        num_attention_heads=32, #æ¯ä¸€å¤´çš„hidden_dim=2048/32=64
        max_position_embeddings=2048//2      
    )
    qwen2model = Qwen2Model(config=qwen2config)

    input_ids = torch.randint(0, qwen2config.vocab_size, (4,30))

    res = qwen2model(input_ids)
    print(type(res))

if __name__=="__main__":
    run_qwen2()
```

# Qwen2Config
Qwen2Configä¸­åŒ…å«ä¸€äº›è‡ªå®šä¹‰çš„è¶…å‚æ•°

```bash
# åˆå§‹åŒ–å‚æ•°é…ç½®
        vocab_size=151936,
        hidden_size=4096,
        intermediate_size=22016,
        num_hidden_layers=32,
        num_attention_heads=32,
        num_key_value_heads=32,
        hidden_act="silu",
        max_position_embeddings=32768,
        initializer_range=0.02,
        rms_norm_eps=1e-6,
        use_cache=True,
        tie_word_embeddings=False,
        rope_theta=10000.0,
        use_sliding_window=False,
        sliding_window=4096,
        max_window_layers=28,
        attention_dropout=0.0
```

# Qwen2Modelç±»

## åˆå§‹åŒ–

```bash
è¾“å…¥ï¼štensor[4,30]
input_ids = torch.randint(0, qwen2config.vocab_size, (4,30))

class Qwen2Model(Qwen2PreTrainedModel):
    def __init__(self, config: Qwen2Config):
        super().__init__(config)
        self.padding_idx = config.pad_token_id #æŒ‡å®šå¡«å……æ ‡è®°çš„ç´¢å¼•
        self.vocab_size = config.vocab_size  #è¯æ±‡è¡¨çš„å¤§å°

        # åµŒå…¥å±‚å°†è¾“å…¥çš„æ ‡è®°æ˜ å°„æˆå¯†é›†çš„å‘é‡è¡¨ç¤º
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)
        # è§£ç å™¨å±‚ï¼ŒåŒ…å«å¤šä¸ªè§£ç å™¨å±‚ï¼ˆ16å±‚ï¼‰
        self.layers = nn.ModuleList(
            [Qwen2DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]
        )
        # å½’ä¸€åŒ–å±‚ä½¿ç”¨çš„æ˜¯ Root Mean Square Layer Normalization
        self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)

        self.gradient_checkpointing = False #ç”¨æ¥èŠ‚çœæ˜¾å­˜
        # Initialize weights and apply final processing
        self.post_init()  # å¯¹å‚æ•°è¿›è¡Œåˆå§‹åŒ–ï¼Œä»¥åŠåˆå§‹åŒ–æ¢¯åº¦æ£€æŸ¥ç‚¹ä½œç”¨
```

```bash
def post_init(self):
    """
    A method executed at the end of each Transformer model initialization, to execute code that needs the model's
    modules properly initialized (such as weight initialization).
    """
    self.init_weights()
    # æ¢¯åº¦æ£€æŸ¥ç‚¹çš„åŸºæœ¬æ€æƒ³æ˜¯åœ¨ç½‘ç»œçš„å‰å‘ä¼ æ’­è¿‡ç¨‹ä¸­ä¸ä¿å­˜æ‰€æœ‰å±‚çš„ä¸­é—´æ¿€æ´»å€¼ï¼ˆå³æ¯ä¸€å±‚è¾“å‡ºçš„ç»“æœï¼‰ï¼Œåªæœ‰é€‰å®šçš„â€œæ£€æŸ¥ç‚¹â€å±‚çš„è¾“å‡ºä¼šè¢«ä¿å­˜ï¼Œä»è€Œå‡å°‘å†…å­˜å ç”¨ã€‚
    # æœªä¿å­˜çš„ï¼Œéœ€è¦åœ¨åå‘ä¼ æ’­æœŸé—´é‡æ–°è®¡ç®—è¾“å‡ºã€‚
    self._backward_compatibility_gradient_checkpointing()
```

## ä¸»å¹²Forward, Embedding+Layers(Qwen2DecoderLayer)+Norm

```bash
inputs_embeds = self.embed_tokens(input_ids)  #input: tensor[4,30] ï¼ˆ4è¡Œ30åˆ—ï¼‰, output: tensor[4,30,2048]
# embed positions
hidden_states = inputs_embeds

for decoder_layer in self.layers: #16å±‚å¾ªç¯å¤„ç†
    # å°†æ‰€æœ‰çš„hidden_statesä¿å­˜æˆtuple
    if output_hidden_states:
        all_hidden_states += (hidden_states,)
    # å°†hsé€å…¥æ¯ä¸€å±‚decoder_layer
    if self.gradient_checkpointing and self.training:
        layer_outputs = self._gradient_checkpointing_func(
            decoder_layer.__call__,
            hidden_states,
            attention_mask,
            position_ids,
            past_key_values,
            output_attentions,
            use_cache,
        )
    else:
        layer_outputs = decoder_layer(
            hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_values,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
    # å–å‡ºä¸Šä¸€å±‚decoder_è¾“å‡ºçš„hs,å†ä¼ å…¥ä¸‹ä¸€ä¸ªlayer
    # åªè¦ç¬¬ä¸€ä¸ª,ç¬¬äºŒä¸ªæ˜¯cacheçš„ä¸€ä¸ªç±»ï¼Œç„¶åè¿›å…¥ä¸‹ä¸€ä¸ªlayer
    hidden_states = layer_outputs[0]

    if use_cache:
        next_decoder_cache = layer_outputs[2 if output_attentions else 1]

    if output_attentions:
        all_self_attns += (layer_outputs[1],)
# å°†æœ€ålayersè¾“å‡ºåçš„hidden_statesè¿›è¡Œæ ‡å‡†åŒ–  
hidden_states = self.norm(hidden_states)

# åŠ ä¸Šæœ€åä¸€å±‚çš„hidden_states
if output_hidden_states:
    all_hidden_states += (hidden_states,)
```

- å¦‚æœä¿å­˜output_hidden_statesçš„è¯ï¼Œå°±æ˜¯ç¬¬ä¸€ä¸ªä¸ºinput_idsè¿›è¡Œembï¼Œç„¶åä¿å­˜åˆ°n-1å±‚çš„decoder_layerçš„è¾“å‡ºhsï¼Œå†åŠ ä¸Šæœ€åä¸€å±‚layerçš„è¾“å‡ºhsè¿›è¡Œè¿‡normåçš„hs.
- æœ€åæ˜¯ä»¥BaseModelOutputWithPastçš„å½¢å¼è¾“å‡ºã€‚

# Qwen2DecoderLayer, attn+MLP+norm

![1725eb39a3bb2bc6b1908c4d6f585a89_decoderlayer](https://github.com/superkong001/learning_in_datawhale/assets/37318654/708fc8ae-d732-4064-9c24-adcff8b5f9ed)

## åˆå§‹åŒ–

```bash
QWEN2_ATTENTION_CLASSES = {
    "eager": Qwen2Attention, # ä¸€èˆ¬æƒ…å†µä¸‹æ˜¯è¿™ä¸ª
    "flash_attention_2": Qwen2FlashAttention2,
    "sdpa": Qwen2SdpaAttention,

class Qwen2DecoderLayer(nn.Module):
    def __init__(self, config: Qwen2Config, layer_idx: int):
        super().__init__()
        self.hidden_size = config.hidden_size

        if config.use_sliding_window and config._attn_implementation != "flash_attention_2":
            logger.warning_once(
                f"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; "
                "unexpected results may be encountered."
            )
        self.self_attn = QWEN2_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)

        self.mlp = Qwen2MLP(config)
        self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
```

input_layernormå’Œpost_attention_layernormå†…å®¹æ˜¯ä¸€æ ·çš„ï¼Œåªæ˜¯åº”ç”¨çš„é¡ºåºä¸ä¸€æ ·ã€‚

## Forward, Norm+attn+(+residual)+Norm+mlp+(+residual)

- é¦–å…ˆå¤åˆ¶ä¸€ä»½hidden_statesä¸ºæ®‹å·®,ç„¶åå°†hidden_statesé€å…¥Norm,å†é€å…¥attnæ¨¡å—ã€‚
- å¾—åˆ°attnçš„è¾“å‡ºåä¸å‰é¢æ®‹å·®ç›¸åŠ ï¼ˆå‘é‡é€ä½ç›¸åŠ ï¼‰ï¼Œå†å¤åˆ¶ä¸€ä»½ä½œä¸ºæ®‹å·®ï¼Œå†å°†hidden_statesé€å…¥Normå’Œmlpï¼Œå†ä¸residualè¿›è¡Œç›¸åŠ ã€‚æœ€åè¾“å‡ºçš„å°±æ˜¯è¿™ä¸ªhidden_statesã€‚

```bash
def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Tuple[torch.Tensor]] = None,
        output_attentions: Optional[bool] = False,
        use_cache: Optional[bool] = False,
        **kwargs,
    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:
        if "padding_mask" in kwargs:
            warnings.warn(
                "Passing `padding_mask` is deprecated and will be removed in v4.37. "
                "Please make sure use `attention_mask` instead.`"
            )
        residual = hidden_states
        #  RMSNormæ ‡å‡†åŒ–åé€å…¥attn
        hidden_states = self.input_layernorm(hidden_states)

        # Self Attention
        hidden_states, self_attn_weights, present_key_value = self.self_attn(
            hidden_states=hidden_states,
            attention_mask=attention_mask,
            position_ids=position_ids,
            past_key_value=past_key_value,
            output_attentions=output_attentions,
            use_cache=use_cache,
        )
        # æ®‹å·®ä¸æ–°çš„hidden_statesç›¸åŠ 
        hidden_states = residual + hidden_states

        # Fully Connected
        residual = hidden_states
        hidden_states = self.post_attention_layernorm(hidden_states)
        hidden_states = self.mlp(hidden_states)
        hidden_states = residual + hidden_states

        outputs = (hidden_states,)

        if output_attentions:
            outputs += (self_attn_weights,)

        if use_cache:
            outputs += (present_key_value,)

        return outputs
```

# Qwen2Attention

![eb0bcb521d1c092d05a30351a3a3b641_Qwen2Attention](https://github.com/superkong001/learning_in_datawhale/assets/37318654/b2e66e42-8c5a-4da8-8c12-c90223976145)

- num_key_value_heads:è¡¨ç¤ºé”®å€¼å¯¹çš„å¤´æ•°
- num_key_value_groups:è¡¨ç¤ºé”®å€¼å¯¹çš„ç»„æ•°ï¼Œè®¡ç®—ä¸ºnum_heads // num_key_value_headsGQAçš„å®ç°ï¼ï¼
- q_proj,k_proj,v_proj,o_projå››ä¸ªLinearæ“ä½œã€‚åç»­LoRaä¹ŸåŸºæœ¬éƒ½å¯¹ä»–åŠ¨çš„åˆ€å­.

## åˆå§‹åŒ–

```bash
def __init__(self, config: Qwen2Config, layer_idx: Optional[int] = None):
        super().__init__()
        self.config = config
        self.layer_idx = layer_idx
        if layer_idx is None:
            logger.warning_once(
                f"Instantiating {self.__class__.__name__} without passing `layer_idx` is not recommended and will "
                "to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` "
                "when creating this class."
            )
        
        self.hidden_size = config.hidden_size
        self.num_heads = config.num_attention_heads
        self.head_dim = self.hidden_size // self.num_heads
        self.num_key_value_heads = config.num_key_value_heads
        self.num_key_value_groups = self.num_heads // self.num_key_value_heads        
        # max_position_embeddings (`int`, *optional*, defaults to 32768):The maximum sequence length that this model might ever be used with.
        self.max_position_embeddings = config.max_position_embeddings
        # rope_theta (`float`, *optional*, defaults to 10000.0):The base period of the RoPE embeddings.
        self.rope_theta = config.rope_theta
        self.is_causal = True
        self.attention_dropout = config.attention_dropout
        
        if (self.head_dim * self.num_heads) != self.hidden_size:
            raise ValueError(
                f"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}"
                f" and `num_heads`: {self.num_heads})."
            )
        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=True)
        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=True)
        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)
        
        self.rotary_emb = Qwen2RotaryEmbedding(
            self.head_dim,
            max_position_embeddings=self.max_position_embeddings,
            base=self.rope_theta,
        )
```

## forward, q&k&v proj(nn.Linear) + reshape + rotary_pos_emb  +k&v expand(GQA) + q*kT/hd_d^0.5 + attn_weightsåŠ ä¸Šattention_mask + (softmax + dropout + values_statesç›¸ä¹˜) + reshape + o_proj

<img width="672" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/c05cdca1-aed1-43b8-ada4-7801fb135bc1">

<img width="661" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/3078c3d0-97da-42bb-a19e-7324b23c9ebd">

<img width="667" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/2061d0a9-fbe5-414e-bbd5-19602a2bbe57">

- é¦–å…ˆå°†hidden_statesé€å…¥Linearä¸­å¾—åˆ°queryã€keyä¸valueã€‚
- ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥æ“ä½œrotary_embï¼Œä½¿ç”¨äº†æ—‹è½¬ä½ç½®åµŒå…¥çš„ä½™å¼¦å’Œæ­£å¼¦éƒ¨åˆ†ï¼Œå°†ä»–ä»¬ä¸queryå’Œkeyç›¸ä¹˜ï¼Œå¹¶å°†ç»“æœç›¸åŠ ï¼Œä»è€Œå®ç°æ—‹è½¬ä½ç½®åµŒå…¥çš„æ•ˆæœã€‚
- å°†key_stateså’Œvalue_statesé‡å¤groupæ¬¡ï¼Œå†æ‰§è¡Œdot attnæ“ä½œã€‚
- åœ¨dot attnæ“ä½œåå¾—åˆ°attn_weights,åŠ ä¸Šattention_maskä»è€Œå®ç°è¯»å–æ©ç›–æ“ä½œï¼Œåœ¨ç»è¿‡softmaxä¸value_statesç›¸ä¹˜ã€‚å¾—åˆ°attn_outputã€‚
- å†å°†ä¸Šè¿°çš„attn_outputè¿›è¡Œreshapeæ“ä½œï¼Œé€å…¥o_projï¼Œå¾—åˆ°æœ€ç»ˆçš„è¾“å‡ºã€‚

![b6fceb434fbc46d94b0cf3683ff4ea4a_GQA](https://github.com/superkong001/learning_in_datawhale/assets/37318654/43f9acf2-389a-439c-afcf-103567b03389)

ä¸»æ—¨:GQAå’ŒMQAä¸éœ€è¦åœ¨æ¨ç†çš„è¿‡ç¨‹å­˜å‚¨é‚£ä¹ˆå¤šçš„kv cache, é‚£ä¹ˆkv cacheå ç”¨çš„æ˜¾å­˜å°±å˜å°ï¼Œé‚£ä¹ˆæˆ‘ä»¬LLM servingå¯ä»¥å¤„ç†çš„è¯·æ±‚æ•°é‡å°±æ›´å¤š

è§£æï¼š

1) åˆå§‹å¼ é‡

```bash
è¾“å…¥ï¼štensor[4, 30](shape:[batch, seq_len]) , headers=32
input_ids = torch.randint(0, qwen2config.vocab_size, (4, 30))
embeddingå: tensor[4, 30, 2048](shape:[batch, seq_len, dim]) 


.view(bsz, q_len, self.num_heads, self.head_dim)å: tensor[4, 30, 32, 64](shape:[batch, seq_len, head, head_dim]) 
.transpose(1, 2)å: tensor[4, 32, 30, 64](shape:[batch, head, seq_len, head_dim]) #æ¯ä¸€å¤´çš„hidden_dim=2048/32=64
åˆ†åˆ«è¾“å…¥åˆ°qã€kã€v
```

```bash
# GQA(grouped-query)æƒ…å†µ:
import torch

# shape:(batch, seq_len, head, head_dim)
query = torch.randn(10, 128, 8, 128)
key = torch.randn(10, 128, 2, 128)
value = torch.randn(10, 128, 2, 128)

## åœ¨æ­¤è®¾ç½®ç»„æ•°ä¸º8/2=4
groups = query.shape[-2] // key.shape[-2]

# keyå’Œvalueéƒ½è¦æ¯”queryå°groupå€ï¼Œä½†æ˜¯ä¸ºåœ¨åç»­åšçŸ©é˜µä¹˜æ³•æ—¶æ–¹ä¾¿ï¼Œæˆ‘ä»¬éœ€è¦å…ˆæŠŠkeyå’Œvalueçš„headé‡å¤åˆ°å’Œqueryç›¸åŒçš„ç»´åº¦ã€‚æ–¹ä¾¿åç»­è®¡ç®—ã€‚
# å®šä¹‰è¾“å…¥xï¼Œ n_repæ˜¯éœ€è¦é‡å¤çš„æ¬¡æ•°ï¼Œåœ¨è¿™é‡Œä¸€èˆ¬æ˜¯ç»„æ•°ï¼Œè¾“å…¥shape:(batch, head, seq_len, head_dim)
def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:

    batch, num_key_value_heads, slen, head_dim = hidden_states.shape
    # dont need repeat here means multi head attention
    if n_rep == 1:
        return hidden_states
    # first we expand x to (bs, seq_len, head, group, head_dim)
    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)
    # reshape make head -> head * group
    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)
```

tips: ä¸ºä»€ä¹ˆè¦ç”¨expandä¹‹åå†reshapeè€Œä¸èƒ½ç›´æ¥ç”¨tensorè‡ªå¸¦çš„repeat?

- expand æ–¹æ³•ç”¨äºå¯¹å¼ é‡è¿›è¡Œæ‰©å±•ï¼Œä½†ä¸å®é™…åˆ†é…æ–°çš„å†…å­˜ã€‚å®ƒè¿”å›çš„å¼ é‡ä¸åŸå§‹å¼ é‡å…±äº«ç›¸åŒçš„æ•°æ®
- repeat æ–¹æ³•é€šè¿‡å®é™…å¤åˆ¶æ•°æ®æ¥æ‰©å±•å¼ é‡ã€‚å®ƒè¿”å›çš„æ–°å¼ é‡ä¸ä¸åŸå§‹å¼ é‡å…±äº«æ•°æ®ï¼Œæ‰©å±•åçš„å¼ é‡å ç”¨äº†æ›´å¤šçš„å†…å­˜ã€‚

2) pos_emb, Qwen2RotaryEmbedding + apply_rotary_pos_emb

### Qwen2RotaryEmbedding

ç›¸å…³çŸ¥è¯†ï¼š

- å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ä½¿ç”¨å·ç§¯æ ¸æ¥æ•è·å•è¯ä¹‹é—´çš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œä½†å…¶ä»…èƒ½æ•è·å›ºå®šå¤§å°çš„å±€éƒ¨ä¸Šä¸‹æ–‡ä¿¡æ¯ã€‚
- å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰åœ¨å¤„ç†åºåˆ—ä¿¡æ¯ä¸Šä¼šæœ‰æ›´å¥½çš„æ•ˆæœï¼Œå…¶ä¾é å¾ªç¯ç»“æ„ï¼Œå°†åºåˆ—ä¿¡æ¯é€æ­¥ä¼ é€’ï¼Œè¿™å…¶ä¸­å°±å¼•å…¥äº†å•è¯çš„ä½ç½®å’Œé¡ºåºä¿¡æ¯ã€‚ä½†éšç€åºåˆ—é•¿åº¦çš„å¢åŠ ï¼ŒRNN ä¼šæ…¢æ…¢å¿˜è®°æ—©å‰çš„ä¿¡æ¯ï¼Œè¿™å°±å¯¼è‡´äº†é•¿æœŸä¾èµ–é—®é¢˜ã€‚é™¤æ­¤ä¹‹å¤–ï¼Œå¾ªç¯ç»“æ„ä¹Ÿä½¿å¾— RNN æ— æ³•å¹¶è¡Œè®¡ç®—ï¼Œè¿™ä½¿å¾— RNN çš„è®­ç»ƒé€Ÿåº¦ååˆ†ç¼“æ…¢ã€‚
- Transformerï¼šç”±äº Transformer ä¸åŒ…å«ä»»ä½•å¾ªç¯ç»“æ„ï¼Œå„ä¸ªå•è¯åœ¨ Transformer ä¸­éƒ½åŒæ—¶ç»è¿‡ Decoder-Encoder çš„å˜æ¢ï¼Œè¿™å°±å¯¼è‡´äº† Transformer æ— æ³•æ•è·å•è¯çš„ä½ç½®ä¿¡æ¯ã€‚

Transformeré‡‡ç”¨çš„æ˜¯é™æ€çš„æ­£å¼¦å’Œä½™å¼¦æ³¢å‡½æ•°çš„ç»„åˆï¼Œä¸»è¦æä¾›ç»å¯¹ä½ç½®ä¿¡æ¯:

<img width="533" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/3ee73abe-a686-429e-8491-fa6dc46d9f5d">

è¿™é‡Œ ğ‘ğ‘œğ‘  æ˜¯è¯åœ¨åºåˆ—ä¸­çš„ä½ç½®ï¼Œğ‘– æ˜¯ä½ç½®å‘é‡ä¸­çš„ç»´åº¦ç´¢å¼•ï¼Œğ‘‘ æ˜¯ä½ç½®å‘é‡çš„ç»´åº¦ï¼ˆé€šå¸¸ä¸æ¨¡å‹çš„éšè—å±‚ç»´åº¦ç›¸åŒï¼Œä¾‹å¦‚512ï¼‰ã€‚è¿™ä¸ªå…¬å¼ä¸­çš„ ${10000^{2n/d}}$ æ˜¯ä¸€ä¸ªç¼©æ”¾å› å­ï¼Œå®ƒéš ğ‘– çš„å¢å¤§è€Œå¢å¤§ï¼Œè¿™æ ·å¯¹äºä¸åŒçš„ ğ‘–ï¼Œæ­£å¼¦å’Œä½™å¼¦å‡½æ•°çš„æ³¢é•¿ä¼šéšä¹‹å¢é•¿ã€‚è¿™ç§è®¾è®¡ä½¿å¾—æ¨¡å‹èƒ½å¤Ÿåœ¨æ¯ä¸ªç»´åº¦æ•æ‰åˆ°ä¸åŒé¢‘ç‡çš„ä½ç½®ä¿¡æ¯ã€‚

æ—‹è½¬ä½ç½®ç¼–ç ï¼ˆRoPEï¼‰ï¼šå¼•å…¥æ—‹è½¬çŸ©é˜µçš„ä½ç½®ç¼–ç ï¼Œä½ç½®ç¼–ç çš„å«ä¹‰æ˜¯å¯¹æ¯ä¸€ä¸ªtokençš„æ¯ä¸€ä¸ªdimèµ‹äºˆä¸åŒçš„ä½ç½®ä¿¡æ¯ã€‚ å…¬å¼å®šä¹‰:

![image](https://github.com/superkong001/learning_in_datawhale/assets/37318654/58f0f9f6-4d7b-4762-b4b5-826af5259975)

æ¦‚å¿µï¼šé€šè¿‡æ—‹è½¬ç¼–ç ï¼Œä½¿å¾—æ¯ä¸ªtokenæ—¢æœ‰ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œåˆæœ‰ç»å¯¹ä½ç½®ä¿¡æ¯ã€‚

- æ—¢èƒ½ä»¥è‡ªæ³¨æ„åŠ›çŸ©é˜µåç½®çš„å½¢å¼ä½œç”¨äº,ç›´æ¥åæ˜ ä¸¤ä¸ªtokençš„ç›¸å¯¹ä½ç½®ä¿¡æ¯ï¼Œåˆèƒ½æ‹†è§£åˆ°å‘é‡å’Œä¸Šï¼Œé€šè¿‡ç›´æ¥ç¼–ç tokençš„ç»å¯¹ä½ç½®å®ç°ã€‚
- RoPEæœ¬è´¨æ˜¯å®ç°å¯¹ç‰¹å¾å‘é‡çš„æ—‹è½¬æ“ä½œï¼Œå¦‚æœä»¥äºŒç»´ç‰¹å¾å‘é‡ä¸¾ä¾‹ï¼Œå¯¹äºç›¸é‚»ä¸¤ä¸ªtokenæ¥è¯´ï¼Œå…¶å¯¹åº”åŒä¸€ä¸ª,å…¶å®šä¹‰ä¸º:

![bcfcb5136238da2cca5641a70169cc23_ROPE2](https://github.com/superkong001/learning_in_datawhale/assets/37318654/3e698be4-2a31-43cf-af96-6e50a8b859cd)

å¯å¾—ï¼Œå…¶æœ¬è´¨å°±æ˜¯: $q_{t}$, $k_{s}$ æ—‹è½¬åçš„ç»“æœï¼Œå°±æ˜¯ $q_{t}$, $k_{s}$ä¹˜ä¸Šcoså†åŠ ä¸Š $q_{t}$, $k_{s}$ç¿»è½¬ç»´åº¦å¹¶å–åä¸€ç»´åä¹˜ä¸Šsinã€‚
- å¯¹äºé«˜çº¬å‘é‡ï¼Œç”±äºå¥‡ã€å¶æ•°ç»´åº¦ä¸¤ä¸¤äº¤é”™å®ç°è¾ƒä¸ºå¤æ‚ï¼Œåˆ™ç°åœ¨å¯ç®€åŒ–ä¸ºå°†ç‰¹å¾ç»´åº¦ä¸€åˆ‡äºŒï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œåœ¨å®ç°è¿‡ç¨‹ä¸­å¯¹å‰åå„åŠè¿›è¡Œçš„æ“ä½œå³ä¸ºrotate_halfæ“ä½œï¼š

![b9732c2d7d6e7e265bfd933fb481cc9b_ROPE3](https://github.com/superkong001/learning_in_datawhale/assets/37318654/2204dd5d-2fae-4455-9fb0-600c17c3aa11)

```bash
# Copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Qwen2
class Qwen2RotaryEmbedding(nn.Module):
    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):
        super().__init__()
        # å®šä¹‰åˆå§‹å€¼
        self.dim = dim # æ—‹è½¬åµŒå…¥çš„ç»´åº¦
        self.max_position_embeddings = max_position_embeddings # æœ€å¤§çš„ä½ç½®ç´¢å¼•ï¼Œç”¨äºå®šä¹‰æœ€å¤§çš„åºåˆ—é•¿åº¦
        self.base = base # é»˜è®¤10000ï¼Œè®¡ç®—é¢‘ç‡çš„åŸºæ•°ï¼Œé€šå¸¸ç”¨äºè°ƒèŠ‚ä½ç½®ç¼–ç çš„å‘¨æœŸæ€§
        # å®šä¹‰æ—‹è½¬è§’Î¸n=10000^(âˆ’2n/d)ï¼Œå…¶ä¸­nè¡¨ç¤ºç»´åº¦æ•°ï¼Œå…¶å–å€¼èŒƒå›´ä¸º[0, 1, ..., d/2-1]
        # å¦‚ï¼š2/64=0.0312ï¼Œ10000^0.0312=1.3335ï¼Œ1/1.3335=7.4989e-01
        # torch.arange(0, self.dim, 2, dtype=torch.int64)ç”Ÿæˆä»0å¼€å§‹åˆ°self.dimï¼ˆä½†ä¸åŒ…æ‹¬self.dimï¼‰ï¼Œæ­¥é•¿ä¸º2çš„åºåˆ—ã€‚
        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))

        # æ³¨å†Œç¼“å†²åŒºï¼ˆbufferï¼‰. ç¬¬ä¸€ä¸ªå‚æ•°"inv_freq"ç¼“å†²åŒºåå­—ï¼Œç¬¬äºŒä¸ªå‚æ•° (inv_freq)ç¼“å†²åŒºçš„å®é™…æ•°æ®ï¼Œç¬¬ä¸‰ä¸ªå‚æ•° (persistent=False)ä¸ä¿å­˜è¿™ä¸ªç¼“å†²åŒº
        self.register_buffer("inv_freq", inv_freq, persistent=False)

        # Build here to make `torch.jit.trace` work.
        self._set_cos_sin_cache(
            seq_len=max_position_embeddings, device=self.inv_freq.device, dtype=torch.get_default_dtype()
        )

    # ä¸ºseqé‡Œé¢çš„æ¯ä¸ªtokenå½¢æˆç‹¬ä¸€æ— äºŒçš„æ—‹è½¬è§’åµŒå…¥(å¤–ç§¯)
    def _set_cos_sin_cache(self, seq_len, device, dtype):
        self.max_seq_len_cached = seq_len
        t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)

        # å°†å‰é¢ç”Ÿæˆè§’åº¦(inv_freq)ä¸æ¯ä¸€ä¸ªä½ç½®ä¹˜ç§¯ï¼ŒåŒºåˆ†ä¸€ä¸ªseqä¸­å„ä¸ªè¯
        # torch.outerè¡¨ç¤ºä¸¤ä¸ªå‘é‡å¤–ç§¯ï¼Œå³ç¬¬ä¸€ä¸ªå‘é‡é€ä¸ªå…ƒç´ ä¸ç¬¬äºŒä¸ªå‘é‡ç›¸ä¹˜å¾—åˆ°æ¯ä¸ªç»“æœå•ç‹¬ä¿å­˜ä¸ºä¸€è¡Œã€‚
        #  t çš„é•¿åº¦ä¸º Lï¼ˆä»£è¡¨åºåˆ—é•¿åº¦ï¼‰ä¸” inv_freq çš„é•¿åº¦ä¸º D/2ï¼ˆå‡è®¾ dim=Dï¼‰ï¼Œé‚£ä¹ˆ freqs çš„å½¢çŠ¶æ˜¯ L x (D/2)ã€‚æœ€ç»ˆå½¢çŠ¶ä¸º(1024,32)
        freqs = torch.outer(t, self.inv_freq)
        # ç”Ÿæˆè§’åº¦ä¿¡æ¯(åˆ©ç”¨æ³¨å†Œæœºåˆ¶ç”Ÿæˆself.cos_cachedä¸sin_cached)
        # Different from paper, but it uses a different permutation in order to obtain the same calculation
        # embå°†äºŒè€…catèµ·æ¥(åˆ—æ–¹å‘æ‹¼æ¥)ï¼Œå¾—åˆ°dimç»´åº¦ï¼Œæ¯dim/2ä¸€å¾ªç¯ã€‚ä¸ºä¸€ä¸ªå½¢çŠ¶ä¸º L x D (1024, 64)çš„çŸ©é˜µï¼Œå…¶ä¸­ L æ˜¯åºåˆ—é•¿åº¦ï¼ŒD æ˜¯ç¼–ç çš„å®Œæ•´ç»´åº¦ã€‚
        # é€šè¿‡æ‹¼æ¥ä¸¤ä»½ freqsï¼Œå¯ä»¥ç¡®ä¿å¯¹äºæ¯ä¸ªä½ç½®ç´¢å¼• iï¼Œæœ‰è¶³å¤Ÿçš„é¢‘ç‡å€¼æ¥åŒæ—¶è®¡ç®—å…¶æ­£å¼¦å’Œä½™å¼¦ã€‚
        emb = torch.cat((freqs, freqs), dim=-1)
        self.register_buffer("cos_cached", emb.cos().to(dtype), persistent=False)
        self.register_buffer("sin_cached", emb.sin().to(dtype), persistent=False)

    def forward(self, x, seq_len=None):
        # x: [batch_size, num_attention_heads, seq_len, head_size]
        if seq_len > self.max_seq_len_cached:
            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)

        # åœ¨å–å‡ºä½ç½®ç¼–ç ä¿¡æ¯cosä¸sinçš„æ—¶å€™ï¼Œå°±æ˜¯å°†seqçš„éƒ¨åˆ†åˆ‡å‡ºæ¥ï¼ŒåŸå…ˆè®¾ç½®çš„1024æ˜¯æœ€å¤§posç¼–ç ï¼Œæ¯æ¬¡ç”¨çš„æ—¶å€™åªå–å½“ä¸‹seq_lençš„å³å¯ï¼Œ
        # ä¹‹å‰æ±‚å¾—å¤–ç§¯ï¼Œæ˜¯ä¸ºäº†ä¿è¯seqé‡Œé¢å¾—æ¯ä¸€ä¸ªè¯éƒ½èƒ½æœ‰ä¸åŒçš„1024ä¸ªä½ç½®ç¼–ç ã€‚
        return (
            self.cos_cached[:seq_len].to(dtype=x.dtype),
            self.sin_cached[:seq_len].to(dtype=x.dtype),
        )
```

a) ç”Ÿæˆè§’åº¦: $$\theta = \left(\frac{1}{10000^{2n/d}}\right)$$

å…¶ä¸­ï¼Œnè¡¨ç¤ºç»´åº¦æ•°ï¼Œå…¶å–å€¼èŒƒå›´ä¸º[0, 1, ..., d/2-1]

b) å°†ä¸Šè¿°ç”Ÿæˆè§’åº¦ä¸æ¯ä¸€ä¸ªä½ç½®ä¹˜ç§¯ï¼ŒåŒºåˆ†ä¸€ä¸ªseqä¸­å„ä¸ªè¯ï¼šå…¶å®ç­‰ä»·äº:
$$\thetaÂ =Â \left(\frac{i}{10000^{2n/d}}\right)$$  
å…¶ä¸­: `i`ä¸ºè¡Œæ•°ã€‚

c) embå°†äºŒè€…catèµ·æ¥ï¼Œå¾—åˆ°dimç»´åº¦ï¼Œæ¯dim/2ä¸€å¾ªç¯ã€‚

d) åœ¨å–å‡ºä½ç½®ç¼–ç ä¿¡æ¯cosä¸sinçš„æ—¶å€™ï¼Œå°±æ˜¯å°†seqçš„éƒ¨åˆ†åˆ‡å‡ºæ¥ï¼ŒåŸå…ˆè®¾ç½®çš„1024æ˜¯æœ€å¤§posç¼–ç ï¼Œæ¯æ¬¡ç”¨çš„æ—¶å€™åªå–å½“ä¸‹seq_lençš„å³å¯.ä¹‹å‰æ±‚å¾—å¤–ç§¯ï¼Œæ˜¯ä¸ºäº†ä¿è¯seqé‡Œé¢å¾—æ¯ä¸€ä¸ªè¯éƒ½èƒ½æœ‰ä¸åŒçš„1024ä¸ªä½ç½®ç¼–ç ã€‚

e) è¿›è¡Œæ—‹è½¬åµŒå…¥ã€‚

å°† ğ‘ è§†ä¸ºå¤æ•°ï¼Œå…¶ä¸­å®éƒ¨å’Œè™šéƒ¨åˆ†åˆ«æ˜¯ ğ‘ å‘é‡çš„ä¸¤ä¸ªåˆ†é‡ã€‚ $ğ‘’^ğ‘–ğœƒ$ æ˜¯ç”± cosâ¡(ğœƒ)+ğ‘–sinâ¡(ğœƒ) è¡¨ç¤ºçš„å•ä½å¤æ•°

å¤æ•°ä¹˜æ³•å¯ä»¥è¡¨ç¤ºä¸ºä¸¤ä¸ªå¤æ•°ç›¸ä¹˜ã€‚å¦‚æœä½ æŠŠä¸€ä¸ªå¤æ•° ğ‘+ğ‘ğ‘– ä¸å¦ä¸€ä¸ªå¤æ•° ğ‘+ğ‘‘ğ‘– ç›¸ä¹˜ï¼Œç»“æœæ˜¯ ğ‘ğ‘âˆ’ğ‘ğ‘‘+(ğ‘ğ‘‘+ğ‘ğ‘)ğ‘–ã€‚

```bash
# ååŠéƒ¨åˆ†å’Œå‰åŠéƒ¨åˆ†è¿›è¡Œäº†äº¤æ¢ï¼Œå¹¶ä¸”å°†ååŠéƒ¨åˆ†çš„ç¬¦å·å–åã€‚
def rotate_half(x):
    """Rotates half the hidden dims of the input."""
    # x1 è¢«å®šä¹‰ä¸ºå¼ é‡ x æœ€åä¸€ä¸ªç»´åº¦çš„å‰åŠéƒ¨åˆ†ã€‚
    x1 = x[..., : x.shape[-1] // 2]
    x2 = x[..., x.shape[-1] // 2 :]
    return torch.cat((-x2, x1), dim=-1)

def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    query and key tensors rotated using the Rotary Position Embedding.
    """
    # unsqueeze(-1)  # è´Ÿ1è¡¨ç¤ºï¼Œåœ¨æœ€åä¸€ç»´ä¸Šæ·»åŠ ä¸€ä¸ªç»´åº¦
    # ä½¿å¾—ç»´åº¦ä¸æŸ¥è¯¢å’Œé”®å¼ é‡åŒ¹é…ï¼Œä»è€Œå¯ä»¥æ‰§è¡Œå…ƒç´ çº§ä¹˜æ³•ï¼ˆå¹¿æ’­ï¼‰ã€‚
    cos = cos[position_ids].unsqueeze(unsqueeze_dim)
    sin = sin[position_ids].unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

### apply_rotary_pos_emb

```bash
# cos.shape(head, head_dim), sin.shape(head, head_dim)
# Copied from transformers.models.mistral.modeling_mistral.apply_rotary_pos_emb
def apply_rotary_pos_emb(q, k, cos, sin, position_ids, unsqueeze_dim=1):
    """Applies Rotary Position Embedding to the query and key tensors.

    Args:
        q (`torch.Tensor`): The query tensor.
        k (`torch.Tensor`): The key tensor.
        cos (`torch.Tensor`): The cosine part of the rotary embedding.
        sin (`torch.Tensor`): The sine part of the rotary embedding.
        position_ids (`torch.Tensor`):
            The position indices of the tokens corresponding to the query and key tensors. For example, this can be
            used to pass offsetted position ids when working with a KV-cache.
        unsqueeze_dim (`int`, *optional*, defaults to 1):
            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and
            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note
            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and
            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes
            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have
            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.
    Returns:
        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.
    """
    cos = cos[position_ids].unsqueeze(unsqueeze_dim)
    sin = sin[position_ids].unsqueeze(unsqueeze_dim)
    q_embed = (q * cos) + (rotate_half(q) * sin)
    k_embed = (k * cos) + (rotate_half(k) * sin)
    return q_embed, k_embed
```

4) çŸ©é˜µä¹˜æ³•å¾—åˆ°scoreä¸output åé¢å°±æ˜¯çœŸæ­£çš„kqvç›¸ä¹˜äº†

```bash
# GQA(grouped-query)æƒ…å†µ:
# åˆå§‹shape:(batch, seq_len, head, head_dim) => shape:(batch, head, seq_len, head_dim)
query = query.transpose(1, 2)
# è¾“å…¥shape:(batch, seq_len, head, head_dim) => shape:(batch, head * n_rep, seq_len, head_dim)
key = repeat_kv(key, 4).transpose(1, 2)
value = repeat_kv(value, 4).transpose(1, 2)

# q*kT/head_dim^0.5 
scores = torch.matmul(query, key.transpose(2, 3)) / math.sqrt(head_dim) # shape:(batch, head, seq_len, kv_seq_len)
scores = torch.nn.functional.softmax(scores, dim=-1)

# (batch, head, seq_len, kv_seq_len)*(batch, head, seq_len, head_dim)=(batch, head, seq_len, head_dim)
out = torch.matmul(scores, value)
#ä¸Šä¸€æ­¥è½¬ç½®äº†ï¼Œè¿˜å¾—è½¬å›å»(batch, seq_len, head, head_dim)
out = out.transpose(1, 2)
```

å®Œæ•´ä»£ç ï¼š

```bash
    def forward(
        self,
        hidden_states: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        position_ids: Optional[torch.LongTensor] = None,
        past_key_value: Optional[Cache] = None,
        output_attentions: bool = False,
        use_cache: bool = False,
        **kwargs,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:
        if "padding_mask" in kwargs:
            warnings.warn(
                "Passing `padding_mask` is deprecated and will be removed in v4.37. Please make sure use `attention_mask` instead.`"
            )
        # è·å–å½¢çŠ¶ä¿¡æ¯,hidden_statesè¾“å…¥çš„ä¸º(bs,T,hd)
        bsz, q_len, _ = hidden_states.size()

        # å¯¹hidden_statesè¿›è¡ŒLinearç”Ÿæˆqueryã€keyã€value
        query_states = self.q_proj(hidden_states)
        key_states = self.k_proj(hidden_states)
        value_states = self.v_proj(hidden_states)

        # reshapeå¤šå¤´å¤„ç†--åˆ†å—--(bs,T,heads,hd_d)ï¼Œäº¤æ¢æ•°ç»„çš„ç¬¬äºŒä¸ªç»´åº¦ï¼ˆç´¢å¼•ä¸º1ï¼‰å’Œç¬¬ä¸‰ä¸ªç»´åº¦
        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)
        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)
        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)

        kv_seq_len = key_states.shape[-2] # = q_len
        if past_key_value is not None:
            if self.layer_idx is None:
                raise ValueError(
                    f"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} "
                    "for auto-regressive decoding with k/v caching, please make sure to initialize the attention class "
                    "with a layer index."
                )
            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)
        # å°†æ—‹è½¬ä½ç½®åµŒå…¥åº”ç”¨äºæŸ¥è¯¢å’Œé”®å¼ é‡ã€‚ä½¿ç”¨äº†æ—‹è½¬ä½ç½®åµŒå…¥çš„ä½™å¼¦å’Œæ­£å¼¦éƒ¨åˆ†ï¼Œå°†å®ƒä»¬ä¸æŸ¥è¯¢å’Œé”®å¼ é‡ç›¸ä¹˜ï¼Œå¹¶å°†ç»“æœç›¸åŠ ï¼Œä»è€Œå®ç°æ—‹è½¬ä½ç½®åµŒå…¥çš„æ•ˆæœ
        cos, sin = self.rotary_emb(value_states, seq_len=kv_seq_len)
        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)

        if past_key_value is not None:
            cache_kwargs = {"sin": sin, "cos": cos}  # Specific to RoPE models
            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)

        # å…ˆå°†key_stateså’Œvalue_statesé‡å¤äº†num_key_value_groupsæ¬¡ï¼ˆGQAï¼‰
        # repeat k/v heads if n_kv_heads < n_heads
        key_states = repeat_kv(key_states, self.num_key_value_groups)
        value_states = repeat_kv(value_states, self.num_key_value_groups)

        # ä½¿ç”¨dot attnå®ç°q*kT/hd_d^0.5
        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)

        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):
            raise ValueError(
                f"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is"
                f" {attn_weights.size()}"
            )

        # ç„¶å attn_weights åŠ ä¸Š attention_maskï¼Œå®ç°è¯»å–é¡ºåº
        if attention_mask is not None:
            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):
                raise ValueError(
                    f"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}"
                )

            attn_weights = attn_weights + attention_mask

        # softmax + dropout + values_statesç›¸ä¹˜
        # upcast attention to fp32
        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)
        attn_output = torch.matmul(attn_weights, value_states)

        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):
            raise ValueError(
                f"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is"
                f" {attn_output.size()}"
            )

        # è½¬ç½®ï¼Œä¿®æ”¹å½¢çŠ¶ç­‰reshapeæ“ä½œ
        attn_output = attn_output.transpose(1, 2).contiguous()
        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)

        # æœ€ååœ¨è¿›è¡Œä¸€æ¬¡o_proj
        attn_output = self.o_proj(attn_output)

        if not output_attentions:
            attn_weights = None

        # è¿”å›ç»“æœ
        return attn_output, attn_weights, past_key_value
```

# Qwen2 MLP

![38d5a025fe702e2d3b1aa624355d90c4_MLP1](https://github.com/superkong001/learning_in_datawhale/assets/37318654/d236cc58-f3bd-4b2b-a591-e5757f211fa7)

è¾“å…¥hidden_stateå¹¶è¡Œé€å…¥ä¸¤ä¸ªLinearå±‚ï¼Œå…¶ä¸­ä¸€ä¸ªæ¿€æ´»ä¸€ä¸‹ï¼Œå†ä¸å¦ä¸€ä¸ªç›¸ä¹˜ï¼Œæœ€ç»ˆå†ç»è¿‡ä¸€ä¸ªLinearï¼Œè¾“å‡ºæœ€ç»ˆç»“æœã€‚

```bash
# Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Qwen2
class Qwen2MLP(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.config = config
        self.hidden_size = config.hidden_size
        self.intermediate_size = config.intermediate_size
        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)
        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)
        self.act_fn = ACT2FN[config.hidden_act]

    def forward(self, x):
        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))
```

# Qwen2RMSNorm, æ ¹å‡æ–¹å½’ä¸€åŒ–

![7d54bafe8e22779a9b9b169b66fe2cea_RMSNorm_formulation](https://github.com/superkong001/learning_in_datawhale/assets/37318654/42f21607-de36-407c-a8d7-75adbacedf3c)

```bash
# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Qwen2
class Qwen2RMSNorm(nn.Module):
    def __init__(self, hidden_size, eps=1e-6):
        """
        Qwen2RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size))
        self.variance_epsilon = eps

    def forward(self, hidden_states):
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        # .pow(2).mean(-1, keepdim=True)è¡¨ç¤ºå¯¹æ¯ä¸ªå…ƒç´ æ±‚å¹³æ–¹ï¼Œç„¶åè®¡ç®—å¼ é‡åœ¨æœ€åä¸€ä¸ªç»´åº¦ï¼ˆç”± -1 æŒ‡å®šï¼‰ä¸Šçš„å¹³å‡å€¼ï¼ˆæ¯ä¸€è¡Œçš„å¹³å‡å€¼ï¼‰å¹¶ä¿æŒç»´åº¦
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        # rsqrtè¡¨ç¤ºå¼€æ ¹çš„å¯¼æ•°
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)
```

<img width="530" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/a3664578-23ad-46b8-9f6e-d0e43f04e9bb">






