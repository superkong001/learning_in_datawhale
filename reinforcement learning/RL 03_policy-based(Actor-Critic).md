策略梯度算法是直接对策略本身进行近似优化：将策略描述成一个带有参数$\theta$的连续函数，该函数将某个状态作为输入，输出的不再是某个确定性（ $\text{deterministic}$ ）的离散动作，而是对应的动作概率分布，通常用 $\pi_{\theta}(a|s)$ 表示，称作随机性（ $\text{stochastic}$ ）策略。

基于价值算法的缺点：
1. 无法表示连续动作，如：机器人的运动控制问题。因为DQN等算法是通过学习状态和动作的价值函数来间接指导策略，因此它们只能处理离散动作空间的问题。
2. 高方差，影响算法的收敛性。
3. 不能很好地平衡探索与利用的关系。DQN等算法在实现时通常选择贪心的确定性策略，而很多问题的最优策略是随机策略，即需要以不同的概率选择不同的动作。



