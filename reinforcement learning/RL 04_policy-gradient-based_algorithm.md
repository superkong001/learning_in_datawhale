# 深度确定性策略梯度算法（deep deterministic policy gradient，DDPG）

DDPG是一种确定性的策略梯度算法。由于DQN算法的一个主要缺点就是不能用于连续动作空间，因此要适配连续动作空间，就将选择动作的过程变成一个直接从状态映射到具体动作的函数 $\mu_\theta (s)$ ，其中 $\theta$ 表示模型的参数，这样就把求解 $Q$ 函数、贪心选择动作这两个过程合并成了一个函数Actor。 $Q(s,a)$ 函数有两个变量的，相当于一个曲线平面，当输入某个状态到 $\text{Actor}$ 时，即固定 $s=s_t$ 时，则相当于把曲线平面截断成一条曲线。而Actor的任务就是寻找这条曲线的最高点，并返回对应的横坐标，即最大 $Q$ 值对应的动作，所以DDPG是在寻找最大值。

<img width="391" alt="image" src="https://github.com/superkong001/learning_in_datawhale/assets/37318654/c3a56242-545e-475a-b35b-8e76a7acf83f">

在强化学习基础算法的研究改进当中主要基础核心问题：
1) 如何提高对值函数的估计，保证其准确性，即尽量无偏且低方差；
2) 如何提高探索以及平衡探索-利用的问题，尤其在探索性比较差的确定性策略中。

## OU(Ornstein-Uhlenbeck)噪声

DDPG算法中使用是 $\text{OU}$ 噪声。它是一种具有回归特性的随机过程，其与高斯噪声相比的优点在于：
* **探索性**：$\text{OU}$ 噪声具有持续的、自相关的特性。
* **控制幅度**：$\text{OU}$ 噪声可以通过调整其参数来控制噪声的幅度。
* **稳定性**：$\text{OU}$ 噪声的回归特性使得噪声在训练过程中具有一定的稳定性。
* **可控性**：由于$\text{OU}$ 噪声具有回归特性，它在训练过程中逐渐回归到均值，

OU噪声主要由两个部分组成：随机高斯噪声和回归项：

$$
d x_t=\theta\left(\mu-x_t\right) d t+\sigma d W_t
$$

