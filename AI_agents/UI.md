# Gradio vs Streamlit
[Gradio](https://www.gradio.app/)æœ‰ è¾“å…¥è¾“å‡ºç»„ä»¶ã€æ§åˆ¶ç»„ä»¶ã€å¸ƒå±€ç»„ä»¶ å‡ ä¸ªåŸºç¡€æ¨¡å—ï¼Œå…¶ä¸­
- è¾“å…¥è¾“å‡ºç»„ä»¶ ç”¨äºå±•ç¤ºå†…å®¹å’Œè·å–å†…å®¹ï¼Œå¦‚ï¼š Textbox æ–‡æœ¬ã€ Image å›¾åƒ
- å¸ƒå±€ç»„ä»¶ ç”¨äºæ›´å¥½åœ°è§„åˆ’ç»„ä»¶çš„å¸ƒå±€ï¼Œå¦‚ï¼š Column ï¼ˆæŠŠç»„ä»¶æ”¾æˆä¸€åˆ—ï¼‰ã€ Row ï¼ˆæŠŠç»„ä»¶æ”¾æˆä¸€è¡Œï¼‰
  - æ¨èä½¿ç”¨ gradio.Blocks() åšæ›´å¤šä¸°å¯Œäº¤äº’çš„ç•Œé¢ï¼Œ gradio.Interface() åªæ”¯æŒå•ä¸ªå‡½æ•°äº¤äº’
- æ§åˆ¶ç»„ä»¶ ç”¨äºç›´æ¥è°ƒç”¨å‡½æ•°ï¼Œæ— æ³•ä½œä¸ºè¾“å…¥è¾“å‡ºä½¿ç”¨ï¼Œå¦‚ï¼š Button ï¼ˆæŒ‰é’®ï¼‰ã€ ClearButton ï¼ˆæ¸…é™¤æŒ‰é’®ï¼‰

Gradioçš„è®¾è®¡å“²å­¦æ˜¯å°†è¾“å…¥å’Œè¾“å‡ºç»„ä»¶ä¸å¸ƒå±€ç»„ä»¶åˆ†å¼€ã€‚è¾“å…¥ç»„ä»¶ï¼ˆå¦‚ Textbox ã€ Slider ç­‰ï¼‰ç”¨äºæ¥æ”¶ç”¨æˆ·è¾“å…¥ï¼Œè¾“å‡ºç»„ä»¶ï¼ˆå¦‚ Label ã€ Image ç­‰ï¼‰ç”¨äºæ˜¾ç¤ºå‡½æ•°çš„è¾“å‡ºç»“æœã€‚è€Œå¸ƒå±€ç»„ä»¶ï¼ˆå¦‚ Tabs ã€ Columns ã€ Row ç­‰ï¼‰åˆ™ç”¨äºç»„ç»‡å’Œæ’åˆ—è¿™äº›è¾“å…¥å’Œè¾“å‡ºç»„ä»¶ï¼Œä»¥åˆ›å»ºç»“æ„åŒ–çš„ç”¨æˆ·ç•Œé¢ã€‚

å¦‚æœæƒ³äº†è§£æ›´å¤šç»„ä»¶è¯¦æƒ…ï¼Œå¯æŸ¥çœ‹[å®˜æ–¹æ–‡æ¡£](https://www.gradio.app/guides/quickstart)ï¼›

å¦å¤–ï¼Œå¦‚æœæƒ³è®¾è®¡æ›´å¤æ‚çš„ç•Œé¢é£æ ¼ï¼Œè¿˜å¯ä»¥æŸ¥çœ‹å­¦ä¹ [å®˜æ–¹æ–‡æ¡£ï¼šä¸»é¢˜](https://www.gradio.app/guides/theming-guide)

[Streamlit](https://streamlit.io/) åŸºç¡€æ¦‚å¿µ

[Streamlitå®˜æ–¹çš„è§†é¢‘æ¼”ç¤º](https://datawhale-business.oss-cn-hangzhou.aliyuncs.com/dashboard/dipwap/1763316509295/hero-video.mp4)ï¼Œå¯ä»¥ç›´æ¥è¾“å…¥markdownæ ¼å¼çš„æ–‡æœ¬ï¼Œç½‘é¡µå³å¯æ¸²æŸ“å¥½ã€‚

Streamlitä¸­æ²¡æœ‰gradioçš„è¾“å…¥å’Œè¾“å‡ºæ¦‚å¿µï¼Œä¹Ÿæ²¡æœ‰å¸ƒå±€ç»„ä»¶çš„æ¦‚å¿µã€‚

Gradioå’ŒStreamlitä¸­çš„ã€æ–‡ä»¶ç»„ä»¶ã€‘å¯¹æ¯”å¦‚ä¸‹å›¾ï¼š

<img width="300" height="300" alt="d2c6d7ed0d253943bb09e9a8e652d7b4_62d8505a-3a42-47e1-b327-fc4aae0f2b4b" src="https://github.com/user-attachments/assets/c3969f83-b2dc-4993-850a-7c08cc5a3cd5" />
<img width="300" height="300" alt="f89b90f0c50dbf2347f7a8141c7a397e_fa54dc19-1739-47c1-b141-e0635ae2a4d3" src="https://github.com/user-attachments/assets/1c5495b9-2c4c-4ee4-b704-3fa151cf5bc1" />

Streamlitæ¯ä¸ªç»„ä»¶éƒ½æ˜¯ç‹¬ç«‹çš„ï¼Œéœ€è¦ç”¨ä»€ä¹ˆç›´æ¥æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£å³å¯ï¼Œå¤§è‡´æœ‰å¦‚ä¸‹å‡ ç§ç»„ä»¶ï¼š

é¡µé¢å…ƒç´ 
- æ–‡æœ¬
- æ•°æ®è¡¨æ ¼
- å›¾æ ‡ç»˜åˆ¶ï¼ˆæŸ±çŠ¶å›¾ï¼Œæ•£ç‚¹å›¾ç­‰ï¼‰
- è¾“å…¥ï¼ˆæ–‡æœ¬æ¡†ï¼ŒæŒ‰é’®ï¼Œä¸‹æ‹‰æ¡†ï¼Œæ»‘å—ï¼Œå¤é€‰æ¡†ï¼Œæ–‡ä»¶ä¸Šä¼ ï¼Œç­‰ï¼‰
- å¤šåª’ä½“ï¼ˆå›¾ç‰‡ï¼ŒéŸ³é¢‘ï¼Œè§†é¢‘ï¼‰
- å¸ƒå±€å’Œå®¹å™¨
- Chatï¼ˆèŠå¤©å¯¹è¯æ§ä»¶ï¼‰
- çŠ¶æ€ï¼ˆè¿›åº¦æ¡ï¼ŒåŠ è½½ä¸­ï¼Œç­‰ç­‰å…ƒç´ ï¼‰
- ç¬¬ä¸‰æ–¹ç»„ä»¶ï¼ˆæä¾›äº†æ›´åŠ ä¸°å¯Œçš„ç»„ä»¶ï¼‰

åº”ç”¨é€»è¾‘
- å¯¼èˆªå’Œé¡µé¢ï¼ˆå¯ä»¥åˆ‡æ¢é¡µé¢ï¼‰
- æ‰§è¡Œæµç¨‹
- ç¼“å­˜å’ŒçŠ¶æ€
- è¿æ¥å’ŒåŠ å¯†ï¼ˆå¯è¿æ¥æ•°æ®åº“ï¼Œä¹Ÿå¯ä»¥å¯¹å†…å®¹è¿›è¡ŒåŠ å¯†å¤„ç†ï¼‰
- è‡ªå®šä¹‰ç»„ä»¶
- å…¬å…±ç»„ä»¶ï¼ˆç”¨æˆ·ä¿¡æ¯å­˜å‚¨ï¼Œå¸®åŠ©ï¼Œä»¥åŠè¾“å‡ºhtmlï¼‰
- Configï¼ˆä½¿ç”¨é…ç½®æ–‡ä»¶ï¼Œæ¥å®šä¹‰ä¸€äº›å†…å®¹ï¼‰

å·¥å…·
- åº”ç”¨æµ‹è¯•
- å‘½ä»¤è¡Œ

# æ¡ˆä¾‹ï¼šæ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹

```python
# å¯¼å…¥æ‰€éœ€çš„åº“
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
import streamlit as st

# åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªå‰¯æ ‡é¢˜
st.title("ğŸ’¬ Yuan2.0 æ™ºèƒ½ç¼–ç¨‹åŠ©æ‰‹")

# æºå¤§æ¨¡å‹ä¸‹è½½
from modelscope import snapshot_download
model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='./')

# å®šä¹‰æ¨¡å‹è·¯å¾„
path = './IEITYuan/Yuan2-2B-Mars-hf'

# å®šä¹‰æ¨¡å‹æ•°æ®ç±»å‹
torch_dtype = torch.bfloat16 # A10
# torch_dtype = torch.float16 # P100

# å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºè·å–æ¨¡å‹å’Œtokenizer
@st.cache_resource
def get_model():
    print("Creat tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')
    tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

    print("Creat model...")
    model = AutoModelForCausalLM.from_pretrained(path, torch_dtype=torch_dtype, trust_remote_code=True).cuda()

    return tokenizer, model

# åŠ è½½modelå’Œtokenizer
tokenizer, model = get_model()

# åˆæ¬¡è¿è¡Œæ—¶ï¼Œsession_stateä¸­æ²¡æœ‰"messages"ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªç©ºåˆ—è¡¨
if "messages" not in st.session_state:
    st.session_state["messages"] = []

# æ¯æ¬¡å¯¹è¯æ—¶ï¼Œéƒ½éœ€è¦éå†session_stateä¸­çš„æ‰€æœ‰æ¶ˆæ¯ï¼Œå¹¶æ˜¾ç¤ºåœ¨èŠå¤©ç•Œé¢ä¸Š
for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

# å¦‚æœç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ¡†ä¸­è¾“å…¥äº†å†…å®¹ï¼Œåˆ™æ‰§è¡Œä»¥ä¸‹æ“ä½œ
if prompt := st.chat_input():
    # å°†ç”¨æˆ·çš„è¾“å…¥æ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "user", "content": prompt})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºç”¨æˆ·çš„è¾“å…¥
    st.chat_message("user").write(prompt)

    # è°ƒç”¨æ¨¡å‹
    prompt = "<n>".join(msg["content"] for msg in st.session_state.messages) + "<sep>" # æ‹¼æ¥å¯¹è¯å†å²
    inputs = tokenizer(prompt, return_tensors="pt")["input_ids"].cuda()
    outputs = model.generate(inputs, do_sample=False, max_length=1024) # è®¾ç½®è§£ç æ–¹å¼å’Œæœ€å¤§ç”Ÿæˆé•¿åº¦
    output = tokenizer.decode(outputs[0])
    response = output.split("<sep>")[-1].replace("<eod>", '')

    # å°†æ¨¡å‹çš„è¾“å‡ºæ·»åŠ åˆ°session_stateä¸­çš„messagesåˆ—è¡¨ä¸­
    st.session_state.messages.append({"role": "assistant", "content": response})

    # åœ¨èŠå¤©ç•Œé¢ä¸Šæ˜¾ç¤ºæ¨¡å‹çš„è¾“å‡º
    st.chat_message("assistant").write(response)
```

