参考：
- [hello-agents 教程](https://github.com/datawhalechina/hello-agents/tree/main)
- [hello-agents 框架开发](https://github.com/jjyaoao/helloagents)

# 概念

在人工智能领域，智能体被定义为任何能够通过 **传感器（Sensors）** 感知其所处 **环境（Environment）** ，并 **自主（Autonomy）** 地通过 **执行器（Actuators）** 采取 **行动（Action）** 以达成特定目标的实体。

<div align="center">
  <img width="605" height="284" alt="image" src="https://github.com/user-attachments/assets/562e2ad9-128f-4a81-ba8a-7ef530fe4890" />
</div>

## 智能体演进

- **反射智能体（Simple Reflex Agent）** ：它们的决策核心由工程师明确设计的“条件-动作”规则构成。经典的自动恒温器便是如此：若传感器感知的室温高于设定值，则启动制冷系统。
- **基于模型的反射智能体（Model-Based Reflex Agent）** ：引入了“状态”。这类智能体拥有一个内部的世界模型（World Model），用于追踪和理解环境中那些无法被直接感知的方面。它试图回答：“世界现在是什么样子的？”。例如，一辆在隧道中行驶的自动驾驶汽车，即便摄像头暂时无法感知到前方的车辆，它的内部模型依然会维持对那辆车存在、速度和预估位置的判断。这个内部模型让智能体拥有了初级的“记忆”，使其决策不再仅仅依赖于瞬时感知，而是基于一个更连贯、更完整的世界状态理解。
- **基于目标的智能体（Goal-Based Agent）** ：增加了有明确的目标。与前两者不同，它的行为不再是被动地对环境做出反应，而是主动地、有预见性地选择能够导向某个特定未来状态的行动。这类智能体需要回答的问题是：“我应该做什么才能达成目标？”。经典的例子是 GPS 导航系统：你的目标是到达公司，智能体会基于地图数据（世界模型），通过搜索算法（如 A*算法）来规划（Planning）出一条最优路径。这类智能体的核心能力体现在了对未来的考量与规划上。
- **基于效用的智能体（Utility-Based Agent）** ：引入多个目标。它为每一个可能的世界状态都赋予一个效用值，这个值代表了满意度的高低。智能体的核心目标不再是简单地达成某个特定状态，而是最大化期望效用。它需要回答一个更复杂的问题：“哪种行为能为我带来最满意的结果？”。这种架构让智能体学会在相互冲突的目标之间进行权衡，使其决策更接近人类的理性选择。
- **学习型智能体（Learning Agent）** ： 不依赖于人类设计师的先验知识。它的核心思想是通过与环境的互动自主学习，而强化学习（Reinforcement Learning, RL）是实现这一思想最具代表性的路径。一个学习型智能体包含一个性能元件（即前面讨论的各类智能体）和一个学习元件。学习元件通过观察性能元件在环境中的行动所带来的结果来不断修正性能元件的决策策略。

### LLM智能体与传统智能体的区别

传统智能体的能力源于工程师的显式编程与知识构建，其行为模式是确定且有边界的；而 LLM 智能体则通过在海量数据上的预训练，获得了隐式的世界模型与强大的涌现能力，使其能够以更灵活、更通用的方式应对复杂任务。

<div align="center"> 
  <img width="627" height="173" alt="image" src="https://github.com/user-attachments/assets/b7fcaea0-293d-4ec0-bb0f-12cc77a1ebe9" />
</div>

以一个“智能旅行助手”为例执行流程：

- **规划与推理**：智能体首先会将这个高层级目标分解为一系列逻辑子任务，例如：[确认出行偏好] -> [查询目的地信息] -> [制定行程草案] -> [预订票务住宿]。这是一个内在的、由模型驱动的规划过程。
- **工具使用**：在执行规划时，智能体识别到信息缺口，会主动调用外部工具来补全。例如，它会调用天气查询接口获取实时天气，并基于“预报有雨”这一信息，在后续规划中倾向于推荐室内活动。
- **动态修正**：在交互过程中，智能体会将用户的反馈（如“这家酒店超出预算”）视为新的约束，并据此调整后续的行动，重新搜索并推荐符合新要求的选项。整个“查天气 → 调行程 → 订酒店”的流程，展现了其根据上下文动态修正自身行为的能力。

## 智能体类型

- 基于内部决策架构的分类
- 基于时间与反应性的分类：反应式智能体 (Reactive Agents)、规划式智能体(Deliberative Agents)、混合式智能体(Hybrid Agents)

  经典的混合架构是分层设计：底层是一个快速的反应模块，处理紧急情况和基本动作；高层则是一个审慎的规划模块，负责制定长远目标，形成分解为一系列“规划-反应”的微循环。而现代的 LLM 智能体，则展现了一种更灵活的混合模式。它们通常在一个“思考-行动-观察”的循环中运作，巧妙地将两种模式融为一体：

  - 规划(Reasoning) ：在“思考”阶段，LLM 分析当前状况，规划出下一步的合理行动。这是一个审议过程。
  - 反应(Acting & Observing) ：在“行动”和“观察”阶段，智能体与外部工具或环境交互，并立即获得反馈。这是一个反应过程。
    
- 基于知识表示的分类
  - 符号主义 AI（Symbolic AI）：智能源于对符号的逻辑操作。这里的符号是人类可读的实体（如词语、概念），操作则遵循严格的逻辑规则。其主要优势在于透明和可解释。
  - 亚符号主义 AI（Sub-symbolic AI）：知识并非显式的规则，而是内隐地分布在一个由大量神经元组成的复杂网络中，是从海量数据中学习到的统计模式。神经网络和深度学习是其代表。
  - 神经符号主义 AI（Neuro-Symbolic AI）：融合两大范式的优点，创造出一个既能像神经网络一样从数据中学习，又能像符号系统一样进行逻辑推理的混合智能体。它试图弥合感知与认知、直觉与理性之间的鸿沟。诺贝尔经济学奖得主丹尼尔·卡尼曼（Daniel Kahneman）在其著作《思考，快与慢》（Thinking, Fast and Slow）中提出的双系统理论。
    - 系统 1是快速、凭直觉、并行的思维模式，类似于亚符号主义 AI 强大的模式识别能力。
    - 系统 2是缓慢、有条理、基于逻辑的审慎思维，恰如符号主义 AI 的推理过程。

<div align="center">  
      <img width="579" height="227" alt="image" src="https://github.com/user-attachments/assets/f9ee63b6-e28f-417c-8321-1a1336881127" />
</div>

## 智能体的构成与运行原理

### 任务环境定义

通常使用 **PEAS 模型** 来精确描述一个任务环境，即 **分析其性能度量(Performance)、环境(Environment)、执行器(Actuators)和传感器(Sensors)** 。

<div align="center">  
  <img width="617" height="139" alt="image" src="https://github.com/user-attachments/assets/3815df48-efa8-4ff7-8904-797573326817" />
</div>

### 智能体的运行机制

<div align="center">  
  <img width="632" height="214" alt="image" src="https://github.com/user-attachments/assets/4eeb0f5c-0603-48f8-9d53-86f080651205" />
</div>

循环主要包含以下几个相互关联的阶段：

1. 感知 (Perception)：这是循环的起点。智能体通过其传感器（例如，API 的监听端口、用户输入接口）接收来自环境的输入信息。这些信息，即观察 (Observation)，既可以是用户的初始指令，也可以是上一步行动所导致的环境状态变化反馈。
2. 思考 (Thought)：接收到观察信息后，智能体进入其核心决策阶段。对于 LLM 智能体而言，这通常是由大语言模型驱动的内部推理过程。如图所示，“思考”阶段可进一步细分为两个关键环节：
   - 规划 (Planning)：智能体基于当前的观察和其内部记忆，更新对任务和环境的理解，并制定或调整一个行动计划。这可能涉及将复杂目标分解为一系列更具体的子任务。
   - 工具选择 (Tool Selection)：根据当前计划，智能体从其可用的工具库中，选择最适合执行下一步骤的工具，并确定调用该工具所需的具体参数。
3. 行动 (Action)：决策完成后，智能体通过其执行器（Actuators）执行具体的行动。这通常表现为调用一个选定的工具（如代码解释器、搜索引擎 API），从而对环境施加影响，意图改变环境的状态。

由 **Thought-Action-Observation** 构成循环的交互范式：
-  Thought (思考)：这是智能体内部决策的“快照”。它以自然语言形式阐述了智能体如何分析当前情境、回顾上一步的观察结果、进行自我反思与问题分解，并最终规划出下一步的具体行动。
-  Action (行动)：这是智能体基于思考后，决定对环境施加的具体操作，通常以函数调用的形式表示。
-  Observation：扮演传感器的角色，将行动执行后环境会返回原始结果输出处理并封装成一段简洁、清晰的自然语言文本，即观察。

## 智能体应用的协作模式

- 作为开发者工具的智能体：智能体被深度集成到开发者的工作流中，作为一种强大的辅助工具。它增强而非取代开发者的角色，通过自动化处理繁琐、重复的任务，让开发者能更专注于创造性的核心工作。如：GitHubCopilot、Claude Code、Trae、Cursor。
- 作为自主协作者的智能体：智能体会像一个真正的项目成员一样，独立地进行规划、推理、执行和反思，直到最终交付成果。这种从助手到协作者的转变，使得 LLM 智能体更深的进入了大众的视野。它标志着我们与 AI 的关系从“命令-执行”演变为“目标-委托”。智能体不再是被动的工具，而是主动的目标追求者。有 CrewAI、AutoGen、MetaGPT、LangGraph 等优秀自主协作框架。

  1. 单智能体自主循环：这是早期的典型范式，如 AgentGPT 所代表的模式。其核心是一个通用智能体通过“思考-规划-执行-反思”的闭环，不断进行自我提示和迭代，以完成一个开放式的高层级目标。
  2. 多智能体协作：这是当前最主流的探索方向，旨在通过模拟人类团队的协作模式来解决复杂问题。它又可细分为不同模式： 角色扮演式对话：如 CAMEL 框架，通过为两个智能体（例如，“程序员”和“产品经理”）设定明确的角色和沟通协议，让它们在一个结构化的对话中协同完成任务。 组织化工作流：如 MetaGPT 和 CrewAI，它们模拟一个分工明确的“虚拟团队”（如软件公司或咨询小组）。每个智能体都有预设的职责和工作流程（SOP），通过层级化或顺序化的方式协作，产出高质量的复杂成果（如完整的代码库或研究报告）。AutoGen 和 AgentScope 则提供了更灵活的对话模式，允许开发者自定义智能体间的复杂交互网络。
  3. 高级控制流架构：诸如 LangGraph 等框架，则更侧重于为智能体提供更强大的底层工程基础。它将智能体的执行过程建模为状态图（State Graph），从而能更灵活、更可靠地实现循环、分支、回溯以及人工介入等复杂流程。
 
## Workflow 和 Agent 的差异

**Workflow 是让 AI 按部就班地执行指令（对一系列任务或步骤进行预先定义的、结构化的编排），而 Agent 则是赋予 AI 自由度去自主达成目标（具备自主性的、以目标为导向的系统，能够在一定程度上理解环境、进行推理、制定计划，并动态地采取行动以达成最终目标。）。**

<img width="631" height="449" alt="image" src="https://github.com/user-attachments/assets/f243a3e1-ca9e-443f-8d93-a265b359eb81" />

# 智能体经典范式构建
## 基础环境配置
### 配置 API 密钥
- 建议使用 Python 3.10 或更高版本。
- openai 库用于与大语言模型交互。
- python-dotenv 库用于安全地管理 API 密钥。（[API设置](https://datawhalechina.github.io/handy-multi-agent/#/chapter1/1.2.api-setup)）

项目根目录下，创建一个名为 .env 的文件，指向任何兼容 OpenAI 接口的本地/第三方服务：

```bash
pip install openai python-dotenv
# .env file
LLM_API_KEY="YOUR-API-KEY"
LLM_MODEL_ID="YOUR-MODEL"
LLM_BASE_URL="YOUR-URL"
```

### 封装基础 LLM 调用函数
LLM客户端类：用于封装所有与模型服务交互的细节。

```python
import os
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict

# 加载 .env 文件中的环境变量
load_dotenv()

class HelloAgentsLLM:
    """
    为本书 "Hello Agents" 定制的LLM客户端。
    它用于调用任何兼容OpenAI接口的服务，并默认使用流式响应。
    """
    def __init__(self, model: str = None, apiKey: str = None, baseUrl: str = None, timeout: int = None):
        """
        初始化客户端。优先使用传入参数，如果未提供，则从环境变量加载。
        """
        self.model = model or os.getenv("LLM_MODEL_ID")
        apiKey = apiKey or os.getenv("LLM_API_KEY")
        baseUrl = baseUrl or os.getenv("LLM_BASE_URL")
        timeout = timeout or int(os.getenv("LLM_TIMEOUT", 60))
        
        if not all([self.model, apiKey, baseUrl]):
            raise ValueError("模型ID、API密钥和服务地址必须被提供或在.env文件中定义。")

        self.client = OpenAI(api_key=apiKey, base_url=baseUrl, timeout=timeout)

    def think(self, messages: List[Dict[str, str]], temperature: float = 0) -> str:
        """
        调用大语言模型进行思考，并返回其响应。
        """
        print(f"🧠 正在调用 {self.model} 模型...")
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=temperature,
                stream=True,
            )
            
            # 处理流式响应
            print("✅ 大语言模型响应成功:")
            collected_content = []
            for chunk in response:
                content = chunk.choices[0].delta.content or ""
                print(content, end="", flush=True)
                collected_content.append(content)
            print()  # 在流式输出结束后换行
            return "".join(collected_content)

        except Exception as e:
            print(f"❌ 调用LLM API时发生错误: {e}")
            return None

# --- 客户端使用示例 ---
if __name__ == '__main__':
    try:
        llmClient = HelloAgentsLLM()
        
        exampleMessages = [
            {"role": "system", "content": "You are a helpful assistant that writes Python code."},
            {"role": "user", "content": "写一个快速排序算法"}
        ]
        
        print("--- 调用LLM ---")
        responseText = llmClient.think(exampleMessages)
        if responseText:
            print("\n\n--- 完整模型响应 ---")
            print(responseText)

    except ValueError as e:
        print(e)


>>>
--- 调用LLM ---
🧠 正在调用 xxxxxx 模型...
✅ 大语言模型响应成功:
快速排序是一种非常高效的排序算法...
```

## 智能体架构范式
为了更好地组织智能体的“思考”与“行动”过程，业界涌现出了LangChain、LlamaIndex等众多优秀框架，以及多种经典的架构范式。其中最具代表性的三种架构范式：

### ReAct (Reasoning and Acting)
一种将“思考”和“行动”紧密结合的范式，让智能体边想边做，动态调整。其核心思想是模仿人类解决问题的方式，将推理 (Reasoning) 与行动 (Acting) 显式地结合起来，形成一个“思考-行动-观察”的循环。不断重复的 Thought -> Action -> Observation 的循环，将新的观察结果追加到历史记录中，形成一个不断增长的上下文，直到它在Thought中认为已经找到了最终答案，然后输出结果。这个过程形成了一个强大的协同效应： **推理使得行动更具目的性，而行动则为推理提供了事实依据**。

<div align="center">
  <img width="665" height="197" alt="image" src="https://github.com/user-attachments/assets/12d32bc0-cef7-44e9-bddd-863c504cda89" />
  <p>图 ReAct 范式中的“思考-行动-观察”协同循环</p>
</div>

将这个过程形式化地表达出来，如图所示。具体来说，在每个时间步 $t$ ，智能体的策略（即大语言模型 $\pi$ ）会根据初始问题 $q$ 和之前所有步骤的“行动-观察”历史轨迹 $((a_1,o_1),\dots,(a_{t-1},o_{t-1}))$ ，来生成当前的思考 $th_t$ 和行动 $a_t$ ：

$$\left(th_t,a_t\right)=\pi\left(q,(a_1,o_1),\ldots,(a_{t-1},o_{t-1})\right)$$

随后，环境中的工具 $T$ 会执行行动 $a_t$ ，并返回一个新的观察结果 $o_t$ ：

$$o_t = T(a_t)$$

这个循环不断进行，将新的 $(a_t,o_t)$ 对追加到历史中，直到模型在思考 $th_t$ 中判断任务已完成。

这种机制特别适用于以下场景：

- 需要外部知识的任务：如查询实时信息（天气、新闻、股价）、搜索专业领域的知识等。
- 需要精确计算的任务：将数学问题交给计算器工具，避免LLM的计算错误。
- 需要与API交互的任务：如操作数据库、调用某个服务的API来完成特定功能。

如果大语言模型是智能体的大脑，那么工具 (Tools) 就是其与外部世界交互的“手和脚”。为了让ReAct范式能够真正解决我们设定的问题，智能体需要具备调用外部工具的能力。一个良好定义的工具应包含以下三个核心要素：

- 名称 (Name)： 一个简洁、唯一的标识符，供智能体在 Action 中调用，例如 Search。
- 描述 (Description)： 一段清晰的自然语言描述，说明这个工具的用途。这是整个机制中最关键的部分，因为大语言模型会依赖这段描述来判断何时使用哪个工具。
- 执行逻辑 (Execution Logic)： 真正执行任务的函数或方法。

### Plan-and-Solve
一种“三思而后行”的范式，智能体首先生成一个完整的行动计划，然后严格执行。这种范式将任务处理明确地分为两个阶段：先规划 (Plan)，后执行 (Solve)。

如果说 ReAct 像一个经验丰富的侦探，根据现场的蛛丝马迹（Observation）一步步推理，随时调整自己的调查方向；那么 Plan-and-Solve 则更像一位建筑师，在动工之前必须先绘制出完整的蓝图（Plan），然后严格按照蓝图来施工（Solve）。事实上现在用的很多大模型工具的Agent模式都融入了这种设计模式。

<div align="center">
  <img width="658" height="289" alt="image" src="https://github.com/user-attachments/assets/b27fe91f-e4c8-471a-b553-72d17dfccfa8" />
  <p>图 Plan-and-Solve 范式的两阶段工作流</p>
</div>

Plan-and-Solve 将整个流程解耦为两个核心阶段，如图所示：

1. **规划阶段 (Planning Phase)**： 首先，智能体会接收用户的完整问题。它的第一个任务不是直接去解决问题或调用工具，而是 **将问题分解，并制定出一个清晰、分步骤的行动计划**。这个计划本身就是一次大语言模型的调用产物。
2. **执行阶段 (Solving Phase)**： 在获得完整的计划后，智能体进入执行阶段。它会 **严格按照计划中的步骤，逐一执行**。每一步的执行都可能是一次独立的 LLM 调用，或者是对上一步结果的加工处理，直到计划中的所有步骤都完成，最终得出答案。

这种“先谋后动”的策略，使得智能体在处理需要长远规划的复杂任务时，能够保持更高的目标一致性，避免在中间步骤中迷失方向。

可以将这个两阶段过程进行形式化表达。首先，规划模型 $\pi_{\text{plan}}$ 根据原始问题 $q$ 生成一个包含 $n$ 个步骤的计划 $P = (p_1, p_2, \dots, p_n)$：

$$
P = \pi_{\text{plan}}(q)
$$

随后，在执行阶段，执行模型 $\pi_{\text{solve}}$ 会逐一完成计划中的步骤。对于第 $i$ 个步骤，其解决方案 $s_i$ 的生成会同时依赖于原始问题 $q$、完整计划 $P$ 以及之前所有步骤的执行结果 $(s_1, \dots, s_{i-1})$：

$$
s_i = \pi_{\text{solve}}(q, P, (s_1, \dots, s_{i-1}))
$$

最终的答案就是最后一个步骤的执行结果 $s_n$。

Plan-and-Solve 尤其适用于那些结构性强、可以被清晰分解的复杂任务，例如：

- 多步数学应用题：需要先列出计算步骤，再逐一求解。
- 需要整合多个信息源的报告撰写：需要先规划好报告结构（引言、数据来源A、数据来源B、总结），再逐一填充内容。
- 代码生成任务：需要先构思好函数、类和模块的结构，再逐一实现。

### Reflection
一种赋予智能体“反思”能力的范式，通过自我批判和修正来优化结果。Reflection 机制的核心思想，正是为智能体引入一种事后（post-hoc）的自我校正循环，使其能够像人类一样，审视自己的工作，发现不足，并进行迭代优化。

Reflection 机制的灵感来源于人类的学习过程：完成初稿后会进行校对，解出数学题后会进行验算。这一思想在多个研究中得到了体现，例如 Shinn, Noah 在2023年提出的 Reflexion 框架。其核心工作流程可以概括为一个简洁的三步循环：执行 -> 反思 -> 优化。

1. **执行 (Execution)**：首先，智能体使用我们熟悉的方法（如 ReAct 或 Plan-and-Solve）尝试完成任务，生成一个初步的解决方案或行动轨迹。这可以看作是“初稿”。
2. **反思 (Reflection)**：接着，智能体进入反思阶段。它会调用一个独立的、或者带有特殊提示词的大语言模型实例，来扮演一个“评审员”的角色。这个“评审员”会审视第一步生成的“初稿”，并从多个维度进行评估，例如：
   - **事实性错误**：是否存在与常识或已知事实相悖的内容？
   - **逻辑漏洞**：推理过程是否存在不连贯或矛盾之处？
   - **效率问题**：是否有更直接、更简洁的路径来完成任务？
   - **遗漏信息**：是否忽略了问题的某些关键约束或方面？ 根据评估，它会生成一段结构化的 **反馈 (Feedback)**，指出具体的问题所在和改进建议。
3.  **优化 (Refinement)**：最后，智能体将“初稿”和“反馈”作为新的上下文，再次调用大语言模型，要求它根据反馈内容对初稿进行修正，生成一个更完善的“修订稿”。

<div align="center">
  <img width="516" height="433" alt="image" src="https://github.com/user-attachments/assets/4cec6988-69c0-4054-b94c-8653133c96d6" />
  <p>图 Reflection 机制中的“执行-反思-优化”迭代循环</p>
</div>

如图所示，这个循环可以重复进行多次，直到反思阶段不再发现新的问题，或者达到预设的迭代次数上限。我们可以将这个迭代优化的过程形式化地表达出来。假设 $O_i$ 是第 $i$ 次迭代产生的输出（ $O_0$ 为初始输出），反思模型 $\pi_{\text{reflect}}$ 会生成针对 $O_i$ 的反馈 $F_i$ ：

$$
F_i = \pi_{\text{reflect}}(\text{Task}, O_i)
$$

随后，优化模型 $\pi_{\text{refine}}$ 会结合原始任务、上一版输出以及反馈，生成新一版的输出 $O_{i+1}$ ：

$$
O_{i+1} = \pi_{\text{refine}}(\text{Task}, O_i, F_i)
$$

与前两种范式相比，Reflection 的价值在于：

- 它为智能体提供了一个内部纠错回路，使其不再完全依赖于外部工具的反馈（ReAct 的 Observation），从而能够修正更高层次的逻辑和策略错误。
- 它将一次性的任务执行，转变为一个持续优化的过程，显著提升了复杂任务的最终成功率和答案质量。
- 它为智能体构建了一个临时的“短期记忆”。整个“执行-反思-优化”的轨迹形成了一个宝贵的经验记录，智能体不仅知道最终答案，还记得自己是如何从有缺陷的初稿迭代到最终版本的。更进一步，这个记忆系统还可以是多模态的，允许智能体反思和修正文本以外的输出（如代码、图像等），为构建更强大的多模态智能体奠定了基础。

<div align="center">
  <img width="526" height="266" alt="dd336456c58b8973b069d562180b2001_4-4" src="https://github.com/user-attachments/assets/1ec5cf13-357c-499e-afdc-eb1f47b76ba1" />
</div>

## 本地模型调用
为了在本地实现高性能、生产级的模型推理服务，社区涌现出了 [VLLM](https://docs.vllm.ai/en/latest/getting_started/installation/) 和 [Ollama](https://ollama.com/) 等优秀工具。它们通过连续批处理、PagedAttention 等技术，显著提升了模型的吞吐量和运行效率，并将模型封装为兼容 OpenAI 标准的 API 服务。

##  HelloAgents 框架样例 

<div align="center">
  <img width="559" height="451" alt="image" src="https://github.com/user-attachments/assets/7a97c51d-20eb-4ea8-92f1-bf25ac4de287" />
  <p>图 基于Helloagents的SimpleAgent运行工作流</p>
</div>

# 智能体通信协议

## 智能体通信协议基础

### MCP（Model Context Protocol）：智能体与工具的桥梁
由 Anthropic 团队提出，其核心设计理念是标准化智能体与外部工具/资源的通信方式，用于智能体与工具的标准化通信。MCP 通过定义统一的协议规范，让所有服务都能以相同的方式被访问。

<div align="center">
  <img width="604" height="467" alt="image" src="https://github.com/user-attachments/assets/8814527e-40af-4f26-9eaf-39ad3cbe8609" />
  <p>图 MCP 设计思想</p>
</div>

MCP 的设计哲学是"上下文共享"。它不仅仅是一个 RPC（远程过程调用）协议，更重要的是它允许智能体和工具之间共享丰富的上下文信息。如图所示，当智能体访问一个代码仓库时，MCP 服务器不仅能提供文件内容，还能提供代码结构、依赖关系、提交历史等上下文信息，让智能体能够做出更智能的决策。

CP 协议采用 Host、Client、Servers 三层架构设计:

<div align="center">
  <img width="600" height="600" alt="a34ee5cedf1ec6671c93ead10c9cf38c_10-5" src="https://github.com/user-attachments/assets/0cc7d963-f25b-4175-9366-617e6a56e272" />
  <p>图 MCP 案例演示</p>
</div>

- Host（宿主层）：Claude Desktop 作为 Host，负责接收用户提问并与 Claude 模型交互。Host 是用户直接交互的界面，它管理整个对话流程。
- Client（客户端层）：当 Claude 模型决定需要访问文件系统时，Host 中内置的 MCP Client 被激活。Client 负责与适当的 MCP Server 建立连接，发送请求并接收响应。
- Server（服务器层）：文件系统 MCP Server 被调用，执行实际的文件扫描操作，访问桌面目录，并返回找到的文档列表。

架构设计的优势在于关注点分离：Host 专注于用户体验，Client 专注于协议通信，Server 专注于具体功能实现。开发者只需专注于开发对应的 MCP Server，无需关心 Host 和 Client 的实现细节。

以“在使用 Claude Desktop 询问："我桌面上有哪些文档？"”为例完整的交互流程：用户问题 → Claude Desktop(Host) → Claude 模型分析 → 需要文件信息 → MCP Client 连接 → 文件系统 MCP Server → 执行操作 → 返回结果 → Claude 生成回答 → 显示在 Claude Desktop 上

MCP 协议的工具访问框架（Tools 主动的执行操作，Resources 被动的提供数据，Prompts 指导性的提供模板）：

<div align="center">
  <p>表 MCP 核心能力</p>
  <img width="613" height="75" alt="image" src="https://github.com/user-attachments/assets/6c3a58ec-f3c1-4c12-9077-4aa06ba119d9" />
</div>

MCP 的工作流程：

<div align="center">
  <img width="600" height="600" alt="d118d132db5ba3d8cea0511d1741b7cb_10-6" src="https://github.com/user-attachments/assets/b67ca055-19c9-4fa8-b467-f45386d5b5c9" />
  <p>图 MCP 案例演示</p>
</div>

<div align="center">
  <p>表 Function Calling 与 MCP 对比示</p>
  <img width="607" height="170" alt="image" src="https://github.com/user-attachments/assets/4788a72f-3a35-4ee7-85ed-2efbd960dc6a" />
</div>

#### MCP 传输方式
<div align="center">
  <p>表 MCP 传输方式对比</p>
  <img width="614" height="112" alt="image" src="https://github.com/user-attachments/assets/b85fb51f-aa9b-4eef-ab6f-9c01f6a26b3a" />
</div>

### A2A（Agent-to-Agent Protocol）：智能体间的对话
协议由 Google 团队提出，其核心设计理念是实现智能体之间的点对点通信，A2A 关注的是智能体之间如何相互点对点协作，像人类团队一样进行对话、协商和协作。

<div align="center">
  <img width="600" height="600" alt="5bbcbc7711c2f8bcba40f9d2a2267935_10-2" src="https://github.com/user-attachments/assets/76f62e15-de7f-40b9-a9f4-80983a58e799" />
  <p>图 A2A 设计思想</p>
</div>

A2A 的设计哲学是"对等通信"。如图 所示，在 A2A 网络中，每个智能体既是服务提供者，也是服务消费者。智能体可以主动发起请求，也可以响应其他智能体的请求。这种对等的设计避免了中心化协调器的瓶颈，让智能体网络更加灵活和可扩展。

### ANP（Agent Network Protocol）：智能体网络的基础设施
是一个概念性的协议框架，目前由开源社区维护，用于构建大规模智能体网络。其核心设计理念是构建大规模智能体网络的基础设施。如果说 MCP 解决的是"如何访问工具"，A2A 解决的是"如何与其他智能体对话"，那么 ANP 解决的是"如何在大规模网络中发现和连接智能体"。

<div align="center">
  <img width="599" height="368" alt="image" src="https://github.com/user-attachments/assets/bcc55882-228d-44eb-8424-2b8effbc4251" />
  <p>图 ANP 设计思想</p>
</div>

ANP 的设计哲学是"去中心化服务发现"。在一个包含成百上千个智能体的网络中，如何让智能体能够找到它需要的服务？如图 所示，ANP 提供了服务注册、发现和路由机制，让智能体能够动态地发现网络中的其他服务，而不需要预先配置所有的连接关系。

<div align="center">
  <p>表 三种协议对比想</p>
  <img width="613" height="131" alt="image" src="https://github.com/user-attachments/assets/fc2313aa-4adc-487d-9c1f-7525a5760643" />
</div>

MCP 的生态相对成熟，不过各种工具的时效性取决于维护者，更推荐选择大公司背书的 MCP 工具。

- 如果智能体需要访问外部服务（文件、数据库、API），选择MCP
- 如果需要多个智能体相互协作完成任务，选择A2A
- 如果要构建大规模的智能体生态系统，考虑ANP

## HelloAgents 通信协议架构设计
如图所示，HelloAgents 的通信协议架构采用三层设计，从底层到上层分别是：协议实现层、工具封装层和智能体集成层。

<div align="center">
  <img width="608" height="528" alt="image" src="https://github.com/user-attachments/assets/9a68c75b-bbbf-4018-b150-d891a9974655" />
  <p>图 HelloAgents 通信协议设计</p>
</div>

1. 协议实现层：这一层包含了三种协议的具体实现。MCP 基于 FastMCP 库实现，提供客户端和服务器功能；A2A 基于 Google 官方的 a2a-sdk 实现；ANP 是需要自研的轻量级实现，提供服务发现和网络管理功能，目前也有官方的[实现](https://github.com/agent-network-protocol/anp)。
2. 工具封装层：这一层将协议实现封装成统一的 Tool 接口。MCPTool、A2ATool 和 ANPTool 都继承自 BaseTool，提供一致的run()方法。这种设计让智能体能够以相同的方式使用不同的协议。
3. 智能体集成层：这一层是智能体与协议的集成点。所有的智能体（ReActAgent、SimpleAgent 等）都通过 Tool System 来使用协议工具，无需关心底层的协议细节。
