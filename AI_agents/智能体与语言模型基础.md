参考：[hello-agents](https://github.com/datawhalechina/hello-agents/tree/main)

# 概念

在人工智能领域，智能体被定义为任何能够通过 **传感器（Sensors）** 感知其所处 **环境（Environment）** ，并 **自主（Autonomy）** 地通过 **执行器（Actuators）** 采取 **行动（Action）** 以达成特定目标的实体。

<img width="605" height="284" alt="image" src="https://github.com/user-attachments/assets/562e2ad9-128f-4a81-ba8a-7ef530fe4890" />

## 智能体演进

- **反射智能体（Simple Reflex Agent）** ：它们的决策核心由工程师明确设计的“条件-动作”规则构成。经典的自动恒温器便是如此：若传感器感知的室温高于设定值，则启动制冷系统。
- **基于模型的反射智能体（Model-Based Reflex Agent）** ：引入了“状态”。这类智能体拥有一个内部的世界模型（World Model），用于追踪和理解环境中那些无法被直接感知的方面。它试图回答：“世界现在是什么样子的？”。例如，一辆在隧道中行驶的自动驾驶汽车，即便摄像头暂时无法感知到前方的车辆，它的内部模型依然会维持对那辆车存在、速度和预估位置的判断。这个内部模型让智能体拥有了初级的“记忆”，使其决策不再仅仅依赖于瞬时感知，而是基于一个更连贯、更完整的世界状态理解。
- **基于目标的智能体（Goal-Based Agent）** ：增加了有明确的目标。与前两者不同，它的行为不再是被动地对环境做出反应，而是主动地、有预见性地选择能够导向某个特定未来状态的行动。这类智能体需要回答的问题是：“我应该做什么才能达成目标？”。经典的例子是 GPS 导航系统：你的目标是到达公司，智能体会基于地图数据（世界模型），通过搜索算法（如 A*算法）来规划（Planning）出一条最优路径。这类智能体的核心能力体现在了对未来的考量与规划上。
- **基于效用的智能体（Utility-Based Agent）** ：引入多个目标。它为每一个可能的世界状态都赋予一个效用值，这个值代表了满意度的高低。智能体的核心目标不再是简单地达成某个特定状态，而是最大化期望效用。它需要回答一个更复杂的问题：“哪种行为能为我带来最满意的结果？”。这种架构让智能体学会在相互冲突的目标之间进行权衡，使其决策更接近人类的理性选择。
- **学习型智能体（Learning Agent）** ： 不依赖于人类设计师的先验知识。它的核心思想是通过与环境的互动自主学习，而强化学习（Reinforcement Learning, RL）是实现这一思想最具代表性的路径。一个学习型智能体包含一个性能元件（即前面讨论的各类智能体）和一个学习元件。学习元件通过观察性能元件在环境中的行动所带来的结果来不断修正性能元件的决策策略。


